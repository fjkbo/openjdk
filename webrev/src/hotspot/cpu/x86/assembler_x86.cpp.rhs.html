<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/assembler.hpp"
  27 #include "asm/assembler.inline.hpp"
  28 #include "gc/shared/cardTableBarrierSet.hpp"
  29 #include "gc/shared/collectedHeap.inline.hpp"
  30 #include "interpreter/interpreter.hpp"
  31 #include "memory/resourceArea.hpp"
  32 #include "prims/methodHandles.hpp"
  33 #include "runtime/biasedLocking.hpp"
  34 #include "runtime/objectMonitor.hpp"
  35 #include "runtime/os.hpp"
  36 #include "runtime/sharedRuntime.hpp"
  37 #include "runtime/stubRoutines.hpp"
  38 #include "utilities/macros.hpp"
  39 
  40 #ifdef PRODUCT
  41 #define BLOCK_COMMENT(str) /* nothing */
  42 #define STOP(error) stop(error)
  43 #else
  44 #define BLOCK_COMMENT(str) block_comment(str)
  45 #define STOP(error) block_comment(error); stop(error)
  46 #endif
  47 
  48 #define BIND(label) bind(label); BLOCK_COMMENT(#label ":")
  49 // Implementation of AddressLiteral
  50 
  51 // A 2-D table for managing compressed displacement(disp8) on EVEX enabled platforms.
  52 unsigned char tuple_table[Assembler::EVEX_ETUP + 1][Assembler::AVX_512bit + 1] = {
  53   // -----------------Table 4.5 -------------------- //
  54   16, 32, 64,  // EVEX_FV(0)
  55   4,  4,  4,   // EVEX_FV(1) - with Evex.b
  56   16, 32, 64,  // EVEX_FV(2) - with Evex.w
  57   8,  8,  8,   // EVEX_FV(3) - with Evex.w and Evex.b
  58   8,  16, 32,  // EVEX_HV(0)
  59   4,  4,  4,   // EVEX_HV(1) - with Evex.b
  60   // -----------------Table 4.6 -------------------- //
  61   16, 32, 64,  // EVEX_FVM(0)
  62   1,  1,  1,   // EVEX_T1S(0)
  63   2,  2,  2,   // EVEX_T1S(1)
  64   4,  4,  4,   // EVEX_T1S(2)
  65   8,  8,  8,   // EVEX_T1S(3)
  66   4,  4,  4,   // EVEX_T1F(0)
  67   8,  8,  8,   // EVEX_T1F(1)
  68   8,  8,  8,   // EVEX_T2(0)
  69   0,  16, 16,  // EVEX_T2(1)
  70   0,  16, 16,  // EVEX_T4(0)
  71   0,  0,  32,  // EVEX_T4(1)
  72   0,  0,  32,  // EVEX_T8(0)
  73   8,  16, 32,  // EVEX_HVM(0)
  74   4,  8,  16,  // EVEX_QVM(0)
  75   2,  4,  8,   // EVEX_OVM(0)
  76   16, 16, 16,  // EVEX_M128(0)
  77   8,  32, 64,  // EVEX_DUP(0)
  78   0,  0,  0    // EVEX_NTUP
  79 };
  80 
  81 AddressLiteral::AddressLiteral(address target, relocInfo::relocType rtype) {
  82   _is_lval = false;
  83   _target = target;
  84   switch (rtype) {
  85   case relocInfo::oop_type:
  86   case relocInfo::metadata_type:
  87     // Oops are a special case. Normally they would be their own section
  88     // but in cases like icBuffer they are literals in the code stream that
  89     // we don't have a section for. We use none so that we get a literal address
  90     // which is always patchable.
  91     break;
  92   case relocInfo::external_word_type:
  93     _rspec = external_word_Relocation::spec(target);
  94     break;
  95   case relocInfo::internal_word_type:
  96     _rspec = internal_word_Relocation::spec(target);
  97     break;
  98   case relocInfo::opt_virtual_call_type:
  99     _rspec = opt_virtual_call_Relocation::spec();
 100     break;
 101   case relocInfo::static_call_type:
 102     _rspec = static_call_Relocation::spec();
 103     break;
 104   case relocInfo::runtime_call_type:
 105     _rspec = runtime_call_Relocation::spec();
 106     break;
 107   case relocInfo::poll_type:
 108   case relocInfo::poll_return_type:
 109     _rspec = Relocation::spec_simple(rtype);
 110     break;
 111   case relocInfo::none:
 112     break;
 113   default:
 114     ShouldNotReachHere();
 115     break;
 116   }
 117 }
 118 
 119 // Implementation of Address
 120 
 121 #ifdef _LP64
 122 
 123 Address Address::make_array(ArrayAddress adr) {
 124   // Not implementable on 64bit machines
 125   // Should have been handled higher up the call chain.
 126   ShouldNotReachHere();
 127   return Address();
 128 }
 129 
 130 // exceedingly dangerous constructor
 131 Address::Address(int disp, address loc, relocInfo::relocType rtype) {
 132   _base  = noreg;
 133   _index = noreg;
 134   _scale = no_scale;
 135   _disp  = disp;
 136   _xmmindex = xnoreg;
 137   _isxmmindex = false;
 138   switch (rtype) {
 139     case relocInfo::external_word_type:
 140       _rspec = external_word_Relocation::spec(loc);
 141       break;
 142     case relocInfo::internal_word_type:
 143       _rspec = internal_word_Relocation::spec(loc);
 144       break;
 145     case relocInfo::runtime_call_type:
 146       // HMM
 147       _rspec = runtime_call_Relocation::spec();
 148       break;
 149     case relocInfo::poll_type:
 150     case relocInfo::poll_return_type:
 151       _rspec = Relocation::spec_simple(rtype);
 152       break;
 153     case relocInfo::none:
 154       break;
 155     default:
 156       ShouldNotReachHere();
 157   }
 158 }
 159 #else // LP64
 160 
 161 Address Address::make_array(ArrayAddress adr) {
 162   AddressLiteral base = adr.base();
 163   Address index = adr.index();
 164   assert(index._disp == 0, "must not have disp"); // maybe it can?
 165   Address array(index._base, index._index, index._scale, (intptr_t) base.target());
 166   array._rspec = base._rspec;
 167   return array;
 168 }
 169 
 170 // exceedingly dangerous constructor
 171 Address::Address(address loc, RelocationHolder spec) {
 172   _base  = noreg;
 173   _index = noreg;
 174   _scale = no_scale;
 175   _disp  = (intptr_t) loc;
 176   _rspec = spec;
 177   _xmmindex = xnoreg;
 178   _isxmmindex = false;
 179 }
 180 
 181 #endif // _LP64
 182 
 183 
 184 
 185 // Convert the raw encoding form into the form expected by the constructor for
 186 // Address.  An index of 4 (rsp) corresponds to having no index, so convert
 187 // that to noreg for the Address constructor.
 188 Address Address::make_raw(int base, int index, int scale, int disp, relocInfo::relocType disp_reloc) {
 189   RelocationHolder rspec;
 190   if (disp_reloc != relocInfo::none) {
 191     rspec = Relocation::spec_simple(disp_reloc);
 192   }
 193   bool valid_index = index != rsp-&gt;encoding();
 194   if (valid_index) {
 195     Address madr(as_Register(base), as_Register(index), (Address::ScaleFactor)scale, in_ByteSize(disp));
 196     madr._rspec = rspec;
 197     return madr;
 198   } else {
 199     Address madr(as_Register(base), noreg, Address::no_scale, in_ByteSize(disp));
 200     madr._rspec = rspec;
 201     return madr;
 202   }
 203 }
 204 
 205 // Implementation of Assembler
 206 
 207 int AbstractAssembler::code_fill_byte() {
 208   return (u_char)'\xF4'; // hlt
 209 }
 210 
 211 // make this go away someday
 212 void Assembler::emit_data(jint data, relocInfo::relocType rtype, int format) {
 213   if (rtype == relocInfo::none)
 214     emit_int32(data);
 215   else
 216     emit_data(data, Relocation::spec_simple(rtype), format);
 217 }
 218 
 219 void Assembler::emit_data(jint data, RelocationHolder const&amp; rspec, int format) {
 220   assert(imm_operand == 0, "default format must be immediate in this file");
 221   assert(inst_mark() != NULL, "must be inside InstructionMark");
 222   if (rspec.type() !=  relocInfo::none) {
 223     #ifdef ASSERT
 224       check_relocation(rspec, format);
 225     #endif
 226     // Do not use AbstractAssembler::relocate, which is not intended for
 227     // embedded words.  Instead, relocate to the enclosing instruction.
 228 
 229     // hack. call32 is too wide for mask so use disp32
 230     if (format == call32_operand)
 231       code_section()-&gt;relocate(inst_mark(), rspec, disp32_operand);
 232     else
 233       code_section()-&gt;relocate(inst_mark(), rspec, format);
 234   }
 235   emit_int32(data);
 236 }
 237 
 238 static int encode(Register r) {
 239   int enc = r-&gt;encoding();
 240   if (enc &gt;= 8) {
 241     enc -= 8;
 242   }
 243   return enc;
 244 }
 245 
 246 void Assembler::emit_arith_b(int op1, int op2, Register dst, int imm8) {
 247   assert(dst-&gt;has_byte_register(), "must have byte register");
 248   assert(isByte(op1) &amp;&amp; isByte(op2), "wrong opcode");
 249   assert(isByte(imm8), "not a byte");
 250   assert((op1 &amp; 0x01) == 0, "should be 8bit operation");
 251   emit_int8(op1);
 252   emit_int8(op2 | encode(dst));
 253   emit_int8(imm8);
 254 }
 255 
 256 
 257 void Assembler::emit_arith(int op1, int op2, Register dst, int32_t imm32) {
 258   assert(isByte(op1) &amp;&amp; isByte(op2), "wrong opcode");
 259   assert((op1 &amp; 0x01) == 1, "should be 32bit operation");
 260   assert((op1 &amp; 0x02) == 0, "sign-extension bit should not be set");
 261   if (is8bit(imm32)) {
 262     emit_int8(op1 | 0x02); // set sign bit
 263     emit_int8(op2 | encode(dst));
 264     emit_int8(imm32 &amp; 0xFF);
 265   } else {
 266     emit_int8(op1);
 267     emit_int8(op2 | encode(dst));
 268     emit_int32(imm32);
 269   }
 270 }
 271 
 272 // Force generation of a 4 byte immediate value even if it fits into 8bit
 273 void Assembler::emit_arith_imm32(int op1, int op2, Register dst, int32_t imm32) {
 274   assert(isByte(op1) &amp;&amp; isByte(op2), "wrong opcode");
 275   assert((op1 &amp; 0x01) == 1, "should be 32bit operation");
 276   assert((op1 &amp; 0x02) == 0, "sign-extension bit should not be set");
 277   emit_int8(op1);
 278   emit_int8(op2 | encode(dst));
 279   emit_int32(imm32);
 280 }
 281 
 282 // immediate-to-memory forms
 283 void Assembler::emit_arith_operand(int op1, Register rm, Address adr, int32_t imm32) {
 284   assert((op1 &amp; 0x01) == 1, "should be 32bit operation");
 285   assert((op1 &amp; 0x02) == 0, "sign-extension bit should not be set");
 286   if (is8bit(imm32)) {
 287     emit_int8(op1 | 0x02); // set sign bit
 288     emit_operand(rm, adr, 1);
 289     emit_int8(imm32 &amp; 0xFF);
 290   } else {
 291     emit_int8(op1);
 292     emit_operand(rm, adr, 4);
 293     emit_int32(imm32);
 294   }
 295 }
 296 
 297 
 298 void Assembler::emit_arith(int op1, int op2, Register dst, Register src) {
 299   assert(isByte(op1) &amp;&amp; isByte(op2), "wrong opcode");
 300   emit_int8(op1);
 301   emit_int8(op2 | encode(dst) &lt;&lt; 3 | encode(src));
 302 }
 303 
 304 
 305 bool Assembler::query_compressed_disp_byte(int disp, bool is_evex_inst, int vector_len,
 306                                            int cur_tuple_type, int in_size_in_bits, int cur_encoding) {
 307   int mod_idx = 0;
 308   // We will test if the displacement fits the compressed format and if so
 309   // apply the compression to the displacment iff the result is8bit.
 310   if (VM_Version::supports_evex() &amp;&amp; is_evex_inst) {
 311     switch (cur_tuple_type) {
 312     case EVEX_FV:
 313       if ((cur_encoding &amp; VEX_W) == VEX_W) {
 314         mod_idx = ((cur_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 3 : 2;
 315       } else {
 316         mod_idx = ((cur_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 1 : 0;
 317       }
 318       break;
 319 
 320     case EVEX_HV:
 321       mod_idx = ((cur_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 1 : 0;
 322       break;
 323 
 324     case EVEX_FVM:
 325       break;
 326 
 327     case EVEX_T1S:
 328       switch (in_size_in_bits) {
 329       case EVEX_8bit:
 330         break;
 331 
 332       case EVEX_16bit:
 333         mod_idx = 1;
 334         break;
 335 
 336       case EVEX_32bit:
 337         mod_idx = 2;
 338         break;
 339 
 340       case EVEX_64bit:
 341         mod_idx = 3;
 342         break;
 343       }
 344       break;
 345 
 346     case EVEX_T1F:
 347     case EVEX_T2:
 348     case EVEX_T4:
 349       mod_idx = (in_size_in_bits == EVEX_64bit) ? 1 : 0;
 350       break;
 351 
 352     case EVEX_T8:
 353       break;
 354 
 355     case EVEX_HVM:
 356       break;
 357 
 358     case EVEX_QVM:
 359       break;
 360 
 361     case EVEX_OVM:
 362       break;
 363 
 364     case EVEX_M128:
 365       break;
 366 
 367     case EVEX_DUP:
 368       break;
 369 
 370     default:
 371       assert(0, "no valid evex tuple_table entry");
 372       break;
 373     }
 374 
 375     if (vector_len &gt;= AVX_128bit &amp;&amp; vector_len &lt;= AVX_512bit) {
 376       int disp_factor = tuple_table[cur_tuple_type + mod_idx][vector_len];
 377       if ((disp % disp_factor) == 0) {
 378         int new_disp = disp / disp_factor;
 379         if ((-0x80 &lt;= new_disp &amp;&amp; new_disp &lt; 0x80)) {
 380           disp = new_disp;
 381         }
 382       } else {
 383         return false;
 384       }
 385     }
 386   }
 387   return (-0x80 &lt;= disp &amp;&amp; disp &lt; 0x80);
 388 }
 389 
 390 
 391 bool Assembler::emit_compressed_disp_byte(int &amp;disp) {
 392   int mod_idx = 0;
 393   // We will test if the displacement fits the compressed format and if so
 394   // apply the compression to the displacment iff the result is8bit.
 395   if (VM_Version::supports_evex() &amp;&amp; _attributes &amp;&amp; _attributes-&gt;is_evex_instruction()) {
 396     int evex_encoding = _attributes-&gt;get_evex_encoding();
 397     int tuple_type = _attributes-&gt;get_tuple_type();
 398     switch (tuple_type) {
 399     case EVEX_FV:
 400       if ((evex_encoding &amp; VEX_W) == VEX_W) {
 401         mod_idx = ((evex_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 3 : 2;
 402       } else {
 403         mod_idx = ((evex_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 1 : 0;
 404       }
 405       break;
 406 
 407     case EVEX_HV:
 408       mod_idx = ((evex_encoding &amp; EVEX_Rb) == EVEX_Rb) ? 1 : 0;
 409       break;
 410 
 411     case EVEX_FVM:
 412       break;
 413 
 414     case EVEX_T1S:
 415       switch (_attributes-&gt;get_input_size()) {
 416       case EVEX_8bit:
 417         break;
 418 
 419       case EVEX_16bit:
 420         mod_idx = 1;
 421         break;
 422 
 423       case EVEX_32bit:
 424         mod_idx = 2;
 425         break;
 426 
 427       case EVEX_64bit:
 428         mod_idx = 3;
 429         break;
 430       }
 431       break;
 432 
 433     case EVEX_T1F:
 434     case EVEX_T2:
 435     case EVEX_T4:
 436       mod_idx = (_attributes-&gt;get_input_size() == EVEX_64bit) ? 1 : 0;
 437       break;
 438 
 439     case EVEX_T8:
 440       break;
 441 
 442     case EVEX_HVM:
 443       break;
 444 
 445     case EVEX_QVM:
 446       break;
 447 
 448     case EVEX_OVM:
 449       break;
 450 
 451     case EVEX_M128:
 452       break;
 453 
 454     case EVEX_DUP:
 455       break;
 456 
 457     default:
 458       assert(0, "no valid evex tuple_table entry");
 459       break;
 460     }
 461 
 462     int vector_len = _attributes-&gt;get_vector_len();
 463     if (vector_len &gt;= AVX_128bit &amp;&amp; vector_len &lt;= AVX_512bit) {
 464       int disp_factor = tuple_table[tuple_type + mod_idx][vector_len];
 465       if ((disp % disp_factor) == 0) {
 466         int new_disp = disp / disp_factor;
 467         if (is8bit(new_disp)) {
 468           disp = new_disp;
 469         }
 470       } else {
 471         return false;
 472       }
 473     }
 474   }
 475   return is8bit(disp);
 476 }
 477 
 478 
 479 void Assembler::emit_operand(Register reg, Register base, Register index,
 480                              Address::ScaleFactor scale, int disp,
 481                              RelocationHolder const&amp; rspec,
 482                              int rip_relative_correction) {
 483   relocInfo::relocType rtype = (relocInfo::relocType) rspec.type();
 484 
 485   // Encode the registers as needed in the fields they are used in
 486 
 487   int regenc = encode(reg) &lt;&lt; 3;
 488   int indexenc = index-&gt;is_valid() ? encode(index) &lt;&lt; 3 : 0;
 489   int baseenc = base-&gt;is_valid() ? encode(base) : 0;
 490 
 491   if (base-&gt;is_valid()) {
 492     if (index-&gt;is_valid()) {
 493       assert(scale != Address::no_scale, "inconsistent address");
 494       // [base + index*scale + disp]
 495       if (disp == 0 &amp;&amp; rtype == relocInfo::none  &amp;&amp;
 496           base != rbp LP64_ONLY(&amp;&amp; base != r13)) {
 497         // [base + index*scale]
 498         // [00 reg 100][ss index base]
 499         assert(index != rsp, "illegal addressing mode");
 500         emit_int8(0x04 | regenc);
 501         emit_int8(scale &lt;&lt; 6 | indexenc | baseenc);
 502       } else if (emit_compressed_disp_byte(disp) &amp;&amp; rtype == relocInfo::none) {
 503         // [base + index*scale + imm8]
 504         // [01 reg 100][ss index base] imm8
 505         assert(index != rsp, "illegal addressing mode");
 506         emit_int8(0x44 | regenc);
 507         emit_int8(scale &lt;&lt; 6 | indexenc | baseenc);
 508         emit_int8(disp &amp; 0xFF);
 509       } else {
 510         // [base + index*scale + disp32]
 511         // [10 reg 100][ss index base] disp32
 512         assert(index != rsp, "illegal addressing mode");
 513         emit_int8(0x84 | regenc);
 514         emit_int8(scale &lt;&lt; 6 | indexenc | baseenc);
 515         emit_data(disp, rspec, disp32_operand);
 516       }
 517     } else if (base == rsp LP64_ONLY(|| base == r12)) {
 518       // [rsp + disp]
 519       if (disp == 0 &amp;&amp; rtype == relocInfo::none) {
 520         // [rsp]
 521         // [00 reg 100][00 100 100]
 522         emit_int8(0x04 | regenc);
 523         emit_int8(0x24);
 524       } else if (emit_compressed_disp_byte(disp) &amp;&amp; rtype == relocInfo::none) {
 525         // [rsp + imm8]
 526         // [01 reg 100][00 100 100] disp8
 527         emit_int8(0x44 | regenc);
 528         emit_int8(0x24);
 529         emit_int8(disp &amp; 0xFF);
 530       } else {
 531         // [rsp + imm32]
 532         // [10 reg 100][00 100 100] disp32
 533         emit_int8(0x84 | regenc);
 534         emit_int8(0x24);
 535         emit_data(disp, rspec, disp32_operand);
 536       }
 537     } else {
 538       // [base + disp]
 539       assert(base != rsp LP64_ONLY(&amp;&amp; base != r12), "illegal addressing mode");
 540       if (disp == 0 &amp;&amp; rtype == relocInfo::none &amp;&amp;
 541           base != rbp LP64_ONLY(&amp;&amp; base != r13)) {
 542         // [base]
 543         // [00 reg base]
 544         emit_int8(0x00 | regenc | baseenc);
 545       } else if (emit_compressed_disp_byte(disp) &amp;&amp; rtype == relocInfo::none) {
 546         // [base + disp8]
 547         // [01 reg base] disp8
 548         emit_int8(0x40 | regenc | baseenc);
 549         emit_int8(disp &amp; 0xFF);
 550       } else {
 551         // [base + disp32]
 552         // [10 reg base] disp32
 553         emit_int8(0x80 | regenc | baseenc);
 554         emit_data(disp, rspec, disp32_operand);
 555       }
 556     }
 557   } else {
 558     if (index-&gt;is_valid()) {
 559       assert(scale != Address::no_scale, "inconsistent address");
 560       // [index*scale + disp]
 561       // [00 reg 100][ss index 101] disp32
 562       assert(index != rsp, "illegal addressing mode");
 563       emit_int8(0x04 | regenc);
 564       emit_int8(scale &lt;&lt; 6 | indexenc | 0x05);
 565       emit_data(disp, rspec, disp32_operand);
 566     } else if (rtype != relocInfo::none ) {
 567       // [disp] (64bit) RIP-RELATIVE (32bit) abs
 568       // [00 000 101] disp32
 569 
 570       emit_int8(0x05 | regenc);
 571       // Note that the RIP-rel. correction applies to the generated
 572       // disp field, but _not_ to the target address in the rspec.
 573 
 574       // disp was created by converting the target address minus the pc
 575       // at the start of the instruction. That needs more correction here.
 576       // intptr_t disp = target - next_ip;
 577       assert(inst_mark() != NULL, "must be inside InstructionMark");
 578       address next_ip = pc() + sizeof(int32_t) + rip_relative_correction;
 579       int64_t adjusted = disp;
 580       // Do rip-rel adjustment for 64bit
 581       LP64_ONLY(adjusted -=  (next_ip - inst_mark()));
 582       assert(is_simm32(adjusted),
 583              "must be 32bit offset (RIP relative address)");
 584       emit_data((int32_t) adjusted, rspec, disp32_operand);
 585 
 586     } else {
 587       // 32bit never did this, did everything as the rip-rel/disp code above
 588       // [disp] ABSOLUTE
 589       // [00 reg 100][00 100 101] disp32
 590       emit_int8(0x04 | regenc);
 591       emit_int8(0x25);
 592       emit_data(disp, rspec, disp32_operand);
 593     }
 594   }
 595 }
 596 
 597 void Assembler::emit_operand(XMMRegister reg, Register base, Register index,
 598                              Address::ScaleFactor scale, int disp,
 599                              RelocationHolder const&amp; rspec) {
 600   if (UseAVX &gt; 2) {
 601     int xreg_enc = reg-&gt;encoding();
 602     if (xreg_enc &gt; 15) {
 603       XMMRegister new_reg = as_XMMRegister(xreg_enc &amp; 0xf);
 604       emit_operand((Register)new_reg, base, index, scale, disp, rspec);
 605       return;
 606     }
 607   }
 608   emit_operand((Register)reg, base, index, scale, disp, rspec);
 609 }
 610 
 611 void Assembler::emit_operand(XMMRegister reg, Register base, XMMRegister index,
 612                              Address::ScaleFactor scale, int disp,
 613                              RelocationHolder const&amp; rspec) {
 614   if (UseAVX &gt; 2) {
 615     int xreg_enc = reg-&gt;encoding();
 616     int xmmindex_enc = index-&gt;encoding();
 617     XMMRegister new_reg = as_XMMRegister(xreg_enc &amp; 0xf);
 618     XMMRegister new_index = as_XMMRegister(xmmindex_enc &amp; 0xf);
 619     emit_operand((Register)new_reg, base, (Register)new_index, scale, disp, rspec);
 620   } else {
 621     emit_operand((Register)reg, base, (Register)index, scale, disp, rspec);
 622   }
 623 }
 624 
 625 
 626 // Secret local extension to Assembler::WhichOperand:
 627 #define end_pc_operand (_WhichOperand_limit)
 628 
 629 address Assembler::locate_operand(address inst, WhichOperand which) {
 630   // Decode the given instruction, and return the address of
 631   // an embedded 32-bit operand word.
 632 
 633   // If "which" is disp32_operand, selects the displacement portion
 634   // of an effective address specifier.
 635   // If "which" is imm64_operand, selects the trailing immediate constant.
 636   // If "which" is call32_operand, selects the displacement of a call or jump.
 637   // Caller is responsible for ensuring that there is such an operand,
 638   // and that it is 32/64 bits wide.
 639 
 640   // If "which" is end_pc_operand, find the end of the instruction.
 641 
 642   address ip = inst;
 643   bool is_64bit = false;
 644 
 645   debug_only(bool has_disp32 = false);
 646   int tail_size = 0; // other random bytes (#32, #16, etc.) at end of insn
 647 
 648   again_after_prefix:
 649   switch (0xFF &amp; *ip++) {
 650 
 651   // These convenience macros generate groups of "case" labels for the switch.
 652 #define REP4(x) (x)+0: case (x)+1: case (x)+2: case (x)+3
 653 #define REP8(x) (x)+0: case (x)+1: case (x)+2: case (x)+3: \
 654              case (x)+4: case (x)+5: case (x)+6: case (x)+7
 655 #define REP16(x) REP8((x)+0): \
 656               case REP8((x)+8)
 657 
 658   case CS_segment:
 659   case SS_segment:
 660   case DS_segment:
 661   case ES_segment:
 662   case FS_segment:
 663   case GS_segment:
 664     // Seems dubious
 665     LP64_ONLY(assert(false, "shouldn't have that prefix"));
 666     assert(ip == inst+1, "only one prefix allowed");
 667     goto again_after_prefix;
 668 
 669   case 0x67:
 670   case REX:
 671   case REX_B:
 672   case REX_X:
 673   case REX_XB:
 674   case REX_R:
 675   case REX_RB:
 676   case REX_RX:
 677   case REX_RXB:
 678     NOT_LP64(assert(false, "64bit prefixes"));
 679     goto again_after_prefix;
 680 
 681   case REX_W:
 682   case REX_WB:
 683   case REX_WX:
 684   case REX_WXB:
 685   case REX_WR:
 686   case REX_WRB:
 687   case REX_WRX:
 688   case REX_WRXB:
 689     NOT_LP64(assert(false, "64bit prefixes"));
 690     is_64bit = true;
 691     goto again_after_prefix;
 692 
 693   case 0xFF: // pushq a; decl a; incl a; call a; jmp a
 694   case 0x88: // movb a, r
 695   case 0x89: // movl a, r
 696   case 0x8A: // movb r, a
 697   case 0x8B: // movl r, a
 698   case 0x8F: // popl a
 699     debug_only(has_disp32 = true);
 700     break;
 701 
 702   case 0x68: // pushq #32
 703     if (which == end_pc_operand) {
 704       return ip + 4;
 705     }
 706     assert(which == imm_operand &amp;&amp; !is_64bit, "pushl has no disp32 or 64bit immediate");
 707     return ip;                  // not produced by emit_operand
 708 
 709   case 0x66: // movw ... (size prefix)
 710     again_after_size_prefix2:
 711     switch (0xFF &amp; *ip++) {
 712     case REX:
 713     case REX_B:
 714     case REX_X:
 715     case REX_XB:
 716     case REX_R:
 717     case REX_RB:
 718     case REX_RX:
 719     case REX_RXB:
 720     case REX_W:
 721     case REX_WB:
 722     case REX_WX:
 723     case REX_WXB:
 724     case REX_WR:
 725     case REX_WRB:
 726     case REX_WRX:
 727     case REX_WRXB:
 728       NOT_LP64(assert(false, "64bit prefix found"));
 729       goto again_after_size_prefix2;
 730     case 0x8B: // movw r, a
 731     case 0x89: // movw a, r
 732       debug_only(has_disp32 = true);
 733       break;
 734     case 0xC7: // movw a, #16
 735       debug_only(has_disp32 = true);
 736       tail_size = 2;  // the imm16
 737       break;
 738     case 0x0F: // several SSE/SSE2 variants
 739       ip--;    // reparse the 0x0F
 740       goto again_after_prefix;
 741     default:
 742       ShouldNotReachHere();
 743     }
 744     break;
 745 
 746   case REP8(0xB8): // movl/q r, #32/#64(oop?)
 747     if (which == end_pc_operand)  return ip + (is_64bit ? 8 : 4);
 748     // these asserts are somewhat nonsensical
 749 #ifndef _LP64
 750     assert(which == imm_operand || which == disp32_operand,
 751            "which %d is_64_bit %d ip " INTPTR_FORMAT, which, is_64bit, p2i(ip));
 752 #else
 753     assert((which == call32_operand || which == imm_operand) &amp;&amp; is_64bit ||
 754            which == narrow_oop_operand &amp;&amp; !is_64bit,
 755            "which %d is_64_bit %d ip " INTPTR_FORMAT, which, is_64bit, p2i(ip));
 756 #endif // _LP64
 757     return ip;
 758 
 759   case 0x69: // imul r, a, #32
 760   case 0xC7: // movl a, #32(oop?)
 761     tail_size = 4;
 762     debug_only(has_disp32 = true); // has both kinds of operands!
 763     break;
 764 
 765   case 0x0F: // movx..., etc.
 766     switch (0xFF &amp; *ip++) {
 767     case 0x3A: // pcmpestri
 768       tail_size = 1;
 769     case 0x38: // ptest, pmovzxbw
 770       ip++; // skip opcode
 771       debug_only(has_disp32 = true); // has both kinds of operands!
 772       break;
 773 
 774     case 0x70: // pshufd r, r/a, #8
 775       debug_only(has_disp32 = true); // has both kinds of operands!
 776     case 0x73: // psrldq r, #8
 777       tail_size = 1;
 778       break;
 779 
 780     case 0x12: // movlps
 781     case 0x28: // movaps
 782     case 0x2E: // ucomiss
 783     case 0x2F: // comiss
 784     case 0x54: // andps
 785     case 0x55: // andnps
 786     case 0x56: // orps
 787     case 0x57: // xorps
 788     case 0x58: // addpd
 789     case 0x59: // mulpd
 790     case 0x6E: // movd
 791     case 0x7E: // movd
 792     case 0xAE: // ldmxcsr, stmxcsr, fxrstor, fxsave, clflush
 793     case 0xFE: // paddd
 794       debug_only(has_disp32 = true);
 795       break;
 796 
 797     case 0xAD: // shrd r, a, %cl
 798     case 0xAF: // imul r, a
 799     case 0xBE: // movsbl r, a (movsxb)
 800     case 0xBF: // movswl r, a (movsxw)
 801     case 0xB6: // movzbl r, a (movzxb)
 802     case 0xB7: // movzwl r, a (movzxw)
 803     case REP16(0x40): // cmovl cc, r, a
 804     case 0xB0: // cmpxchgb
 805     case 0xB1: // cmpxchg
 806     case 0xC1: // xaddl
 807     case 0xC7: // cmpxchg8
 808     case REP16(0x90): // setcc a
 809       debug_only(has_disp32 = true);
 810       // fall out of the switch to decode the address
 811       break;
 812 
 813     case 0xC4: // pinsrw r, a, #8
 814       debug_only(has_disp32 = true);
 815     case 0xC5: // pextrw r, r, #8
 816       tail_size = 1;  // the imm8
 817       break;
 818 
 819     case 0xAC: // shrd r, a, #8
 820       debug_only(has_disp32 = true);
 821       tail_size = 1;  // the imm8
 822       break;
 823 
 824     case REP16(0x80): // jcc rdisp32
 825       if (which == end_pc_operand)  return ip + 4;
 826       assert(which == call32_operand, "jcc has no disp32 or imm");
 827       return ip;
 828     default:
 829       ShouldNotReachHere();
 830     }
 831     break;
 832 
 833   case 0x81: // addl a, #32; addl r, #32
 834     // also: orl, adcl, sbbl, andl, subl, xorl, cmpl
 835     // on 32bit in the case of cmpl, the imm might be an oop
 836     tail_size = 4;
 837     debug_only(has_disp32 = true); // has both kinds of operands!
 838     break;
 839 
 840   case 0x83: // addl a, #8; addl r, #8
 841     // also: orl, adcl, sbbl, andl, subl, xorl, cmpl
 842     debug_only(has_disp32 = true); // has both kinds of operands!
 843     tail_size = 1;
 844     break;
 845 
 846   case 0x9B:
 847     switch (0xFF &amp; *ip++) {
 848     case 0xD9: // fnstcw a
 849       debug_only(has_disp32 = true);
 850       break;
 851     default:
 852       ShouldNotReachHere();
 853     }
 854     break;
 855 
 856   case REP4(0x00): // addb a, r; addl a, r; addb r, a; addl r, a
 857   case REP4(0x10): // adc...
 858   case REP4(0x20): // and...
 859   case REP4(0x30): // xor...
 860   case REP4(0x08): // or...
 861   case REP4(0x18): // sbb...
 862   case REP4(0x28): // sub...
 863   case 0xF7: // mull a
 864   case 0x8D: // lea r, a
 865   case 0x87: // xchg r, a
 866   case REP4(0x38): // cmp...
 867   case 0x85: // test r, a
 868     debug_only(has_disp32 = true); // has both kinds of operands!
 869     break;
 870 
 871   case 0xC1: // sal a, #8; sar a, #8; shl a, #8; shr a, #8
 872   case 0xC6: // movb a, #8
 873   case 0x80: // cmpb a, #8
 874   case 0x6B: // imul r, a, #8
 875     debug_only(has_disp32 = true); // has both kinds of operands!
 876     tail_size = 1; // the imm8
 877     break;
 878 
 879   case 0xC4: // VEX_3bytes
 880   case 0xC5: // VEX_2bytes
 881     assert((UseAVX &gt; 0), "shouldn't have VEX prefix");
 882     assert(ip == inst+1, "no prefixes allowed");
 883     // C4 and C5 are also used as opcodes for PINSRW and PEXTRW instructions
 884     // but they have prefix 0x0F and processed when 0x0F processed above.
 885     //
 886     // In 32-bit mode the VEX first byte C4 and C5 alias onto LDS and LES
 887     // instructions (these instructions are not supported in 64-bit mode).
 888     // To distinguish them bits [7:6] are set in the VEX second byte since
 889     // ModRM byte can not be of the form 11xxxxxx in 32-bit mode. To set
 890     // those VEX bits REX and vvvv bits are inverted.
 891     //
 892     // Fortunately C2 doesn't generate these instructions so we don't need
 893     // to check for them in product version.
 894 
 895     // Check second byte
 896     NOT_LP64(assert((0xC0 &amp; *ip) == 0xC0, "shouldn't have LDS and LES instructions"));
 897 
 898     int vex_opcode;
 899     // First byte
 900     if ((0xFF &amp; *inst) == VEX_3bytes) {
 901       vex_opcode = VEX_OPCODE_MASK &amp; *ip;
 902       ip++; // third byte
 903       is_64bit = ((VEX_W &amp; *ip) == VEX_W);
 904     } else {
 905       vex_opcode = VEX_OPCODE_0F;
 906     }
 907     ip++; // opcode
 908     // To find the end of instruction (which == end_pc_operand).
 909     switch (vex_opcode) {
 910       case VEX_OPCODE_0F:
 911         switch (0xFF &amp; *ip) {
 912         case 0x70: // pshufd r, r/a, #8
 913         case 0x71: // ps[rl|ra|ll]w r, #8
 914         case 0x72: // ps[rl|ra|ll]d r, #8
 915         case 0x73: // ps[rl|ra|ll]q r, #8
 916         case 0xC2: // cmp[ps|pd|ss|sd] r, r, r/a, #8
 917         case 0xC4: // pinsrw r, r, r/a, #8
 918         case 0xC5: // pextrw r/a, r, #8
 919         case 0xC6: // shufp[s|d] r, r, r/a, #8
 920           tail_size = 1;  // the imm8
 921           break;
 922         }
 923         break;
 924       case VEX_OPCODE_0F_3A:
 925         tail_size = 1;
 926         break;
 927     }
 928     ip++; // skip opcode
 929     debug_only(has_disp32 = true); // has both kinds of operands!
 930     break;
 931 
 932   case 0x62: // EVEX_4bytes
 933     assert(VM_Version::supports_evex(), "shouldn't have EVEX prefix");
 934     assert(ip == inst+1, "no prefixes allowed");
 935     // no EVEX collisions, all instructions that have 0x62 opcodes
 936     // have EVEX versions and are subopcodes of 0x66
 937     ip++; // skip P0 and exmaine W in P1
 938     is_64bit = ((VEX_W &amp; *ip) == VEX_W);
 939     ip++; // move to P2
 940     ip++; // skip P2, move to opcode
 941     // To find the end of instruction (which == end_pc_operand).
 942     switch (0xFF &amp; *ip) {
 943     case 0x22: // pinsrd r, r/a, #8
 944     case 0x61: // pcmpestri r, r/a, #8
 945     case 0x70: // pshufd r, r/a, #8
 946     case 0x73: // psrldq r, #8
 947       tail_size = 1;  // the imm8
 948       break;
 949     default:
 950       break;
 951     }
 952     ip++; // skip opcode
 953     debug_only(has_disp32 = true); // has both kinds of operands!
 954     break;
 955 
 956   case 0xD1: // sal a, 1; sar a, 1; shl a, 1; shr a, 1
 957   case 0xD3: // sal a, %cl; sar a, %cl; shl a, %cl; shr a, %cl
 958   case 0xD9: // fld_s a; fst_s a; fstp_s a; fldcw a
 959   case 0xDD: // fld_d a; fst_d a; fstp_d a
 960   case 0xDB: // fild_s a; fistp_s a; fld_x a; fstp_x a
 961   case 0xDF: // fild_d a; fistp_d a
 962   case 0xD8: // fadd_s a; fsubr_s a; fmul_s a; fdivr_s a; fcomp_s a
 963   case 0xDC: // fadd_d a; fsubr_d a; fmul_d a; fdivr_d a; fcomp_d a
 964   case 0xDE: // faddp_d a; fsubrp_d a; fmulp_d a; fdivrp_d a; fcompp_d a
 965     debug_only(has_disp32 = true);
 966     break;
 967 
 968   case 0xE8: // call rdisp32
 969   case 0xE9: // jmp  rdisp32
 970     if (which == end_pc_operand)  return ip + 4;
 971     assert(which == call32_operand, "call has no disp32 or imm");
 972     return ip;
 973 
 974   case 0xF0:                    // Lock
 975     goto again_after_prefix;
 976 
 977   case 0xF3:                    // For SSE
 978   case 0xF2:                    // For SSE2
 979     switch (0xFF &amp; *ip++) {
 980     case REX:
 981     case REX_B:
 982     case REX_X:
 983     case REX_XB:
 984     case REX_R:
 985     case REX_RB:
 986     case REX_RX:
 987     case REX_RXB:
 988     case REX_W:
 989     case REX_WB:
 990     case REX_WX:
 991     case REX_WXB:
 992     case REX_WR:
 993     case REX_WRB:
 994     case REX_WRX:
 995     case REX_WRXB:
 996       NOT_LP64(assert(false, "found 64bit prefix"));
 997       ip++;
 998     default:
 999       ip++;
1000     }
1001     debug_only(has_disp32 = true); // has both kinds of operands!
1002     break;
1003 
1004   default:
1005     ShouldNotReachHere();
1006 
1007 #undef REP8
1008 #undef REP16
1009   }
1010 
1011   assert(which != call32_operand, "instruction is not a call, jmp, or jcc");
1012 #ifdef _LP64
1013   assert(which != imm_operand, "instruction is not a movq reg, imm64");
1014 #else
1015   // assert(which != imm_operand || has_imm32, "instruction has no imm32 field");
1016   assert(which != imm_operand || has_disp32, "instruction has no imm32 field");
1017 #endif // LP64
1018   assert(which != disp32_operand || has_disp32, "instruction has no disp32 field");
1019 
1020   // parse the output of emit_operand
1021   int op2 = 0xFF &amp; *ip++;
1022   int base = op2 &amp; 0x07;
1023   int op3 = -1;
1024   const int b100 = 4;
1025   const int b101 = 5;
1026   if (base == b100 &amp;&amp; (op2 &gt;&gt; 6) != 3) {
1027     op3 = 0xFF &amp; *ip++;
1028     base = op3 &amp; 0x07;   // refetch the base
1029   }
1030   // now ip points at the disp (if any)
1031 
1032   switch (op2 &gt;&gt; 6) {
1033   case 0:
1034     // [00 reg  100][ss index base]
1035     // [00 reg  100][00   100  esp]
1036     // [00 reg base]
1037     // [00 reg  100][ss index  101][disp32]
1038     // [00 reg  101]               [disp32]
1039 
1040     if (base == b101) {
1041       if (which == disp32_operand)
1042         return ip;              // caller wants the disp32
1043       ip += 4;                  // skip the disp32
1044     }
1045     break;
1046 
1047   case 1:
1048     // [01 reg  100][ss index base][disp8]
1049     // [01 reg  100][00   100  esp][disp8]
1050     // [01 reg base]               [disp8]
1051     ip += 1;                    // skip the disp8
1052     break;
1053 
1054   case 2:
1055     // [10 reg  100][ss index base][disp32]
1056     // [10 reg  100][00   100  esp][disp32]
1057     // [10 reg base]               [disp32]
1058     if (which == disp32_operand)
1059       return ip;                // caller wants the disp32
1060     ip += 4;                    // skip the disp32
1061     break;
1062 
1063   case 3:
1064     // [11 reg base]  (not a memory addressing mode)
1065     break;
1066   }
1067 
1068   if (which == end_pc_operand) {
1069     return ip + tail_size;
1070   }
1071 
1072 #ifdef _LP64
1073   assert(which == narrow_oop_operand &amp;&amp; !is_64bit, "instruction is not a movl adr, imm32");
1074 #else
1075   assert(which == imm_operand, "instruction has only an imm field");
1076 #endif // LP64
1077   return ip;
1078 }
1079 
1080 address Assembler::locate_next_instruction(address inst) {
1081   // Secretly share code with locate_operand:
1082   return locate_operand(inst, end_pc_operand);
1083 }
1084 
1085 
1086 #ifdef ASSERT
1087 void Assembler::check_relocation(RelocationHolder const&amp; rspec, int format) {
1088   address inst = inst_mark();
1089   assert(inst != NULL &amp;&amp; inst &lt; pc(), "must point to beginning of instruction");
1090   address opnd;
1091 
1092   Relocation* r = rspec.reloc();
1093   if (r-&gt;type() == relocInfo::none) {
1094     return;
1095   } else if (r-&gt;is_call() || format == call32_operand) {
1096     // assert(format == imm32_operand, "cannot specify a nonzero format");
1097     opnd = locate_operand(inst, call32_operand);
1098   } else if (r-&gt;is_data()) {
1099     assert(format == imm_operand || format == disp32_operand
1100            LP64_ONLY(|| format == narrow_oop_operand), "format ok");
1101     opnd = locate_operand(inst, (WhichOperand)format);
1102   } else {
1103     assert(format == imm_operand, "cannot specify a format");
1104     return;
1105   }
1106   assert(opnd == pc(), "must put operand where relocs can find it");
1107 }
1108 #endif // ASSERT
1109 
1110 void Assembler::emit_operand32(Register reg, Address adr) {
1111   assert(reg-&gt;encoding() &lt; 8, "no extended registers");
1112   assert(!adr.base_needs_rex() &amp;&amp; !adr.index_needs_rex(), "no extended registers");
1113   emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,
1114                adr._rspec);
1115 }
1116 
1117 void Assembler::emit_operand(Register reg, Address adr,
1118                              int rip_relative_correction) {
1119   emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,
1120                adr._rspec,
1121                rip_relative_correction);
1122 }
1123 
1124 void Assembler::emit_operand(XMMRegister reg, Address adr) {
1125     if (adr.isxmmindex()) {
1126        emit_operand(reg, adr._base, adr._xmmindex, adr._scale, adr._disp, adr._rspec);
1127     } else {
1128        emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,
1129        adr._rspec);
1130     }
1131 }
1132 
1133 // MMX operations
1134 void Assembler::emit_operand(MMXRegister reg, Address adr) {
1135   assert(!adr.base_needs_rex() &amp;&amp; !adr.index_needs_rex(), "no extended registers");
1136   emit_operand((Register)reg, adr._base, adr._index, adr._scale, adr._disp, adr._rspec);
1137 }
1138 
1139 // work around gcc (3.2.1-7a) bug
1140 void Assembler::emit_operand(Address adr, MMXRegister reg) {
1141   assert(!adr.base_needs_rex() &amp;&amp; !adr.index_needs_rex(), "no extended registers");
1142   emit_operand((Register)reg, adr._base, adr._index, adr._scale, adr._disp, adr._rspec);
1143 }
1144 
1145 
1146 void Assembler::emit_farith(int b1, int b2, int i) {
1147   assert(isByte(b1) &amp;&amp; isByte(b2), "wrong opcode");
1148   assert(0 &lt;= i &amp;&amp;  i &lt; 8, "illegal stack offset");
1149   emit_int8(b1);
1150   emit_int8(b2 + i);
1151 }
1152 
1153 
1154 // Now the Assembler instructions (identical for 32/64 bits)
1155 
1156 void Assembler::adcl(Address dst, int32_t imm32) {
1157   InstructionMark im(this);
1158   prefix(dst);
1159   emit_arith_operand(0x81, rdx, dst, imm32);
1160 }
1161 
1162 void Assembler::adcl(Address dst, Register src) {
1163   InstructionMark im(this);
1164   prefix(dst, src);
1165   emit_int8(0x11);
1166   emit_operand(src, dst);
1167 }
1168 
1169 void Assembler::adcl(Register dst, int32_t imm32) {
1170   prefix(dst);
1171   emit_arith(0x81, 0xD0, dst, imm32);
1172 }
1173 
1174 void Assembler::adcl(Register dst, Address src) {
1175   InstructionMark im(this);
1176   prefix(src, dst);
1177   emit_int8(0x13);
1178   emit_operand(dst, src);
1179 }
1180 
1181 void Assembler::adcl(Register dst, Register src) {
1182   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1183   emit_arith(0x13, 0xC0, dst, src);
1184 }
1185 
1186 void Assembler::addl(Address dst, int32_t imm32) {
1187   InstructionMark im(this);
1188   prefix(dst);
1189   emit_arith_operand(0x81, rax, dst, imm32);
1190 }
1191 
1192 void Assembler::addb(Address dst, int imm8) {
1193   InstructionMark im(this);
1194   prefix(dst);
1195   emit_int8((unsigned char)0x80);
1196   emit_operand(rax, dst, 1);
1197   emit_int8(imm8);
1198 }
1199 
1200 void Assembler::addw(Address dst, int imm16) {
1201   InstructionMark im(this);
1202   emit_int8(0x66);
1203   prefix(dst);
1204   emit_int8((unsigned char)0x81);
1205   emit_operand(rax, dst, 2);
1206   emit_int16(imm16);
1207 }
1208 
1209 void Assembler::addl(Address dst, Register src) {
1210   InstructionMark im(this);
1211   prefix(dst, src);
1212   emit_int8(0x01);
1213   emit_operand(src, dst);
1214 }
1215 
1216 void Assembler::addl(Register dst, int32_t imm32) {
1217   prefix(dst);
1218   emit_arith(0x81, 0xC0, dst, imm32);
1219 }
1220 
1221 void Assembler::addl(Register dst, Address src) {
1222   InstructionMark im(this);
1223   prefix(src, dst);
1224   emit_int8(0x03);
1225   emit_operand(dst, src);
1226 }
1227 
1228 void Assembler::addl(Register dst, Register src) {
1229   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1230   emit_arith(0x03, 0xC0, dst, src);
1231 }
1232 
1233 void Assembler::addr_nop_4() {
1234   assert(UseAddressNop, "no CPU support");
1235   // 4 bytes: NOP DWORD PTR [EAX+0]
1236   emit_int8(0x0F);
1237   emit_int8(0x1F);
1238   emit_int8(0x40); // emit_rm(cbuf, 0x1, EAX_enc, EAX_enc);
1239   emit_int8(0);    // 8-bits offset (1 byte)
1240 }
1241 
1242 void Assembler::addr_nop_5() {
1243   assert(UseAddressNop, "no CPU support");
1244   // 5 bytes: NOP DWORD PTR [EAX+EAX*0+0] 8-bits offset
1245   emit_int8(0x0F);
1246   emit_int8(0x1F);
1247   emit_int8(0x44); // emit_rm(cbuf, 0x1, EAX_enc, 0x4);
1248   emit_int8(0x00); // emit_rm(cbuf, 0x0, EAX_enc, EAX_enc);
1249   emit_int8(0);    // 8-bits offset (1 byte)
1250 }
1251 
1252 void Assembler::addr_nop_7() {
1253   assert(UseAddressNop, "no CPU support");
1254   // 7 bytes: NOP DWORD PTR [EAX+0] 32-bits offset
1255   emit_int8(0x0F);
1256   emit_int8(0x1F);
1257   emit_int8((unsigned char)0x80);
1258                    // emit_rm(cbuf, 0x2, EAX_enc, EAX_enc);
1259   emit_int32(0);   // 32-bits offset (4 bytes)
1260 }
1261 
1262 void Assembler::addr_nop_8() {
1263   assert(UseAddressNop, "no CPU support");
1264   // 8 bytes: NOP DWORD PTR [EAX+EAX*0+0] 32-bits offset
1265   emit_int8(0x0F);
1266   emit_int8(0x1F);
1267   emit_int8((unsigned char)0x84);
1268                    // emit_rm(cbuf, 0x2, EAX_enc, 0x4);
1269   emit_int8(0x00); // emit_rm(cbuf, 0x0, EAX_enc, EAX_enc);
1270   emit_int32(0);   // 32-bits offset (4 bytes)
1271 }
1272 
1273 void Assembler::addsd(XMMRegister dst, XMMRegister src) {
1274   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1275   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1276   attributes.set_rex_vex_w_reverted();
1277   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1278   emit_int8(0x58);
1279   emit_int8((unsigned char)(0xC0 | encode));
1280 }
1281 
1282 void Assembler::addsd(XMMRegister dst, Address src) {
1283   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1284   InstructionMark im(this);
1285   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1286   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
1287   attributes.set_rex_vex_w_reverted();
1288   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1289   emit_int8(0x58);
1290   emit_operand(dst, src);
1291 }
1292 
1293 void Assembler::addss(XMMRegister dst, XMMRegister src) {
1294   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1295   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1296   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1297   emit_int8(0x58);
1298   emit_int8((unsigned char)(0xC0 | encode));
1299 }
1300 
1301 void Assembler::addss(XMMRegister dst, Address src) {
1302   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1303   InstructionMark im(this);
1304   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1305   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1306   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1307   emit_int8(0x58);
1308   emit_operand(dst, src);
1309 }
1310 
1311 void Assembler::aesdec(XMMRegister dst, Address src) {
1312   assert(VM_Version::supports_aes(), "");
1313   InstructionMark im(this);
1314   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1315   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1316   emit_int8((unsigned char)0xDE);
1317   emit_operand(dst, src);
1318 }
1319 
1320 void Assembler::aesdec(XMMRegister dst, XMMRegister src) {
1321   assert(VM_Version::supports_aes(), "");
1322   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1323   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1324   emit_int8((unsigned char)0xDE);
1325   emit_int8(0xC0 | encode);
1326 }
1327 
1328 void Assembler::vaesdec(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
1329   assert(VM_Version::supports_vaes(), "");
1330   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
1331   attributes.set_is_evex_instruction();
1332   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1333   emit_int8((unsigned char)0xDE);
1334   emit_int8((unsigned char)(0xC0 | encode));
1335 }
1336 
1337 
1338 void Assembler::aesdeclast(XMMRegister dst, Address src) {
1339   assert(VM_Version::supports_aes(), "");
1340   InstructionMark im(this);
1341   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1342   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1343   emit_int8((unsigned char)0xDF);
1344   emit_operand(dst, src);
1345 }
1346 
1347 void Assembler::aesdeclast(XMMRegister dst, XMMRegister src) {
1348   assert(VM_Version::supports_aes(), "");
1349   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1350   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1351   emit_int8((unsigned char)0xDF);
1352   emit_int8((unsigned char)(0xC0 | encode));
1353 }
1354 
1355 void Assembler::vaesdeclast(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
1356   assert(VM_Version::supports_vaes(), "");
1357   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
1358   attributes.set_is_evex_instruction();
1359   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1360   emit_int8((unsigned char)0xDF);
1361   emit_int8((unsigned char)(0xC0 | encode));
1362 }
1363 
1364 void Assembler::aesenc(XMMRegister dst, Address src) {
1365   assert(VM_Version::supports_aes(), "");
1366   InstructionMark im(this);
1367   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1368   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1369   emit_int8((unsigned char)0xDC);
1370   emit_operand(dst, src);
1371 }
1372 
1373 void Assembler::aesenc(XMMRegister dst, XMMRegister src) {
1374   assert(VM_Version::supports_aes(), "");
1375   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1376   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1377   emit_int8((unsigned char)0xDC);
1378   emit_int8(0xC0 | encode);
1379 }
1380 
1381 void Assembler::aesenclast(XMMRegister dst, Address src) {
1382   assert(VM_Version::supports_aes(), "");
1383   InstructionMark im(this);
1384   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1385   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1386   emit_int8((unsigned char)0xDD);
1387   emit_operand(dst, src);
1388 }
1389 
1390 void Assembler::aesenclast(XMMRegister dst, XMMRegister src) {
1391   assert(VM_Version::supports_aes(), "");
1392   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1393   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
1394   emit_int8((unsigned char)0xDD);
1395   emit_int8((unsigned char)(0xC0 | encode));
1396 }
1397 
1398 void Assembler::andl(Address dst, int32_t imm32) {
1399   InstructionMark im(this);
1400   prefix(dst);
1401   emit_int8((unsigned char)0x81);
1402   emit_operand(rsp, dst, 4);
1403   emit_int32(imm32);
1404 }
1405 
1406 void Assembler::andl(Register dst, int32_t imm32) {
1407   prefix(dst);
1408   emit_arith(0x81, 0xE0, dst, imm32);
1409 }
1410 
1411 void Assembler::andl(Register dst, Address src) {
1412   InstructionMark im(this);
1413   prefix(src, dst);
1414   emit_int8(0x23);
1415   emit_operand(dst, src);
1416 }
1417 
1418 void Assembler::andl(Register dst, Register src) {
1419   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1420   emit_arith(0x23, 0xC0, dst, src);
1421 }
1422 
1423 void Assembler::andnl(Register dst, Register src1, Register src2) {
1424   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1425   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1426   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1427   emit_int8((unsigned char)0xF2);
1428   emit_int8((unsigned char)(0xC0 | encode));
1429 }
1430 
1431 void Assembler::andnl(Register dst, Register src1, Address src2) {
1432   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1433   InstructionMark im(this);
1434   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1435   vex_prefix(src2, src1-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1436   emit_int8((unsigned char)0xF2);
1437   emit_operand(dst, src2);
1438 }
1439 
1440 void Assembler::bsfl(Register dst, Register src) {
1441   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1442   emit_int8(0x0F);
1443   emit_int8((unsigned char)0xBC);
1444   emit_int8((unsigned char)(0xC0 | encode));
1445 }
1446 
1447 void Assembler::bsrl(Register dst, Register src) {
1448   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1449   emit_int8(0x0F);
1450   emit_int8((unsigned char)0xBD);
1451   emit_int8((unsigned char)(0xC0 | encode));
1452 }
1453 
1454 void Assembler::bswapl(Register reg) { // bswap
1455   int encode = prefix_and_encode(reg-&gt;encoding());
1456   emit_int8(0x0F);
1457   emit_int8((unsigned char)(0xC8 | encode));
1458 }
1459 
1460 void Assembler::blsil(Register dst, Register src) {
1461   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1462   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1463   int encode = vex_prefix_and_encode(rbx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1464   emit_int8((unsigned char)0xF3);
1465   emit_int8((unsigned char)(0xC0 | encode));
1466 }
1467 
1468 void Assembler::blsil(Register dst, Address src) {
1469   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1470   InstructionMark im(this);
1471   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1472   vex_prefix(src, dst-&gt;encoding(), rbx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1473   emit_int8((unsigned char)0xF3);
1474   emit_operand(rbx, src);
1475 }
1476 
1477 void Assembler::blsmskl(Register dst, Register src) {
1478   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1479   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1480   int encode = vex_prefix_and_encode(rdx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1481   emit_int8((unsigned char)0xF3);
1482   emit_int8((unsigned char)(0xC0 | encode));
1483 }
1484 
1485 void Assembler::blsmskl(Register dst, Address src) {
1486   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1487   InstructionMark im(this);
1488   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1489   vex_prefix(src, dst-&gt;encoding(), rdx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1490   emit_int8((unsigned char)0xF3);
1491   emit_operand(rdx, src);
1492 }
1493 
1494 void Assembler::blsrl(Register dst, Register src) {
1495   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1496   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1497   int encode = vex_prefix_and_encode(rcx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1498   emit_int8((unsigned char)0xF3);
1499   emit_int8((unsigned char)(0xC0 | encode));
1500 }
1501 
1502 void Assembler::blsrl(Register dst, Address src) {
1503   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
1504   InstructionMark im(this);
1505   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
1506   vex_prefix(src, dst-&gt;encoding(), rcx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
1507   emit_int8((unsigned char)0xF3);
1508   emit_operand(rcx, src);
1509 }
1510 
1511 void Assembler::call(Label&amp; L, relocInfo::relocType rtype) {
1512   // suspect disp32 is always good
1513   int operand = LP64_ONLY(disp32_operand) NOT_LP64(imm_operand);
1514 
1515   if (L.is_bound()) {
1516     const int long_size = 5;
1517     int offs = (int)( target(L) - pc() );
1518     assert(offs &lt;= 0, "assembler error");
1519     InstructionMark im(this);
1520     // 1110 1000 #32-bit disp
1521     emit_int8((unsigned char)0xE8);
1522     emit_data(offs - long_size, rtype, operand);
1523   } else {
1524     InstructionMark im(this);
1525     // 1110 1000 #32-bit disp
1526     L.add_patch_at(code(), locator());
1527 
1528     emit_int8((unsigned char)0xE8);
1529     emit_data(int(0), rtype, operand);
1530   }
1531 }
1532 
1533 void Assembler::call(Register dst) {
1534   int encode = prefix_and_encode(dst-&gt;encoding());
1535   emit_int8((unsigned char)0xFF);
1536   emit_int8((unsigned char)(0xD0 | encode));
1537 }
1538 
1539 
1540 void Assembler::call(Address adr) {
1541   InstructionMark im(this);
1542   prefix(adr);
1543   emit_int8((unsigned char)0xFF);
1544   emit_operand(rdx, adr);
1545 }
1546 
1547 void Assembler::call_literal(address entry, RelocationHolder const&amp; rspec) {
1548   InstructionMark im(this);
1549   emit_int8((unsigned char)0xE8);
1550   intptr_t disp = entry - (pc() + sizeof(int32_t));
1551   // Entry is NULL in case of a scratch emit.
1552   assert(entry == NULL || is_simm32(disp), "disp=" INTPTR_FORMAT " must be 32bit offset (call2)", disp);
1553   // Technically, should use call32_operand, but this format is
1554   // implied by the fact that we're emitting a call instruction.
1555 
1556   int operand = LP64_ONLY(disp32_operand) NOT_LP64(call32_operand);
1557   emit_data((int) disp, rspec, operand);
1558 }
1559 
1560 void Assembler::cdql() {
1561   emit_int8((unsigned char)0x99);
1562 }
1563 
1564 void Assembler::cld() {
1565   emit_int8((unsigned char)0xFC);
1566 }
1567 
1568 void Assembler::cmovl(Condition cc, Register dst, Register src) {
1569   NOT_LP64(guarantee(VM_Version::supports_cmov(), "illegal instruction"));
1570   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1571   emit_int8(0x0F);
1572   emit_int8(0x40 | cc);
1573   emit_int8((unsigned char)(0xC0 | encode));
1574 }
1575 
1576 
1577 void Assembler::cmovl(Condition cc, Register dst, Address src) {
1578   NOT_LP64(guarantee(VM_Version::supports_cmov(), "illegal instruction"));
1579   prefix(src, dst);
1580   emit_int8(0x0F);
1581   emit_int8(0x40 | cc);
1582   emit_operand(dst, src);
1583 }
1584 
1585 void Assembler::cmpb(Address dst, int imm8) {
1586   InstructionMark im(this);
1587   prefix(dst);
1588   emit_int8((unsigned char)0x80);
1589   emit_operand(rdi, dst, 1);
1590   emit_int8(imm8);
1591 }
1592 
1593 void Assembler::cmpl(Address dst, int32_t imm32) {
1594   InstructionMark im(this);
1595   prefix(dst);
1596   emit_int8((unsigned char)0x81);
1597   emit_operand(rdi, dst, 4);
1598   emit_int32(imm32);
1599 }
1600 
1601 void Assembler::cmpl(Register dst, int32_t imm32) {
1602   prefix(dst);
1603   emit_arith(0x81, 0xF8, dst, imm32);
1604 }
1605 
1606 void Assembler::cmpl(Register dst, Register src) {
1607   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1608   emit_arith(0x3B, 0xC0, dst, src);
1609 }
1610 
1611 void Assembler::cmpl(Register dst, Address  src) {
1612   InstructionMark im(this);
1613   prefix(src, dst);
1614   emit_int8((unsigned char)0x3B);
1615   emit_operand(dst, src);
1616 }
1617 
1618 void Assembler::cmpw(Address dst, int imm16) {
1619   InstructionMark im(this);
1620   assert(!dst.base_needs_rex() &amp;&amp; !dst.index_needs_rex(), "no extended registers");
1621   emit_int8(0x66);
1622   emit_int8((unsigned char)0x81);
1623   emit_operand(rdi, dst, 2);
1624   emit_int16(imm16);
1625 }
1626 
1627 // The 32-bit cmpxchg compares the value at adr with the contents of rax,
1628 // and stores reg into adr if so; otherwise, the value at adr is loaded into rax,.
1629 // The ZF is set if the compared values were equal, and cleared otherwise.
1630 void Assembler::cmpxchgl(Register reg, Address adr) { // cmpxchg
1631   InstructionMark im(this);
1632   prefix(adr, reg);
1633   emit_int8(0x0F);
1634   emit_int8((unsigned char)0xB1);
1635   emit_operand(reg, adr);
1636 }
1637 
1638 // The 8-bit cmpxchg compares the value at adr with the contents of rax,
1639 // and stores reg into adr if so; otherwise, the value at adr is loaded into rax,.
1640 // The ZF is set if the compared values were equal, and cleared otherwise.
1641 void Assembler::cmpxchgb(Register reg, Address adr) { // cmpxchg
1642   InstructionMark im(this);
1643   prefix(adr, reg, true);
1644   emit_int8(0x0F);
1645   emit_int8((unsigned char)0xB0);
1646   emit_operand(reg, adr);
1647 }
1648 
1649 void Assembler::comisd(XMMRegister dst, Address src) {
1650   // NOTE: dbx seems to decode this as comiss even though the
1651   // 0x66 is there. Strangly ucomisd comes out correct
1652   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1653   InstructionMark im(this);
1654   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);;
1655   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
1656   attributes.set_rex_vex_w_reverted();
1657   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
1658   emit_int8(0x2F);
1659   emit_operand(dst, src);
1660 }
1661 
1662 void Assembler::comisd(XMMRegister dst, XMMRegister src) {
1663   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1664   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1665   attributes.set_rex_vex_w_reverted();
1666   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
1667   emit_int8(0x2F);
1668   emit_int8((unsigned char)(0xC0 | encode));
1669 }
1670 
1671 void Assembler::comiss(XMMRegister dst, Address src) {
1672   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1673   InstructionMark im(this);
1674   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1675   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1676   simd_prefix(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
1677   emit_int8(0x2F);
1678   emit_operand(dst, src);
1679 }
1680 
1681 void Assembler::comiss(XMMRegister dst, XMMRegister src) {
1682   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1683   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1684   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
1685   emit_int8(0x2F);
1686   emit_int8((unsigned char)(0xC0 | encode));
1687 }
1688 
1689 void Assembler::cpuid() {
1690   emit_int8(0x0F);
1691   emit_int8((unsigned char)0xA2);
1692 }
1693 
1694 // Opcode / Instruction                      Op /  En  64 - Bit Mode     Compat / Leg Mode Description                  Implemented
1695 // F2 0F 38 F0 / r       CRC32 r32, r / m8   RM        Valid             Valid             Accumulate CRC32 on r / m8.  v
1696 // F2 REX 0F 38 F0 / r   CRC32 r32, r / m8*  RM        Valid             N.E.              Accumulate CRC32 on r / m8.  -
1697 // F2 REX.W 0F 38 F0 / r CRC32 r64, r / m8   RM        Valid             N.E.              Accumulate CRC32 on r / m8.  -
1698 //
1699 // F2 0F 38 F1 / r       CRC32 r32, r / m16  RM        Valid             Valid             Accumulate CRC32 on r / m16. v
1700 //
1701 // F2 0F 38 F1 / r       CRC32 r32, r / m32  RM        Valid             Valid             Accumulate CRC32 on r / m32. v
1702 //
1703 // F2 REX.W 0F 38 F1 / r CRC32 r64, r / m64  RM        Valid             N.E.              Accumulate CRC32 on r / m64. v
1704 void Assembler::crc32(Register crc, Register v, int8_t sizeInBytes) {
1705   assert(VM_Version::supports_sse4_2(), "");
1706   int8_t w = 0x01;
1707   Prefix p = Prefix_EMPTY;
1708 
1709   emit_int8((int8_t)0xF2);
1710   switch (sizeInBytes) {
1711   case 1:
1712     w = 0;
1713     break;
1714   case 2:
1715   case 4:
1716     break;
1717   LP64_ONLY(case 8:)
1718     // This instruction is not valid in 32 bits
1719     // Note:
1720     // http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf
1721     //
1722     // Page B - 72   Vol. 2C says
1723     // qwreg2 to qwreg            1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : 11 qwreg1 qwreg2
1724     // mem64 to qwreg             1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : mod qwreg r / m
1725     //                                                                            F0!!!
1726     // while 3 - 208 Vol. 2A
1727     // F2 REX.W 0F 38 F1 / r       CRC32 r64, r / m64             RM         Valid      N.E.Accumulate CRC32 on r / m64.
1728     //
1729     // the 0 on a last bit is reserved for a different flavor of this instruction :
1730     // F2 REX.W 0F 38 F0 / r       CRC32 r64, r / m8              RM         Valid      N.E.Accumulate CRC32 on r / m8.
1731     p = REX_W;
1732     break;
1733   default:
1734     assert(0, "Unsupported value for a sizeInBytes argument");
1735     break;
1736   }
1737   LP64_ONLY(prefix(crc, v, p);)
1738   emit_int8((int8_t)0x0F);
1739   emit_int8(0x38);
1740   emit_int8((int8_t)(0xF0 | w));
1741   emit_int8(0xC0 | ((crc-&gt;encoding() &amp; 0x7) &lt;&lt; 3) | (v-&gt;encoding() &amp; 7));
1742 }
1743 
1744 void Assembler::crc32(Register crc, Address adr, int8_t sizeInBytes) {
1745   assert(VM_Version::supports_sse4_2(), "");
1746   InstructionMark im(this);
1747   int8_t w = 0x01;
1748   Prefix p = Prefix_EMPTY;
1749 
1750   emit_int8((int8_t)0xF2);
1751   switch (sizeInBytes) {
1752   case 1:
1753     w = 0;
1754     break;
1755   case 2:
1756   case 4:
1757     break;
1758   LP64_ONLY(case 8:)
1759     // This instruction is not valid in 32 bits
1760     p = REX_W;
1761     break;
1762   default:
1763     assert(0, "Unsupported value for a sizeInBytes argument");
1764     break;
1765   }
1766   LP64_ONLY(prefix(crc, adr, p);)
1767   emit_int8((int8_t)0x0F);
1768   emit_int8(0x38);
1769   emit_int8((int8_t)(0xF0 | w));
1770   emit_operand(crc, adr);
1771 }
1772 
1773 void Assembler::cvtdq2pd(XMMRegister dst, XMMRegister src) {
1774   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1775   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
1776   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1777   emit_int8((unsigned char)0xE6);
1778   emit_int8((unsigned char)(0xC0 | encode));
1779 }
1780 
1781 void Assembler::cvtdq2ps(XMMRegister dst, XMMRegister src) {
1782   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1783   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
1784   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
1785   emit_int8(0x5B);
1786   emit_int8((unsigned char)(0xC0 | encode));
1787 }
1788 
1789 void Assembler::cvtsd2ss(XMMRegister dst, XMMRegister src) {
1790   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1791   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1792   attributes.set_rex_vex_w_reverted();
1793   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1794   emit_int8(0x5A);
1795   emit_int8((unsigned char)(0xC0 | encode));
1796 }
1797 
1798 void Assembler::cvtsd2ss(XMMRegister dst, Address src) {
1799   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1800   InstructionMark im(this);
1801   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1802   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
1803   attributes.set_rex_vex_w_reverted();
1804   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1805   emit_int8(0x5A);
1806   emit_operand(dst, src);
1807 }
1808 
1809 void Assembler::cvtsi2sdl(XMMRegister dst, Register src) {
1810   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1811   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1812   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1813   emit_int8(0x2A);
1814   emit_int8((unsigned char)(0xC0 | encode));
1815 }
1816 
1817 void Assembler::cvtsi2sdl(XMMRegister dst, Address src) {
1818   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1819   InstructionMark im(this);
1820   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1821   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1822   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1823   emit_int8(0x2A);
1824   emit_operand(dst, src);
1825 }
1826 
1827 void Assembler::cvtsi2ssl(XMMRegister dst, Register src) {
1828   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1829   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1830   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1831   emit_int8(0x2A);
1832   emit_int8((unsigned char)(0xC0 | encode));
1833 }
1834 
1835 void Assembler::cvtsi2ssl(XMMRegister dst, Address src) {
1836   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1837   InstructionMark im(this);
1838   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1839   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1840   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1841   emit_int8(0x2A);
1842   emit_operand(dst, src);
1843 }
1844 
1845 void Assembler::cvtsi2ssq(XMMRegister dst, Register src) {
1846   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1847   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1848   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1849   emit_int8(0x2A);
1850   emit_int8((unsigned char)(0xC0 | encode));
1851 }
1852 
1853 void Assembler::cvtss2sd(XMMRegister dst, XMMRegister src) {
1854   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1855   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1856   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1857   emit_int8(0x5A);
1858   emit_int8((unsigned char)(0xC0 | encode));
1859 }
1860 
1861 void Assembler::cvtss2sd(XMMRegister dst, Address src) {
1862   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1863   InstructionMark im(this);
1864   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1865   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1866   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1867   emit_int8(0x5A);
1868   emit_operand(dst, src);
1869 }
1870 
1871 
1872 void Assembler::cvttsd2sil(Register dst, XMMRegister src) {
1873   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1874   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1875   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1876   emit_int8(0x2C);
1877   emit_int8((unsigned char)(0xC0 | encode));
1878 }
1879 
1880 void Assembler::cvttss2sil(Register dst, XMMRegister src) {
1881   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1882   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1883   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1884   emit_int8(0x2C);
1885   emit_int8((unsigned char)(0xC0 | encode));
1886 }
1887 
1888 void Assembler::cvttpd2dq(XMMRegister dst, XMMRegister src) {
1889   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1890   int vector_len = VM_Version::supports_avx512novl() ? AVX_512bit : AVX_128bit;
1891   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
1892   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
1893   emit_int8((unsigned char)0xE6);
1894   emit_int8((unsigned char)(0xC0 | encode));
1895 }
1896 
1897 void Assembler::decl(Address dst) {
1898   // Don't use it directly. Use MacroAssembler::decrement() instead.
1899   InstructionMark im(this);
1900   prefix(dst);
1901   emit_int8((unsigned char)0xFF);
1902   emit_operand(rcx, dst);
1903 }
1904 
1905 void Assembler::divsd(XMMRegister dst, Address src) {
1906   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1907   InstructionMark im(this);
1908   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1909   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
1910   attributes.set_rex_vex_w_reverted();
1911   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1912   emit_int8(0x5E);
1913   emit_operand(dst, src);
1914 }
1915 
1916 void Assembler::divsd(XMMRegister dst, XMMRegister src) {
1917   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
1918   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1919   attributes.set_rex_vex_w_reverted();
1920   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
1921   emit_int8(0x5E);
1922   emit_int8((unsigned char)(0xC0 | encode));
1923 }
1924 
1925 void Assembler::divss(XMMRegister dst, Address src) {
1926   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1927   InstructionMark im(this);
1928   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1929   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
1930   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1931   emit_int8(0x5E);
1932   emit_operand(dst, src);
1933 }
1934 
1935 void Assembler::divss(XMMRegister dst, XMMRegister src) {
1936   NOT_LP64(assert(VM_Version::supports_sse(), ""));
1937   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
1938   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
1939   emit_int8(0x5E);
1940   emit_int8((unsigned char)(0xC0 | encode));
1941 }
1942 
1943 void Assembler::emms() {
1944   NOT_LP64(assert(VM_Version::supports_mmx(), ""));
1945   emit_int8(0x0F);
1946   emit_int8(0x77);
1947 }
1948 
1949 void Assembler::hlt() {
1950   emit_int8((unsigned char)0xF4);
1951 }
1952 
1953 void Assembler::idivl(Register src) {
1954   int encode = prefix_and_encode(src-&gt;encoding());
1955   emit_int8((unsigned char)0xF7);
1956   emit_int8((unsigned char)(0xF8 | encode));
1957 }
1958 
1959 void Assembler::divl(Register src) { // Unsigned
1960   int encode = prefix_and_encode(src-&gt;encoding());
1961   emit_int8((unsigned char)0xF7);
1962   emit_int8((unsigned char)(0xF0 | encode));
1963 }
1964 
1965 void Assembler::imull(Register src) {
1966   int encode = prefix_and_encode(src-&gt;encoding());
1967   emit_int8((unsigned char)0xF7);
1968   emit_int8((unsigned char)(0xE8 | encode));
1969 }
1970 
1971 void Assembler::imull(Register dst, Register src) {
1972   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1973   emit_int8(0x0F);
1974   emit_int8((unsigned char)0xAF);
1975   emit_int8((unsigned char)(0xC0 | encode));
1976 }
1977 
1978 
1979 void Assembler::imull(Register dst, Register src, int value) {
1980   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
1981   if (is8bit(value)) {
1982     emit_int8(0x6B);
1983     emit_int8((unsigned char)(0xC0 | encode));
1984     emit_int8(value &amp; 0xFF);
1985   } else {
1986     emit_int8(0x69);
1987     emit_int8((unsigned char)(0xC0 | encode));
1988     emit_int32(value);
1989   }
1990 }
1991 
1992 void Assembler::imull(Register dst, Address src) {
1993   InstructionMark im(this);
1994   prefix(src, dst);
1995   emit_int8(0x0F);
1996   emit_int8((unsigned char) 0xAF);
1997   emit_operand(dst, src);
1998 }
1999 
2000 
2001 void Assembler::incl(Address dst) {
2002   // Don't use it directly. Use MacroAssembler::increment() instead.
2003   InstructionMark im(this);
2004   prefix(dst);
2005   emit_int8((unsigned char)0xFF);
2006   emit_operand(rax, dst);
2007 }
2008 
2009 void Assembler::jcc(Condition cc, Label&amp; L, bool maybe_short) {
2010   InstructionMark im(this);
2011   assert((0 &lt;= cc) &amp;&amp; (cc &lt; 16), "illegal cc");
2012   if (L.is_bound()) {
2013     address dst = target(L);
2014     assert(dst != NULL, "jcc most probably wrong");
2015 
2016     const int short_size = 2;
2017     const int long_size = 6;
2018     intptr_t offs = (intptr_t)dst - (intptr_t)pc();
2019     if (maybe_short &amp;&amp; is8bit(offs - short_size)) {
2020       // 0111 tttn #8-bit disp
2021       emit_int8(0x70 | cc);
2022       emit_int8((offs - short_size) &amp; 0xFF);
2023     } else {
2024       // 0000 1111 1000 tttn #32-bit disp
2025       assert(is_simm32(offs - long_size),
2026              "must be 32bit offset (call4)");
2027       emit_int8(0x0F);
2028       emit_int8((unsigned char)(0x80 | cc));
2029       emit_int32(offs - long_size);
2030     }
2031   } else {
2032     // Note: could eliminate cond. jumps to this jump if condition
2033     //       is the same however, seems to be rather unlikely case.
2034     // Note: use jccb() if label to be bound is very close to get
2035     //       an 8-bit displacement
2036     L.add_patch_at(code(), locator());
2037     emit_int8(0x0F);
2038     emit_int8((unsigned char)(0x80 | cc));
2039     emit_int32(0);
2040   }
2041 }
2042 
2043 void Assembler::jccb_0(Condition cc, Label&amp; L, const char* file, int line) {
2044   if (L.is_bound()) {
2045     const int short_size = 2;
2046     address entry = target(L);
2047 #ifdef ASSERT
2048     intptr_t dist = (intptr_t)entry - ((intptr_t)pc() + short_size);
2049     intptr_t delta = short_branch_delta();
2050     if (delta != 0) {
2051       dist += (dist &lt; 0 ? (-delta) :delta);
2052     }
2053     assert(is8bit(dist), "Dispacement too large for a short jmp at %s:%d", file, line);
2054 #endif
2055     intptr_t offs = (intptr_t)entry - (intptr_t)pc();
2056     // 0111 tttn #8-bit disp
2057     emit_int8(0x70 | cc);
2058     emit_int8((offs - short_size) &amp; 0xFF);
2059   } else {
2060     InstructionMark im(this);
2061     L.add_patch_at(code(), locator(), file, line);
2062     emit_int8(0x70 | cc);
2063     emit_int8(0);
2064   }
2065 }
2066 
2067 void Assembler::jmp(Address adr) {
2068   InstructionMark im(this);
2069   prefix(adr);
2070   emit_int8((unsigned char)0xFF);
2071   emit_operand(rsp, adr);
2072 }
2073 
2074 void Assembler::jmp(Label&amp; L, bool maybe_short) {
2075   if (L.is_bound()) {
2076     address entry = target(L);
2077     assert(entry != NULL, "jmp most probably wrong");
2078     InstructionMark im(this);
2079     const int short_size = 2;
2080     const int long_size = 5;
2081     intptr_t offs = entry - pc();
2082     if (maybe_short &amp;&amp; is8bit(offs - short_size)) {
2083       emit_int8((unsigned char)0xEB);
2084       emit_int8((offs - short_size) &amp; 0xFF);
2085     } else {
2086       emit_int8((unsigned char)0xE9);
2087       emit_int32(offs - long_size);
2088     }
2089   } else {
2090     // By default, forward jumps are always 32-bit displacements, since
2091     // we can't yet know where the label will be bound.  If you're sure that
2092     // the forward jump will not run beyond 256 bytes, use jmpb to
2093     // force an 8-bit displacement.
2094     InstructionMark im(this);
2095     L.add_patch_at(code(), locator());
2096     emit_int8((unsigned char)0xE9);
2097     emit_int32(0);
2098   }
2099 }
2100 
2101 void Assembler::jmp(Register entry) {
2102   int encode = prefix_and_encode(entry-&gt;encoding());
2103   emit_int8((unsigned char)0xFF);
2104   emit_int8((unsigned char)(0xE0 | encode));
2105 }
2106 
2107 void Assembler::jmp_literal(address dest, RelocationHolder const&amp; rspec) {
2108   InstructionMark im(this);
2109   emit_int8((unsigned char)0xE9);
2110   assert(dest != NULL, "must have a target");
2111   intptr_t disp = dest - (pc() + sizeof(int32_t));
2112   assert(is_simm32(disp), "must be 32bit offset (jmp)");
2113   emit_data(disp, rspec.reloc(), call32_operand);
2114 }
2115 
2116 void Assembler::jmpb_0(Label&amp; L, const char* file, int line) {
2117   if (L.is_bound()) {
2118     const int short_size = 2;
2119     address entry = target(L);
2120     assert(entry != NULL, "jmp most probably wrong");
2121 #ifdef ASSERT
2122     intptr_t dist = (intptr_t)entry - ((intptr_t)pc() + short_size);
2123     intptr_t delta = short_branch_delta();
2124     if (delta != 0) {
2125       dist += (dist &lt; 0 ? (-delta) :delta);
2126     }
2127     assert(is8bit(dist), "Dispacement too large for a short jmp at %s:%d", file, line);
2128 #endif
2129     intptr_t offs = entry - pc();
2130     emit_int8((unsigned char)0xEB);
2131     emit_int8((offs - short_size) &amp; 0xFF);
2132   } else {
2133     InstructionMark im(this);
2134     L.add_patch_at(code(), locator(), file, line);
2135     emit_int8((unsigned char)0xEB);
2136     emit_int8(0);
2137   }
2138 }
2139 
2140 void Assembler::ldmxcsr( Address src) {
2141   if (UseAVX &gt; 0 ) {
2142     InstructionMark im(this);
2143     InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2144     vex_prefix(src, 0, 0, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2145     emit_int8((unsigned char)0xAE);
2146     emit_operand(as_Register(2), src);
2147   } else {
2148     NOT_LP64(assert(VM_Version::supports_sse(), ""));
2149     InstructionMark im(this);
2150     prefix(src);
2151     emit_int8(0x0F);
2152     emit_int8((unsigned char)0xAE);
2153     emit_operand(as_Register(2), src);
2154   }
2155 }
2156 
2157 void Assembler::leal(Register dst, Address src) {
2158   InstructionMark im(this);
2159 #ifdef _LP64
2160   emit_int8(0x67); // addr32
2161   prefix(src, dst);
2162 #endif // LP64
2163   emit_int8((unsigned char)0x8D);
2164   emit_operand(dst, src);
2165 }
2166 
2167 void Assembler::lfence() {
2168   emit_int8(0x0F);
2169   emit_int8((unsigned char)0xAE);
2170   emit_int8((unsigned char)0xE8);
2171 }
2172 
2173 void Assembler::lock() {
2174   emit_int8((unsigned char)0xF0);
2175 }
2176 
2177 void Assembler::lzcntl(Register dst, Register src) {
2178   assert(VM_Version::supports_lzcnt(), "encoding is treated as BSR");
2179   emit_int8((unsigned char)0xF3);
2180   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
2181   emit_int8(0x0F);
2182   emit_int8((unsigned char)0xBD);
2183   emit_int8((unsigned char)(0xC0 | encode));
2184 }
2185 
2186 // Emit mfence instruction
2187 void Assembler::mfence() {
2188   NOT_LP64(assert(VM_Version::supports_sse2(), "unsupported");)
2189   emit_int8(0x0F);
2190   emit_int8((unsigned char)0xAE);
2191   emit_int8((unsigned char)0xF0);
2192 }
2193 
2194 void Assembler::mov(Register dst, Register src) {
2195   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
2196 }
2197 
2198 void Assembler::movapd(XMMRegister dst, XMMRegister src) {
2199   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2200   int vector_len = VM_Version::supports_avx512novl() ? AVX_512bit : AVX_128bit;
2201   InstructionAttr attributes(vector_len, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2202   attributes.set_rex_vex_w_reverted();
2203   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2204   emit_int8(0x28);
2205   emit_int8((unsigned char)(0xC0 | encode));
2206 }
2207 
2208 void Assembler::movaps(XMMRegister dst, XMMRegister src) {
2209   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2210   int vector_len = VM_Version::supports_avx512novl() ? AVX_512bit : AVX_128bit;
2211   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2212   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2213   emit_int8(0x28);
2214   emit_int8((unsigned char)(0xC0 | encode));
2215 }
2216 
2217 void Assembler::movlhps(XMMRegister dst, XMMRegister src) {
2218   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2219   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2220   int encode = simd_prefix_and_encode(dst, src, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2221   emit_int8(0x16);
2222   emit_int8((unsigned char)(0xC0 | encode));
2223 }
2224 
2225 void Assembler::movb(Register dst, Address src) {
2226   NOT_LP64(assert(dst-&gt;has_byte_register(), "must have byte register"));
2227   InstructionMark im(this);
2228   prefix(src, dst, true);
2229   emit_int8((unsigned char)0x8A);
2230   emit_operand(dst, src);
2231 }
2232 
2233 void Assembler::movddup(XMMRegister dst, XMMRegister src) {
2234   NOT_LP64(assert(VM_Version::supports_sse3(), ""));
2235   int vector_len = VM_Version::supports_avx512novl() ? AVX_512bit : AVX_128bit;
2236   InstructionAttr attributes(vector_len, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2237   attributes.set_rex_vex_w_reverted();
2238   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2239   emit_int8(0x12);
2240   emit_int8(0xC0 | encode);
2241 }
2242 
2243 void Assembler::kmovbl(KRegister dst, Register src) {
2244   assert(VM_Version::supports_avx512dq(), "");
2245   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2246   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2247   emit_int8((unsigned char)0x92);
2248   emit_int8((unsigned char)(0xC0 | encode));
2249 }
2250 
2251 void Assembler::kmovbl(Register dst, KRegister src) {
2252   assert(VM_Version::supports_avx512dq(), "");
2253   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2254   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2255   emit_int8((unsigned char)0x93);
2256   emit_int8((unsigned char)(0xC0 | encode));
2257 }
2258 
2259 void Assembler::kmovwl(KRegister dst, Register src) {
2260   assert(VM_Version::supports_evex(), "");
2261   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2262   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2263   emit_int8((unsigned char)0x92);
2264   emit_int8((unsigned char)(0xC0 | encode));
2265 }
2266 
2267 void Assembler::kmovwl(Register dst, KRegister src) {
2268   assert(VM_Version::supports_evex(), "");
2269   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2270   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2271   emit_int8((unsigned char)0x93);
2272   emit_int8((unsigned char)(0xC0 | encode));
2273 }
2274 
2275 void Assembler::kmovwl(KRegister dst, Address src) {
2276   assert(VM_Version::supports_evex(), "");
2277   InstructionMark im(this);
2278   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2279   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2280   emit_int8((unsigned char)0x90);
2281   emit_operand((Register)dst, src);
2282 }
2283 
2284 void Assembler::kmovdl(KRegister dst, Register src) {
2285   assert(VM_Version::supports_avx512bw(), "");
2286   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2287   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2288   emit_int8((unsigned char)0x92);
2289   emit_int8((unsigned char)(0xC0 | encode));
2290 }
2291 
2292 void Assembler::kmovdl(Register dst, KRegister src) {
2293   assert(VM_Version::supports_avx512bw(), "");
2294   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2295   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2296   emit_int8((unsigned char)0x93);
2297   emit_int8((unsigned char)(0xC0 | encode));
2298 }
2299 
2300 void Assembler::kmovql(KRegister dst, KRegister src) {
2301   assert(VM_Version::supports_avx512bw(), "");
2302   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2303   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2304   emit_int8((unsigned char)0x90);
2305   emit_int8((unsigned char)(0xC0 | encode));
2306 }
2307 
2308 void Assembler::kmovql(KRegister dst, Address src) {
2309   assert(VM_Version::supports_avx512bw(), "");
2310   InstructionMark im(this);
2311   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2312   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2313   emit_int8((unsigned char)0x90);
2314   emit_operand((Register)dst, src);
2315 }
2316 
2317 void Assembler::kmovql(Address dst, KRegister src) {
2318   assert(VM_Version::supports_avx512bw(), "");
2319   InstructionMark im(this);
2320   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2321   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2322   emit_int8((unsigned char)0x90);
2323   emit_operand((Register)src, dst);
2324 }
2325 
2326 void Assembler::kmovql(KRegister dst, Register src) {
2327   assert(VM_Version::supports_avx512bw(), "");
2328   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2329   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2330   emit_int8((unsigned char)0x92);
2331   emit_int8((unsigned char)(0xC0 | encode));
2332 }
2333 
2334 void Assembler::kmovql(Register dst, KRegister src) {
2335   assert(VM_Version::supports_avx512bw(), "");
2336   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2337   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2338   emit_int8((unsigned char)0x93);
2339   emit_int8((unsigned char)(0xC0 | encode));
2340 }
2341 
2342 void Assembler::knotwl(KRegister dst, KRegister src) {
2343   assert(VM_Version::supports_evex(), "");
2344   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2345   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2346   emit_int8((unsigned char)0x44);
2347   emit_int8((unsigned char)(0xC0 | encode));
2348 }
2349 
2350 // This instruction produces ZF or CF flags
2351 void Assembler::kortestbl(KRegister src1, KRegister src2) {
2352   assert(VM_Version::supports_avx512dq(), "");
2353   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2354   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2355   emit_int8((unsigned char)0x98);
2356   emit_int8((unsigned char)(0xC0 | encode));
2357 }
2358 
2359 // This instruction produces ZF or CF flags
2360 void Assembler::kortestwl(KRegister src1, KRegister src2) {
2361   assert(VM_Version::supports_evex(), "");
2362   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2363   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2364   emit_int8((unsigned char)0x98);
2365   emit_int8((unsigned char)(0xC0 | encode));
2366 }
2367 
2368 // This instruction produces ZF or CF flags
2369 void Assembler::kortestdl(KRegister src1, KRegister src2) {
2370   assert(VM_Version::supports_avx512bw(), "");
2371   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2372   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2373   emit_int8((unsigned char)0x98);
2374   emit_int8((unsigned char)(0xC0 | encode));
2375 }
2376 
2377 // This instruction produces ZF or CF flags
2378 void Assembler::kortestql(KRegister src1, KRegister src2) {
2379   assert(VM_Version::supports_avx512bw(), "");
2380   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2381   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2382   emit_int8((unsigned char)0x98);
2383   emit_int8((unsigned char)(0xC0 | encode));
2384 }
2385 
2386 // This instruction produces ZF or CF flags
2387 void Assembler::ktestql(KRegister src1, KRegister src2) {
2388   assert(VM_Version::supports_avx512bw(), "");
2389   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2390   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2391   emit_int8((unsigned char)0x99);
2392   emit_int8((unsigned char)(0xC0 | encode));
2393 }
2394 
2395 void Assembler::ktestq(KRegister src1, KRegister src2) {
2396   assert(VM_Version::supports_avx512bw(), "");
2397   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2398   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
2399   emit_int8((unsigned char)0x99);
2400   emit_int8((unsigned char)(0xC0 | encode));
2401 }
2402 
2403 void Assembler::ktestd(KRegister src1, KRegister src2) {
2404   assert(VM_Version::supports_avx512bw(), "");
2405   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
2406   int encode = vex_prefix_and_encode(src1-&gt;encoding(), 0, src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2407   emit_int8((unsigned char)0x99);
2408   emit_int8((unsigned char)(0xC0 | encode));
2409 }
2410 
2411 void Assembler::movb(Address dst, int imm8) {
2412   InstructionMark im(this);
2413    prefix(dst);
2414   emit_int8((unsigned char)0xC6);
2415   emit_operand(rax, dst, 1);
2416   emit_int8(imm8);
2417 }
2418 
2419 
2420 void Assembler::movb(Address dst, Register src) {
2421   assert(src-&gt;has_byte_register(), "must have byte register");
2422   InstructionMark im(this);
2423   prefix(dst, src, true);
2424   emit_int8((unsigned char)0x88);
2425   emit_operand(src, dst);
2426 }
2427 
2428 void Assembler::movdl(XMMRegister dst, Register src) {
2429   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2430   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2431   int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2432   emit_int8(0x6E);
2433   emit_int8((unsigned char)(0xC0 | encode));
2434 }
2435 
2436 void Assembler::movdl(Register dst, XMMRegister src) {
2437   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2438   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2439   // swap src/dst to get correct prefix
2440   int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2441   emit_int8(0x7E);
2442   emit_int8((unsigned char)(0xC0 | encode));
2443 }
2444 
2445 void Assembler::movdl(XMMRegister dst, Address src) {
2446   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2447   InstructionMark im(this);
2448   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2449   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
2450   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2451   emit_int8(0x6E);
2452   emit_operand(dst, src);
2453 }
2454 
2455 void Assembler::movdl(Address dst, XMMRegister src) {
2456   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2457   InstructionMark im(this);
2458   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2459   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
2460   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2461   emit_int8(0x7E);
2462   emit_operand(src, dst);
2463 }
2464 
2465 void Assembler::movdqa(XMMRegister dst, XMMRegister src) {
2466   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2467   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2468   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2469   emit_int8(0x6F);
2470   emit_int8((unsigned char)(0xC0 | encode));
2471 }
2472 
2473 void Assembler::movdqa(XMMRegister dst, Address src) {
2474   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2475   InstructionMark im(this);
2476   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2477   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2478   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2479   emit_int8(0x6F);
2480   emit_operand(dst, src);
2481 }
2482 
2483 void Assembler::movdqu(XMMRegister dst, Address src) {
2484   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2485   InstructionMark im(this);
2486   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2487   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2488   simd_prefix(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2489   emit_int8(0x6F);
2490   emit_operand(dst, src);
2491 }
2492 
2493 void Assembler::movdqu(XMMRegister dst, XMMRegister src) {
2494   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2495   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2496   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2497   emit_int8(0x6F);
2498   emit_int8((unsigned char)(0xC0 | encode));
2499 }
2500 
2501 void Assembler::movdqu(Address dst, XMMRegister src) {
2502   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2503   InstructionMark im(this);
2504   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2505   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2506   attributes.reset_is_clear_context();
2507   simd_prefix(src, xnoreg, dst, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2508   emit_int8(0x7F);
2509   emit_operand(src, dst);
2510 }
2511 
2512 // Move Unaligned 256bit Vector
2513 void Assembler::vmovdqu(XMMRegister dst, XMMRegister src) {
2514   assert(UseAVX &gt; 0, "");
2515   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2516   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2517   emit_int8(0x6F);
2518   emit_int8((unsigned char)(0xC0 | encode));
2519 }
2520 
2521 void Assembler::vmovdqu(XMMRegister dst, Address src) {
2522   assert(UseAVX &gt; 0, "");
2523   InstructionMark im(this);
2524   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2525   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2526   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2527   emit_int8(0x6F);
2528   emit_operand(dst, src);
2529 }
2530 
2531 void Assembler::vmovdqu(Address dst, XMMRegister src) {
2532   assert(UseAVX &gt; 0, "");
2533   InstructionMark im(this);
2534   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2535   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2536   attributes.reset_is_clear_context();
2537   // swap src&lt;-&gt;dst for encoding
2538   assert(src != xnoreg, "sanity");
2539   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2540   emit_int8(0x7F);
2541   emit_operand(src, dst);
2542 }
2543 
2544 // Move Unaligned EVEX enabled Vector (programmable : 8,16,32,64)
2545 void Assembler::evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) {
2546   assert(VM_Version::supports_evex(), "");
2547   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
2548   attributes.set_is_evex_instruction();
2549   int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;
2550   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &amp;attributes);
2551   emit_int8(0x6F);
2552   emit_int8((unsigned char)(0xC0 | encode));
2553 }
2554 
2555 void Assembler::evmovdqub(XMMRegister dst, Address src, int vector_len) {
2556   assert(VM_Version::supports_evex(), "");
2557   InstructionMark im(this);
2558   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
2559   int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;
2560   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2561   attributes.set_is_evex_instruction();
2562   vex_prefix(src, 0, dst-&gt;encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &amp;attributes);
2563   emit_int8(0x6F);
2564   emit_operand(dst, src);
2565 }
2566 
2567 void Assembler::evmovdqub(Address dst, XMMRegister src, int vector_len) {
2568   assert(VM_Version::supports_evex(), "");
2569   assert(src != xnoreg, "sanity");
2570   InstructionMark im(this);
2571   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
2572   int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;
2573   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2574   attributes.set_is_evex_instruction();
2575   vex_prefix(dst, 0, src-&gt;encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &amp;attributes);
2576   emit_int8(0x7F);
2577   emit_operand(src, dst);
2578 }
2579 
2580 void Assembler::evmovdqub(XMMRegister dst, KRegister mask, Address src, int vector_len) {
2581   assert(VM_Version::supports_avx512vlbw(), "");
2582   InstructionMark im(this);
2583   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
2584   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2585   attributes.set_embedded_opmask_register_specifier(mask);
2586   attributes.set_is_evex_instruction();
2587   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2588   emit_int8(0x6F);
2589   emit_operand(dst, src);
2590 }
2591 
2592 void Assembler::evmovdquw(XMMRegister dst, Address src, int vector_len) {
2593   assert(VM_Version::supports_evex(), "");
2594   InstructionMark im(this);
2595   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
2596   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2597   attributes.set_is_evex_instruction();
2598   int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;
2599   vex_prefix(src, 0, dst-&gt;encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &amp;attributes);
2600   emit_int8(0x6F);
2601   emit_operand(dst, src);
2602 }
2603 
2604 void Assembler::evmovdquw(XMMRegister dst, KRegister mask, Address src, int vector_len) {
2605   assert(VM_Version::supports_avx512vlbw(), "");
2606   InstructionMark im(this);
2607   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
2608   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2609   attributes.set_embedded_opmask_register_specifier(mask);
2610   attributes.set_is_evex_instruction();
2611   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2612   emit_int8(0x6F);
2613   emit_operand(dst, src);
2614 }
2615 
2616 void Assembler::evmovdquw(Address dst, XMMRegister src, int vector_len) {
2617   assert(VM_Version::supports_evex(), "");
2618   assert(src != xnoreg, "sanity");
2619   InstructionMark im(this);
2620   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
2621   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2622   attributes.set_is_evex_instruction();
2623   int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;
2624   vex_prefix(dst, 0, src-&gt;encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &amp;attributes);
2625   emit_int8(0x7F);
2626   emit_operand(src, dst);
2627 }
2628 
2629 void Assembler::evmovdquw(Address dst, KRegister mask, XMMRegister src, int vector_len) {
2630   assert(VM_Version::supports_avx512vlbw(), "");
2631   assert(src != xnoreg, "sanity");
2632   InstructionMark im(this);
2633   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
2634   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2635   attributes.reset_is_clear_context();
2636   attributes.set_embedded_opmask_register_specifier(mask);
2637   attributes.set_is_evex_instruction();
2638   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2639   emit_int8(0x7F);
2640   emit_operand(src, dst);
2641 }
2642 
2643 void Assembler::evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {
2644   assert(VM_Version::supports_evex(), "");
2645   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2646   attributes.set_is_evex_instruction();
2647   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2648   emit_int8(0x6F);
2649   emit_int8((unsigned char)(0xC0 | encode));
2650 }
2651 
2652 void Assembler::evmovdqul(XMMRegister dst, Address src, int vector_len) {
2653   assert(VM_Version::supports_evex(), "");
2654   InstructionMark im(this);
2655   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true , /* uses_vl */ true);
2656   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2657   attributes.set_is_evex_instruction();
2658   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2659   emit_int8(0x6F);
2660   emit_operand(dst, src);
2661 }
2662 
2663 void Assembler::evmovdqul(Address dst, XMMRegister src, int vector_len) {
2664   assert(VM_Version::supports_evex(), "");
2665   assert(src != xnoreg, "sanity");
2666   InstructionMark im(this);
2667   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2668   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2669   attributes.reset_is_clear_context();
2670   attributes.set_is_evex_instruction();
2671   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2672   emit_int8(0x7F);
2673   emit_operand(src, dst);
2674 }
2675 
2676 void Assembler::evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {
2677   assert(VM_Version::supports_evex(), "");
2678   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2679   attributes.set_is_evex_instruction();
2680   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2681   emit_int8(0x6F);
2682   emit_int8((unsigned char)(0xC0 | encode));
2683 }
2684 
2685 void Assembler::evmovdquq(XMMRegister dst, Address src, int vector_len) {
2686   assert(VM_Version::supports_evex(), "");
2687   InstructionMark im(this);
2688   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2689   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2690   attributes.set_is_evex_instruction();
2691   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2692   emit_int8(0x6F);
2693   emit_operand(dst, src);
2694 }
2695 
2696 void Assembler::evmovdquq(Address dst, XMMRegister src, int vector_len) {
2697   assert(VM_Version::supports_evex(), "");
2698   assert(src != xnoreg, "sanity");
2699   InstructionMark im(this);
2700   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2701   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
2702   attributes.reset_is_clear_context();
2703   attributes.set_is_evex_instruction();
2704   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2705   emit_int8(0x7F);
2706   emit_operand(src, dst);
2707 }
2708 
2709 // Uses zero extension on 64bit
2710 
2711 void Assembler::movl(Register dst, int32_t imm32) {
2712   int encode = prefix_and_encode(dst-&gt;encoding());
2713   emit_int8((unsigned char)(0xB8 | encode));
2714   emit_int32(imm32);
2715 }
2716 
2717 void Assembler::movl(Register dst, Register src) {
2718   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
2719   emit_int8((unsigned char)0x8B);
2720   emit_int8((unsigned char)(0xC0 | encode));
2721 }
2722 
2723 void Assembler::movl(Register dst, Address src) {
2724   InstructionMark im(this);
2725   prefix(src, dst);
2726   emit_int8((unsigned char)0x8B);
2727   emit_operand(dst, src);
2728 }
2729 
2730 void Assembler::movl(Address dst, int32_t imm32) {
2731   InstructionMark im(this);
2732   prefix(dst);
2733   emit_int8((unsigned char)0xC7);
2734   emit_operand(rax, dst, 4);
2735   emit_int32(imm32);
2736 }
2737 
2738 void Assembler::movl(Address dst, Register src) {
2739   InstructionMark im(this);
2740   prefix(dst, src);
2741   emit_int8((unsigned char)0x89);
2742   emit_operand(src, dst);
2743 }
2744 
2745 // New cpus require to use movsd and movss to avoid partial register stall
2746 // when loading from memory. But for old Opteron use movlpd instead of movsd.
2747 // The selection is done in MacroAssembler::movdbl() and movflt().
2748 void Assembler::movlpd(XMMRegister dst, Address src) {
2749   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2750   InstructionMark im(this);
2751   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
2752   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2753   attributes.set_rex_vex_w_reverted();
2754   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2755   emit_int8(0x12);
2756   emit_operand(dst, src);
2757 }
2758 
2759 void Assembler::movq( MMXRegister dst, Address src ) {
2760   assert( VM_Version::supports_mmx(), "" );
2761   emit_int8(0x0F);
2762   emit_int8(0x6F);
2763   emit_operand(dst, src);
2764 }
2765 
2766 void Assembler::movq( Address dst, MMXRegister src ) {
2767   assert( VM_Version::supports_mmx(), "" );
2768   emit_int8(0x0F);
2769   emit_int8(0x7F);
2770   // workaround gcc (3.2.1-7a) bug
2771   // In that version of gcc with only an emit_operand(MMX, Address)
2772   // gcc will tail jump and try and reverse the parameters completely
2773   // obliterating dst in the process. By having a version available
2774   // that doesn't need to swap the args at the tail jump the bug is
2775   // avoided.
2776   emit_operand(dst, src);
2777 }
2778 
2779 void Assembler::movq(XMMRegister dst, Address src) {
2780   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2781   InstructionMark im(this);
2782   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2783   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2784   attributes.set_rex_vex_w_reverted();
2785   simd_prefix(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2786   emit_int8(0x7E);
2787   emit_operand(dst, src);
2788 }
2789 
2790 void Assembler::movq(Address dst, XMMRegister src) {
2791   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2792   InstructionMark im(this);
2793   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2794   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2795   attributes.set_rex_vex_w_reverted();
2796   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
2797   emit_int8((unsigned char)0xD6);
2798   emit_operand(src, dst);
2799 }
2800 
2801 void Assembler::movsbl(Register dst, Address src) { // movsxb
2802   InstructionMark im(this);
2803   prefix(src, dst);
2804   emit_int8(0x0F);
2805   emit_int8((unsigned char)0xBE);
2806   emit_operand(dst, src);
2807 }
2808 
2809 void Assembler::movsbl(Register dst, Register src) { // movsxb
2810   NOT_LP64(assert(src-&gt;has_byte_register(), "must have byte register"));
2811   int encode = prefix_and_encode(dst-&gt;encoding(), false, src-&gt;encoding(), true);
2812   emit_int8(0x0F);
2813   emit_int8((unsigned char)0xBE);
2814   emit_int8((unsigned char)(0xC0 | encode));
2815 }
2816 
2817 void Assembler::movsd(XMMRegister dst, XMMRegister src) {
2818   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2819   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2820   attributes.set_rex_vex_w_reverted();
2821   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2822   emit_int8(0x10);
2823   emit_int8((unsigned char)(0xC0 | encode));
2824 }
2825 
2826 void Assembler::movsd(XMMRegister dst, Address src) {
2827   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2828   InstructionMark im(this);
2829   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2830   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2831   attributes.set_rex_vex_w_reverted();
2832   simd_prefix(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2833   emit_int8(0x10);
2834   emit_operand(dst, src);
2835 }
2836 
2837 void Assembler::movsd(Address dst, XMMRegister src) {
2838   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2839   InstructionMark im(this);
2840   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2841   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2842   attributes.reset_is_clear_context();
2843   attributes.set_rex_vex_w_reverted();
2844   simd_prefix(src, xnoreg, dst, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2845   emit_int8(0x11);
2846   emit_operand(src, dst);
2847 }
2848 
2849 void Assembler::movss(XMMRegister dst, XMMRegister src) {
2850   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2851   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2852   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2853   emit_int8(0x10);
2854   emit_int8((unsigned char)(0xC0 | encode));
2855 }
2856 
2857 void Assembler::movss(XMMRegister dst, Address src) {
2858   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2859   InstructionMark im(this);
2860   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2861   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
2862   simd_prefix(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2863   emit_int8(0x10);
2864   emit_operand(dst, src);
2865 }
2866 
2867 void Assembler::movss(Address dst, XMMRegister src) {
2868   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2869   InstructionMark im(this);
2870   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2871   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
2872   attributes.reset_is_clear_context();
2873   simd_prefix(src, xnoreg, dst, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2874   emit_int8(0x11);
2875   emit_operand(src, dst);
2876 }
2877 
2878 void Assembler::movswl(Register dst, Address src) { // movsxw
2879   InstructionMark im(this);
2880   prefix(src, dst);
2881   emit_int8(0x0F);
2882   emit_int8((unsigned char)0xBF);
2883   emit_operand(dst, src);
2884 }
2885 
2886 void Assembler::movswl(Register dst, Register src) { // movsxw
2887   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
2888   emit_int8(0x0F);
2889   emit_int8((unsigned char)0xBF);
2890   emit_int8((unsigned char)(0xC0 | encode));
2891 }
2892 
2893 void Assembler::movw(Address dst, int imm16) {
2894   InstructionMark im(this);
2895 
2896   emit_int8(0x66); // switch to 16-bit mode
2897   prefix(dst);
2898   emit_int8((unsigned char)0xC7);
2899   emit_operand(rax, dst, 2);
2900   emit_int16(imm16);
2901 }
2902 
2903 void Assembler::movw(Register dst, Address src) {
2904   InstructionMark im(this);
2905   emit_int8(0x66);
2906   prefix(src, dst);
2907   emit_int8((unsigned char)0x8B);
2908   emit_operand(dst, src);
2909 }
2910 
2911 void Assembler::movw(Address dst, Register src) {
2912   InstructionMark im(this);
2913   emit_int8(0x66);
2914   prefix(dst, src);
2915   emit_int8((unsigned char)0x89);
2916   emit_operand(src, dst);
2917 }
2918 
2919 void Assembler::movzbl(Register dst, Address src) { // movzxb
2920   InstructionMark im(this);
2921   prefix(src, dst);
2922   emit_int8(0x0F);
2923   emit_int8((unsigned char)0xB6);
2924   emit_operand(dst, src);
2925 }
2926 
2927 void Assembler::movzbl(Register dst, Register src) { // movzxb
2928   NOT_LP64(assert(src-&gt;has_byte_register(), "must have byte register"));
2929   int encode = prefix_and_encode(dst-&gt;encoding(), false, src-&gt;encoding(), true);
2930   emit_int8(0x0F);
2931   emit_int8((unsigned char)0xB6);
2932   emit_int8(0xC0 | encode);
2933 }
2934 
2935 void Assembler::movzwl(Register dst, Address src) { // movzxw
2936   InstructionMark im(this);
2937   prefix(src, dst);
2938   emit_int8(0x0F);
2939   emit_int8((unsigned char)0xB7);
2940   emit_operand(dst, src);
2941 }
2942 
2943 void Assembler::movzwl(Register dst, Register src) { // movzxw
2944   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
2945   emit_int8(0x0F);
2946   emit_int8((unsigned char)0xB7);
2947   emit_int8(0xC0 | encode);
2948 }
2949 
2950 void Assembler::mull(Address src) {
2951   InstructionMark im(this);
2952   prefix(src);
2953   emit_int8((unsigned char)0xF7);
2954   emit_operand(rsp, src);
2955 }
2956 
2957 void Assembler::mull(Register src) {
2958   int encode = prefix_and_encode(src-&gt;encoding());
2959   emit_int8((unsigned char)0xF7);
2960   emit_int8((unsigned char)(0xE0 | encode));
2961 }
2962 
2963 void Assembler::mulsd(XMMRegister dst, Address src) {
2964   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2965   InstructionMark im(this);
2966   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2967   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
2968   attributes.set_rex_vex_w_reverted();
2969   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2970   emit_int8(0x59);
2971   emit_operand(dst, src);
2972 }
2973 
2974 void Assembler::mulsd(XMMRegister dst, XMMRegister src) {
2975   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
2976   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2977   attributes.set_rex_vex_w_reverted();
2978   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
2979   emit_int8(0x59);
2980   emit_int8((unsigned char)(0xC0 | encode));
2981 }
2982 
2983 void Assembler::mulss(XMMRegister dst, Address src) {
2984   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2985   InstructionMark im(this);
2986   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2987   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
2988   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2989   emit_int8(0x59);
2990   emit_operand(dst, src);
2991 }
2992 
2993 void Assembler::mulss(XMMRegister dst, XMMRegister src) {
2994   NOT_LP64(assert(VM_Version::supports_sse(), ""));
2995   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
2996   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
2997   emit_int8(0x59);
2998   emit_int8((unsigned char)(0xC0 | encode));
2999 }
3000 
3001 void Assembler::negl(Register dst) {
3002   int encode = prefix_and_encode(dst-&gt;encoding());
3003   emit_int8((unsigned char)0xF7);
3004   emit_int8((unsigned char)(0xD8 | encode));
3005 }
3006 
3007 void Assembler::nop(int i) {
3008 #ifdef ASSERT
3009   assert(i &gt; 0, " ");
3010   // The fancy nops aren't currently recognized by debuggers making it a
3011   // pain to disassemble code while debugging. If asserts are on clearly
3012   // speed is not an issue so simply use the single byte traditional nop
3013   // to do alignment.
3014 
3015   for (; i &gt; 0 ; i--) emit_int8((unsigned char)0x90);
3016   return;
3017 
3018 #endif // ASSERT
3019 
3020   if (UseAddressNop &amp;&amp; VM_Version::is_intel()) {
3021     //
3022     // Using multi-bytes nops "0x0F 0x1F [address]" for Intel
3023     //  1: 0x90
3024     //  2: 0x66 0x90
3025     //  3: 0x66 0x66 0x90 (don't use "0x0F 0x1F 0x00" - need patching safe padding)
3026     //  4: 0x0F 0x1F 0x40 0x00
3027     //  5: 0x0F 0x1F 0x44 0x00 0x00
3028     //  6: 0x66 0x0F 0x1F 0x44 0x00 0x00
3029     //  7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
3030     //  8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3031     //  9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3032     // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3033     // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3034 
3035     // The rest coding is Intel specific - don't use consecutive address nops
3036 
3037     // 12: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3038     // 13: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3039     // 14: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3040     // 15: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3041 
3042     while(i &gt;= 15) {
3043       // For Intel don't generate consecutive addess nops (mix with regular nops)
3044       i -= 15;
3045       emit_int8(0x66);   // size prefix
3046       emit_int8(0x66);   // size prefix
3047       emit_int8(0x66);   // size prefix
3048       addr_nop_8();
3049       emit_int8(0x66);   // size prefix
3050       emit_int8(0x66);   // size prefix
3051       emit_int8(0x66);   // size prefix
3052       emit_int8((unsigned char)0x90);
3053                          // nop
3054     }
3055     switch (i) {
3056       case 14:
3057         emit_int8(0x66); // size prefix
3058       case 13:
3059         emit_int8(0x66); // size prefix
3060       case 12:
3061         addr_nop_8();
3062         emit_int8(0x66); // size prefix
3063         emit_int8(0x66); // size prefix
3064         emit_int8(0x66); // size prefix
3065         emit_int8((unsigned char)0x90);
3066                          // nop
3067         break;
3068       case 11:
3069         emit_int8(0x66); // size prefix
3070       case 10:
3071         emit_int8(0x66); // size prefix
3072       case 9:
3073         emit_int8(0x66); // size prefix
3074       case 8:
3075         addr_nop_8();
3076         break;
3077       case 7:
3078         addr_nop_7();
3079         break;
3080       case 6:
3081         emit_int8(0x66); // size prefix
3082       case 5:
3083         addr_nop_5();
3084         break;
3085       case 4:
3086         addr_nop_4();
3087         break;
3088       case 3:
3089         // Don't use "0x0F 0x1F 0x00" - need patching safe padding
3090         emit_int8(0x66); // size prefix
3091       case 2:
3092         emit_int8(0x66); // size prefix
3093       case 1:
3094         emit_int8((unsigned char)0x90);
3095                          // nop
3096         break;
3097       default:
3098         assert(i == 0, " ");
3099     }
3100     return;
3101   }
<a name="1" id="anc1"></a><span class="changed">3102   if (UseAddressNop &amp;&amp;</span>
<span class="changed">3103           (VM_Version::is_amd() || VM_Version::is_hygon())) {</span>
3104     //
3105     // Using multi-bytes nops "0x0F 0x1F [address]" for AMD.
3106     //  1: 0x90
3107     //  2: 0x66 0x90
3108     //  3: 0x66 0x66 0x90 (don't use "0x0F 0x1F 0x00" - need patching safe padding)
3109     //  4: 0x0F 0x1F 0x40 0x00
3110     //  5: 0x0F 0x1F 0x44 0x00 0x00
3111     //  6: 0x66 0x0F 0x1F 0x44 0x00 0x00
3112     //  7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
3113     //  8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3114     //  9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3115     // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3116     // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3117 
3118     // The rest coding is AMD specific - use consecutive address nops
3119 
3120     // 12: 0x66 0x0F 0x1F 0x44 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00
3121     // 13: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00
3122     // 14: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
3123     // 15: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
3124     // 16: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3125     //     Size prefixes (0x66) are added for larger sizes
3126 
3127     while(i &gt;= 22) {
3128       i -= 11;
3129       emit_int8(0x66); // size prefix
3130       emit_int8(0x66); // size prefix
3131       emit_int8(0x66); // size prefix
3132       addr_nop_8();
3133     }
3134     // Generate first nop for size between 21-12
3135     switch (i) {
3136       case 21:
3137         i -= 1;
3138         emit_int8(0x66); // size prefix
3139       case 20:
3140       case 19:
3141         i -= 1;
3142         emit_int8(0x66); // size prefix
3143       case 18:
3144       case 17:
3145         i -= 1;
3146         emit_int8(0x66); // size prefix
3147       case 16:
3148       case 15:
3149         i -= 8;
3150         addr_nop_8();
3151         break;
3152       case 14:
3153       case 13:
3154         i -= 7;
3155         addr_nop_7();
3156         break;
3157       case 12:
3158         i -= 6;
3159         emit_int8(0x66); // size prefix
3160         addr_nop_5();
3161         break;
3162       default:
3163         assert(i &lt; 12, " ");
3164     }
3165 
3166     // Generate second nop for size between 11-1
3167     switch (i) {
3168       case 11:
3169         emit_int8(0x66); // size prefix
3170       case 10:
3171         emit_int8(0x66); // size prefix
3172       case 9:
3173         emit_int8(0x66); // size prefix
3174       case 8:
3175         addr_nop_8();
3176         break;
3177       case 7:
3178         addr_nop_7();
3179         break;
3180       case 6:
3181         emit_int8(0x66); // size prefix
3182       case 5:
3183         addr_nop_5();
3184         break;
3185       case 4:
3186         addr_nop_4();
3187         break;
3188       case 3:
3189         // Don't use "0x0F 0x1F 0x00" - need patching safe padding
3190         emit_int8(0x66); // size prefix
3191       case 2:
3192         emit_int8(0x66); // size prefix
3193       case 1:
3194         emit_int8((unsigned char)0x90);
3195                          // nop
3196         break;
3197       default:
3198         assert(i == 0, " ");
3199     }
3200     return;
3201   }
3202 
3203   if (UseAddressNop &amp;&amp; VM_Version::is_zx()) {
3204     //
3205     // Using multi-bytes nops "0x0F 0x1F [address]" for ZX
3206     //  1: 0x90
3207     //  2: 0x66 0x90
3208     //  3: 0x66 0x66 0x90 (don't use "0x0F 0x1F 0x00" - need patching safe padding)
3209     //  4: 0x0F 0x1F 0x40 0x00
3210     //  5: 0x0F 0x1F 0x44 0x00 0x00
3211     //  6: 0x66 0x0F 0x1F 0x44 0x00 0x00
3212     //  7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
3213     //  8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3214     //  9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3215     // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3216     // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
3217 
3218     // The rest coding is ZX specific - don't use consecutive address nops
3219 
3220     // 12: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3221     // 13: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3222     // 14: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3223     // 15: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
3224 
3225     while (i &gt;= 15) {
3226       // For ZX don't generate consecutive addess nops (mix with regular nops)
3227       i -= 15;
3228       emit_int8(0x66);   // size prefix
3229       emit_int8(0x66);   // size prefix
3230       emit_int8(0x66);   // size prefix
3231       addr_nop_8();
3232       emit_int8(0x66);   // size prefix
3233       emit_int8(0x66);   // size prefix
3234       emit_int8(0x66);   // size prefix
3235       emit_int8((unsigned char)0x90);
3236                          // nop
3237     }
3238     switch (i) {
3239       case 14:
3240         emit_int8(0x66); // size prefix
3241       case 13:
3242         emit_int8(0x66); // size prefix
3243       case 12:
3244         addr_nop_8();
3245         emit_int8(0x66); // size prefix
3246         emit_int8(0x66); // size prefix
3247         emit_int8(0x66); // size prefix
3248         emit_int8((unsigned char)0x90);
3249                          // nop
3250         break;
3251       case 11:
3252         emit_int8(0x66); // size prefix
3253       case 10:
3254         emit_int8(0x66); // size prefix
3255       case 9:
3256         emit_int8(0x66); // size prefix
3257       case 8:
3258         addr_nop_8();
3259         break;
3260       case 7:
3261         addr_nop_7();
3262         break;
3263       case 6:
3264         emit_int8(0x66); // size prefix
3265       case 5:
3266         addr_nop_5();
3267         break;
3268       case 4:
3269         addr_nop_4();
3270         break;
3271       case 3:
3272         // Don't use "0x0F 0x1F 0x00" - need patching safe padding
3273         emit_int8(0x66); // size prefix
3274       case 2:
3275         emit_int8(0x66); // size prefix
3276       case 1:
3277         emit_int8((unsigned char)0x90);
3278                          // nop
3279         break;
3280       default:
3281         assert(i == 0, " ");
3282     }
3283     return;
3284   }
3285 
3286   // Using nops with size prefixes "0x66 0x90".
3287   // From AMD Optimization Guide:
3288   //  1: 0x90
3289   //  2: 0x66 0x90
3290   //  3: 0x66 0x66 0x90
3291   //  4: 0x66 0x66 0x66 0x90
3292   //  5: 0x66 0x66 0x90 0x66 0x90
3293   //  6: 0x66 0x66 0x90 0x66 0x66 0x90
3294   //  7: 0x66 0x66 0x66 0x90 0x66 0x66 0x90
3295   //  8: 0x66 0x66 0x66 0x90 0x66 0x66 0x66 0x90
3296   //  9: 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
3297   // 10: 0x66 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
3298   //
3299   while(i &gt; 12) {
3300     i -= 4;
3301     emit_int8(0x66); // size prefix
3302     emit_int8(0x66);
3303     emit_int8(0x66);
3304     emit_int8((unsigned char)0x90);
3305                      // nop
3306   }
3307   // 1 - 12 nops
3308   if(i &gt; 8) {
3309     if(i &gt; 9) {
3310       i -= 1;
3311       emit_int8(0x66);
3312     }
3313     i -= 3;
3314     emit_int8(0x66);
3315     emit_int8(0x66);
3316     emit_int8((unsigned char)0x90);
3317   }
3318   // 1 - 8 nops
3319   if(i &gt; 4) {
3320     if(i &gt; 6) {
3321       i -= 1;
3322       emit_int8(0x66);
3323     }
3324     i -= 3;
3325     emit_int8(0x66);
3326     emit_int8(0x66);
3327     emit_int8((unsigned char)0x90);
3328   }
3329   switch (i) {
3330     case 4:
3331       emit_int8(0x66);
3332     case 3:
3333       emit_int8(0x66);
3334     case 2:
3335       emit_int8(0x66);
3336     case 1:
3337       emit_int8((unsigned char)0x90);
3338       break;
3339     default:
3340       assert(i == 0, " ");
3341   }
3342 }
3343 
3344 void Assembler::notl(Register dst) {
3345   int encode = prefix_and_encode(dst-&gt;encoding());
3346   emit_int8((unsigned char)0xF7);
3347   emit_int8((unsigned char)(0xD0 | encode));
3348 }
3349 
3350 void Assembler::orl(Address dst, int32_t imm32) {
3351   InstructionMark im(this);
3352   prefix(dst);
3353   emit_arith_operand(0x81, rcx, dst, imm32);
3354 }
3355 
3356 void Assembler::orl(Register dst, int32_t imm32) {
3357   prefix(dst);
3358   emit_arith(0x81, 0xC8, dst, imm32);
3359 }
3360 
3361 void Assembler::orl(Register dst, Address src) {
3362   InstructionMark im(this);
3363   prefix(src, dst);
3364   emit_int8(0x0B);
3365   emit_operand(dst, src);
3366 }
3367 
3368 void Assembler::orl(Register dst, Register src) {
3369   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
3370   emit_arith(0x0B, 0xC0, dst, src);
3371 }
3372 
3373 void Assembler::orl(Address dst, Register src) {
3374   InstructionMark im(this);
3375   prefix(dst, src);
3376   emit_int8(0x09);
3377   emit_operand(src, dst);
3378 }
3379 
3380 void Assembler::orb(Address dst, int imm8) {
3381   InstructionMark im(this);
3382   prefix(dst);
3383   emit_int8((unsigned char)0x80);
3384   emit_operand(rcx, dst, 1);
3385   emit_int8(imm8);
3386 }
3387 
3388 void Assembler::packuswb(XMMRegister dst, Address src) {
3389   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
3390   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
3391   InstructionMark im(this);
3392   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3393   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
3394   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3395   emit_int8(0x67);
3396   emit_operand(dst, src);
3397 }
3398 
3399 void Assembler::packuswb(XMMRegister dst, XMMRegister src) {
3400   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
3401   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3402   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3403   emit_int8(0x67);
3404   emit_int8((unsigned char)(0xC0 | encode));
3405 }
3406 
3407 void Assembler::vpackuswb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3408   assert(UseAVX &gt; 0, "some form of AVX must be enabled");
3409   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3410   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3411   emit_int8(0x67);
3412   emit_int8((unsigned char)(0xC0 | encode));
3413 }
3414 
3415 void Assembler::vpermq(XMMRegister dst, XMMRegister src, int imm8, int vector_len) {
3416   assert(VM_Version::supports_avx2(), "");
3417   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3418   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3419   emit_int8(0x00);
3420   emit_int8(0xC0 | encode);
3421   emit_int8(imm8);
3422 }
3423 
3424 void Assembler::vperm2i128(XMMRegister dst,  XMMRegister nds, XMMRegister src, int imm8) {
3425   assert(VM_Version::supports_avx2(), "");
3426   InstructionAttr attributes(AVX_256bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3427   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3428   emit_int8(0x46);
3429   emit_int8(0xC0 | encode);
3430   emit_int8(imm8);
3431 }
3432 
3433 void Assembler::vperm2f128(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8) {
3434   assert(VM_Version::supports_avx(), "");
3435   InstructionAttr attributes(AVX_256bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3436   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3437   emit_int8(0x06);
3438   emit_int8(0xC0 | encode);
3439   emit_int8(imm8);
3440 }
3441 
3442 void Assembler::evpermi2q(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3443   assert(VM_Version::supports_evex(), "");
3444   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3445   attributes.set_is_evex_instruction();
3446   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3447   emit_int8(0x76);
3448   emit_int8((unsigned char)(0xC0 | encode));
3449 }
3450 
3451 
3452 void Assembler::pause() {
3453   emit_int8((unsigned char)0xF3);
3454   emit_int8((unsigned char)0x90);
3455 }
3456 
3457 void Assembler::ud2() {
3458   emit_int8(0x0F);
3459   emit_int8(0x0B);
3460 }
3461 
3462 void Assembler::pcmpestri(XMMRegister dst, Address src, int imm8) {
3463   assert(VM_Version::supports_sse4_2(), "");
3464   InstructionMark im(this);
3465   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3466   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3467   emit_int8(0x61);
3468   emit_operand(dst, src);
3469   emit_int8(imm8);
3470 }
3471 
3472 void Assembler::pcmpestri(XMMRegister dst, XMMRegister src, int imm8) {
3473   assert(VM_Version::supports_sse4_2(), "");
3474   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3475   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3476   emit_int8(0x61);
3477   emit_int8((unsigned char)(0xC0 | encode));
3478   emit_int8(imm8);
3479 }
3480 
3481 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3482 void Assembler::pcmpeqb(XMMRegister dst, XMMRegister src) {
3483   assert(VM_Version::supports_sse2(), "");
3484   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3485   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3486   emit_int8(0x74);
3487   emit_int8((unsigned char)(0xC0 | encode));
3488 }
3489 
3490 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3491 void Assembler::vpcmpeqb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3492   assert(VM_Version::supports_avx(), "");
3493   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3494   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3495   emit_int8(0x74);
3496   emit_int8((unsigned char)(0xC0 | encode));
3497 }
3498 
3499 // In this context, kdst is written the mask used to process the equal components
3500 void Assembler::evpcmpeqb(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len) {
3501   assert(VM_Version::supports_avx512bw(), "");
3502   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3503   attributes.set_is_evex_instruction();
3504   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3505   emit_int8(0x74);
3506   emit_int8((unsigned char)(0xC0 | encode));
3507 }
3508 
3509 void Assembler::evpcmpgtb(KRegister kdst, XMMRegister nds, Address src, int vector_len) {
3510   assert(VM_Version::supports_avx512vlbw(), "");
3511   InstructionMark im(this);
3512   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3513   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3514   attributes.set_is_evex_instruction();
3515   int dst_enc = kdst-&gt;encoding();
3516   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3517   emit_int8(0x64);
3518   emit_operand(as_Register(dst_enc), src);
3519 }
3520 
3521 void Assembler::evpcmpgtb(KRegister kdst, KRegister mask, XMMRegister nds, Address src, int vector_len) {
3522   assert(VM_Version::supports_avx512vlbw(), "");
3523   InstructionMark im(this);
3524   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
3525   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3526   attributes.reset_is_clear_context();
3527   attributes.set_embedded_opmask_register_specifier(mask);
3528   attributes.set_is_evex_instruction();
3529   int dst_enc = kdst-&gt;encoding();
3530   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3531   emit_int8(0x64);
3532   emit_operand(as_Register(dst_enc), src);
3533 }
3534 
3535 void Assembler::evpcmpuw(KRegister kdst, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len) {
3536   assert(VM_Version::supports_avx512vlbw(), "");
3537   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3538   attributes.set_is_evex_instruction();
3539   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3540   emit_int8(0x3E);
3541   emit_int8((unsigned char)(0xC0 | encode));
3542   emit_int8(vcc);
3543 }
3544 
3545 void Assembler::evpcmpuw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len) {
3546   assert(VM_Version::supports_avx512vlbw(), "");
3547   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
3548   attributes.reset_is_clear_context();
3549   attributes.set_embedded_opmask_register_specifier(mask);
3550   attributes.set_is_evex_instruction();
3551   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3552   emit_int8(0x3E);
3553   emit_int8((unsigned char)(0xC0 | encode));
3554   emit_int8(vcc);
3555 }
3556 
3557 void Assembler::evpcmpuw(KRegister kdst, XMMRegister nds, Address src, ComparisonPredicate vcc, int vector_len) {
3558   assert(VM_Version::supports_avx512vlbw(), "");
3559   InstructionMark im(this);
3560   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3561   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3562   attributes.set_is_evex_instruction();
3563   int dst_enc = kdst-&gt;encoding();
3564   vex_prefix(src, nds-&gt;encoding(), kdst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3565   emit_int8(0x3E);
3566   emit_operand(as_Register(dst_enc), src);
3567   emit_int8(vcc);
3568 }
3569 
3570 void Assembler::evpcmpeqb(KRegister kdst, XMMRegister nds, Address src, int vector_len) {
3571   assert(VM_Version::supports_avx512bw(), "");
3572   InstructionMark im(this);
3573   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3574   attributes.set_is_evex_instruction();
3575   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3576   int dst_enc = kdst-&gt;encoding();
3577   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3578   emit_int8(0x74);
3579   emit_operand(as_Register(dst_enc), src);
3580 }
3581 
3582 void Assembler::evpcmpeqb(KRegister kdst, KRegister mask, XMMRegister nds, Address src, int vector_len) {
3583   assert(VM_Version::supports_avx512vlbw(), "");
3584   InstructionMark im(this);
3585   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_reg_mask */ false, /* uses_vl */ true);
3586   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3587   attributes.reset_is_clear_context();
3588   attributes.set_embedded_opmask_register_specifier(mask);
3589   attributes.set_is_evex_instruction();
3590   vex_prefix(src, nds-&gt;encoding(), kdst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3591   emit_int8(0x74);
3592   emit_operand(as_Register(kdst-&gt;encoding()), src);
3593 }
3594 
3595 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3596 void Assembler::pcmpeqw(XMMRegister dst, XMMRegister src) {
3597   assert(VM_Version::supports_sse2(), "");
3598   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3599   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3600   emit_int8(0x75);
3601   emit_int8((unsigned char)(0xC0 | encode));
3602 }
3603 
3604 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3605 void Assembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3606   assert(VM_Version::supports_avx(), "");
3607   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3608   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3609   emit_int8(0x75);
3610   emit_int8((unsigned char)(0xC0 | encode));
3611 }
3612 
3613 // In this context, kdst is written the mask used to process the equal components
3614 void Assembler::evpcmpeqw(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len) {
3615   assert(VM_Version::supports_avx512bw(), "");
3616   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3617   attributes.set_is_evex_instruction();
3618   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3619   emit_int8(0x75);
3620   emit_int8((unsigned char)(0xC0 | encode));
3621 }
3622 
3623 void Assembler::evpcmpeqw(KRegister kdst, XMMRegister nds, Address src, int vector_len) {
3624   assert(VM_Version::supports_avx512bw(), "");
3625   InstructionMark im(this);
3626   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3627   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
3628   attributes.set_is_evex_instruction();
3629   int dst_enc = kdst-&gt;encoding();
3630   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3631   emit_int8(0x75);
3632   emit_operand(as_Register(dst_enc), src);
3633 }
3634 
3635 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3636 void Assembler::pcmpeqd(XMMRegister dst, XMMRegister src) {
3637   assert(VM_Version::supports_sse2(), "");
3638   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3639   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3640   emit_int8(0x76);
3641   emit_int8((unsigned char)(0xC0 | encode));
3642 }
3643 
3644 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3645 void Assembler::vpcmpeqd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3646   assert(VM_Version::supports_avx(), "");
3647   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3648   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3649   emit_int8(0x76);
3650   emit_int8((unsigned char)(0xC0 | encode));
3651 }
3652 
3653 // In this context, kdst is written the mask used to process the equal components
3654 void Assembler::evpcmpeqd(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len) {
3655   assert(VM_Version::supports_evex(), "");
3656   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3657   attributes.set_is_evex_instruction();
3658   attributes.reset_is_clear_context();
3659   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3660   emit_int8(0x76);
3661   emit_int8((unsigned char)(0xC0 | encode));
3662 }
3663 
3664 void Assembler::evpcmpeqd(KRegister kdst, XMMRegister nds, Address src, int vector_len) {
3665   assert(VM_Version::supports_evex(), "");
3666   InstructionMark im(this);
3667   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3668   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
3669   attributes.reset_is_clear_context();
3670   attributes.set_is_evex_instruction();
3671   int dst_enc = kdst-&gt;encoding();
3672   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3673   emit_int8(0x76);
3674   emit_operand(as_Register(dst_enc), src);
3675 }
3676 
3677 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3678 void Assembler::pcmpeqq(XMMRegister dst, XMMRegister src) {
3679   assert(VM_Version::supports_sse4_1(), "");
3680   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3681   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3682   emit_int8(0x29);
3683   emit_int8((unsigned char)(0xC0 | encode));
3684 }
3685 
3686 // In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst
3687 void Assembler::vpcmpeqq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3688   assert(VM_Version::supports_avx(), "");
3689   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3690   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3691   emit_int8(0x29);
3692   emit_int8((unsigned char)(0xC0 | encode));
3693 }
3694 
3695 // In this context, kdst is written the mask used to process the equal components
3696 void Assembler::evpcmpeqq(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len) {
3697   assert(VM_Version::supports_evex(), "");
3698   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3699   attributes.reset_is_clear_context();
3700   attributes.set_is_evex_instruction();
3701   int encode = vex_prefix_and_encode(kdst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3702   emit_int8(0x29);
3703   emit_int8((unsigned char)(0xC0 | encode));
3704 }
3705 
3706 // In this context, kdst is written the mask used to process the equal components
3707 void Assembler::evpcmpeqq(KRegister kdst, XMMRegister nds, Address src, int vector_len) {
3708   assert(VM_Version::supports_evex(), "");
3709   InstructionMark im(this);
3710   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3711   attributes.reset_is_clear_context();
3712   attributes.set_is_evex_instruction();
3713   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
3714   int dst_enc = kdst-&gt;encoding();
3715   vex_prefix(src, nds-&gt;encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3716   emit_int8(0x29);
3717   emit_operand(as_Register(dst_enc), src);
3718 }
3719 
3720 void Assembler::pmovmskb(Register dst, XMMRegister src) {
3721   assert(VM_Version::supports_sse2(), "");
3722   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3723   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3724   emit_int8((unsigned char)0xD7);
3725   emit_int8((unsigned char)(0xC0 | encode));
3726 }
3727 
3728 void Assembler::vpmovmskb(Register dst, XMMRegister src) {
3729   assert(VM_Version::supports_avx2(), "");
3730   InstructionAttr attributes(AVX_256bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
3731   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3732   emit_int8((unsigned char)0xD7);
3733   emit_int8((unsigned char)(0xC0 | encode));
3734 }
3735 
3736 void Assembler::pextrd(Register dst, XMMRegister src, int imm8) {
3737   assert(VM_Version::supports_sse4_1(), "");
3738   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3739   int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3740   emit_int8(0x16);
3741   emit_int8((unsigned char)(0xC0 | encode));
3742   emit_int8(imm8);
3743 }
3744 
3745 void Assembler::pextrd(Address dst, XMMRegister src, int imm8) {
3746   assert(VM_Version::supports_sse4_1(), "");
3747   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3748   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
3749   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3750   emit_int8(0x16);
3751   emit_operand(src, dst);
3752   emit_int8(imm8);
3753 }
3754 
3755 void Assembler::pextrq(Register dst, XMMRegister src, int imm8) {
3756   assert(VM_Version::supports_sse4_1(), "");
3757   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3758   int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3759   emit_int8(0x16);
3760   emit_int8((unsigned char)(0xC0 | encode));
3761   emit_int8(imm8);
3762 }
3763 
3764 void Assembler::pextrq(Address dst, XMMRegister src, int imm8) {
3765   assert(VM_Version::supports_sse4_1(), "");
3766   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3767   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
3768   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3769   emit_int8(0x16);
3770   emit_operand(src, dst);
3771   emit_int8(imm8);
3772 }
3773 
3774 void Assembler::pextrw(Register dst, XMMRegister src, int imm8) {
3775   assert(VM_Version::supports_sse2(), "");
3776   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3777   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3778   emit_int8((unsigned char)0xC5);
3779   emit_int8((unsigned char)(0xC0 | encode));
3780   emit_int8(imm8);
3781 }
3782 
3783 void Assembler::pextrw(Address dst, XMMRegister src, int imm8) {
3784   assert(VM_Version::supports_sse4_1(), "");
3785   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3786   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_16bit);
3787   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3788   emit_int8((unsigned char)0x15);
3789   emit_operand(src, dst);
3790   emit_int8(imm8);
3791 }
3792 
3793 void Assembler::pextrb(Address dst, XMMRegister src, int imm8) {
3794   assert(VM_Version::supports_sse4_1(), "");
3795   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3796   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_8bit);
3797   simd_prefix(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3798   emit_int8(0x14);
3799   emit_operand(src, dst);
3800   emit_int8(imm8);
3801 }
3802 
3803 void Assembler::pinsrd(XMMRegister dst, Register src, int imm8) {
3804   assert(VM_Version::supports_sse4_1(), "");
3805   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3806   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3807   emit_int8(0x22);
3808   emit_int8((unsigned char)(0xC0 | encode));
3809   emit_int8(imm8);
3810 }
3811 
3812 void Assembler::pinsrd(XMMRegister dst, Address src, int imm8) {
3813   assert(VM_Version::supports_sse4_1(), "");
3814   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3815   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
3816   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3817   emit_int8(0x22);
3818   emit_operand(dst,src);
3819   emit_int8(imm8);
3820 }
3821 
3822 void Assembler::pinsrq(XMMRegister dst, Register src, int imm8) {
3823   assert(VM_Version::supports_sse4_1(), "");
3824   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3825   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3826   emit_int8(0x22);
3827   emit_int8((unsigned char)(0xC0 | encode));
3828   emit_int8(imm8);
3829 }
3830 
3831 void Assembler::pinsrq(XMMRegister dst, Address src, int imm8) {
3832   assert(VM_Version::supports_sse4_1(), "");
3833   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
3834   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
3835   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3836   emit_int8(0x22);
3837   emit_operand(dst, src);
3838   emit_int8(imm8);
3839 }
3840 
3841 void Assembler::pinsrw(XMMRegister dst, Register src, int imm8) {
3842   assert(VM_Version::supports_sse2(), "");
3843   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3844   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3845   emit_int8((unsigned char)0xC4);
3846   emit_int8((unsigned char)(0xC0 | encode));
3847   emit_int8(imm8);
3848 }
3849 
3850 void Assembler::pinsrw(XMMRegister dst, Address src, int imm8) {
3851   assert(VM_Version::supports_sse2(), "");
3852   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3853   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_16bit);
3854   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3855   emit_int8((unsigned char)0xC4);
3856   emit_operand(dst, src);
3857   emit_int8(imm8);
3858 }
3859 
3860 void Assembler::pinsrb(XMMRegister dst, Address src, int imm8) {
3861   assert(VM_Version::supports_sse4_1(), "");
3862   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3863   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_8bit);
3864   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
3865   emit_int8(0x20);
3866   emit_operand(dst, src);
3867   emit_int8(imm8);
3868 }
3869 
3870 void Assembler::pmovzxbw(XMMRegister dst, Address src) {
3871   assert(VM_Version::supports_sse4_1(), "");
3872   InstructionMark im(this);
3873   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3874   attributes.set_address_attributes(/* tuple_type */ EVEX_HVM, /* input_size_in_bits */ EVEX_NObit);
3875   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3876   emit_int8(0x30);
3877   emit_operand(dst, src);
3878 }
3879 
3880 void Assembler::pmovzxbw(XMMRegister dst, XMMRegister src) {
3881   assert(VM_Version::supports_sse4_1(), "");
3882   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3883   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3884   emit_int8(0x30);
3885   emit_int8((unsigned char)(0xC0 | encode));
3886 }
3887 
3888 void Assembler::vpmovzxbw(XMMRegister dst, Address src, int vector_len) {
3889   assert(VM_Version::supports_avx(), "");
3890   InstructionMark im(this);
3891   assert(dst != xnoreg, "sanity");
3892   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3893   attributes.set_address_attributes(/* tuple_type */ EVEX_HVM, /* input_size_in_bits */ EVEX_NObit);
3894   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3895   emit_int8(0x30);
3896   emit_operand(dst, src);
3897 }
3898 
3899 void Assembler::vpmovzxbw(XMMRegister dst, XMMRegister src, int vector_len) {
3900   assert(vector_len == AVX_128bit? VM_Version::supports_avx() :
3901   vector_len == AVX_256bit? VM_Version::supports_avx2() :
3902   vector_len == AVX_512bit? VM_Version::supports_avx512bw() : 0, "");
3903   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3904   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3905   emit_int8(0x30);
3906   emit_int8((unsigned char) (0xC0 | encode));
3907 }
3908 
3909 
3910 void Assembler::evpmovzxbw(XMMRegister dst, KRegister mask, Address src, int vector_len) {
3911   assert(VM_Version::supports_avx512vlbw(), "");
3912   assert(dst != xnoreg, "sanity");
3913   InstructionMark im(this);
3914   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
3915   attributes.set_address_attributes(/* tuple_type */ EVEX_HVM, /* input_size_in_bits */ EVEX_NObit);
3916   attributes.set_embedded_opmask_register_specifier(mask);
3917   attributes.set_is_evex_instruction();
3918   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3919   emit_int8(0x30);
3920   emit_operand(dst, src);
3921 }
3922 void Assembler::evpmovwb(Address dst, XMMRegister src, int vector_len) {
3923   assert(VM_Version::supports_avx512vlbw(), "");
3924   assert(src != xnoreg, "sanity");
3925   InstructionMark im(this);
3926   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3927   attributes.set_address_attributes(/* tuple_type */ EVEX_HVM, /* input_size_in_bits */ EVEX_NObit);
3928   attributes.set_is_evex_instruction();
3929   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &amp;attributes);
3930   emit_int8(0x30);
3931   emit_operand(src, dst);
3932 }
3933 
3934 void Assembler::evpmovwb(Address dst, KRegister mask, XMMRegister src, int vector_len) {
3935   assert(VM_Version::supports_avx512vlbw(), "");
3936   assert(src != xnoreg, "sanity");
3937   InstructionMark im(this);
3938   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
3939   attributes.set_address_attributes(/* tuple_type */ EVEX_HVM, /* input_size_in_bits */ EVEX_NObit);
3940   attributes.reset_is_clear_context();
3941   attributes.set_embedded_opmask_register_specifier(mask);
3942   attributes.set_is_evex_instruction();
3943   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &amp;attributes);
3944   emit_int8(0x30);
3945   emit_operand(src, dst);
3946 }
3947 
3948 void Assembler::evpmovdb(Address dst, XMMRegister src, int vector_len) {
3949   assert(VM_Version::supports_evex(), "");
3950   assert(src != xnoreg, "sanity");
3951   InstructionMark im(this);
3952   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3953   attributes.set_address_attributes(/* tuple_type */ EVEX_QVM, /* input_size_in_bits */ EVEX_NObit);
3954   attributes.set_is_evex_instruction();
3955   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &amp;attributes);
3956   emit_int8(0x31);
3957   emit_operand(src, dst);
3958 }
3959 
3960 void Assembler::vpmovzxwd(XMMRegister dst, XMMRegister src, int vector_len) {
3961   assert(vector_len == AVX_128bit? VM_Version::supports_avx() :
3962   vector_len == AVX_256bit? VM_Version::supports_avx2() :
3963   vector_len == AVX_512bit? VM_Version::supports_evex() : 0, " ");
3964   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3965   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3966   emit_int8(0x33);
3967   emit_int8((unsigned char)(0xC0 | encode));
3968 }
3969 
3970 void Assembler::pmaddwd(XMMRegister dst, XMMRegister src) {
3971   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
3972   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3973   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3974   emit_int8((unsigned char)0xF5);
3975   emit_int8((unsigned char)(0xC0 | encode));
3976 }
3977 
3978 void Assembler::vpmaddwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3979   assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :
3980     (vector_len == AVX_256bit ? VM_Version::supports_avx2() :
3981     (vector_len == AVX_512bit ? VM_Version::supports_evex() : 0)), "");
3982   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
3983   int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
3984   emit_int8((unsigned char)0xF5);
3985   emit_int8((unsigned char)(0xC0 | encode));
3986 }
3987 
3988 void Assembler::evpdpwssd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
3989   assert(VM_Version::supports_evex(), "");
3990   assert(VM_Version::supports_vnni(), "must support vnni");
3991   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
3992   attributes.set_is_evex_instruction();
3993   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
3994   emit_int8(0x52);
3995   emit_int8((unsigned char)(0xC0 | encode));
3996 }
3997 
3998 // generic
3999 void Assembler::pop(Register dst) {
4000   int encode = prefix_and_encode(dst-&gt;encoding());
4001   emit_int8(0x58 | encode);
4002 }
4003 
4004 void Assembler::popcntl(Register dst, Address src) {
4005   assert(VM_Version::supports_popcnt(), "must support");
4006   InstructionMark im(this);
4007   emit_int8((unsigned char)0xF3);
4008   prefix(src, dst);
4009   emit_int8(0x0F);
4010   emit_int8((unsigned char)0xB8);
4011   emit_operand(dst, src);
4012 }
4013 
4014 void Assembler::popcntl(Register dst, Register src) {
4015   assert(VM_Version::supports_popcnt(), "must support");
4016   emit_int8((unsigned char)0xF3);
4017   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4018   emit_int8(0x0F);
4019   emit_int8((unsigned char)0xB8);
4020   emit_int8((unsigned char)(0xC0 | encode));
4021 }
4022 
4023 void Assembler::vpopcntd(XMMRegister dst, XMMRegister src, int vector_len) {
4024   assert(VM_Version::supports_vpopcntdq(), "must support vpopcntdq feature");
4025   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4026   attributes.set_is_evex_instruction();
4027   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4028   emit_int8(0x55);
4029   emit_int8((unsigned char)(0xC0 | encode));
4030 }
4031 
4032 void Assembler::popf() {
4033   emit_int8((unsigned char)0x9D);
4034 }
4035 
4036 #ifndef _LP64 // no 32bit push/pop on amd64
4037 void Assembler::popl(Address dst) {
4038   // NOTE: this will adjust stack by 8byte on 64bits
4039   InstructionMark im(this);
4040   prefix(dst);
4041   emit_int8((unsigned char)0x8F);
4042   emit_operand(rax, dst);
4043 }
4044 #endif
4045 
4046 void Assembler::prefetch_prefix(Address src) {
4047   prefix(src);
4048   emit_int8(0x0F);
4049 }
4050 
4051 void Assembler::prefetchnta(Address src) {
4052   NOT_LP64(assert(VM_Version::supports_sse(), "must support"));
4053   InstructionMark im(this);
4054   prefetch_prefix(src);
4055   emit_int8(0x18);
4056   emit_operand(rax, src); // 0, src
4057 }
4058 
4059 void Assembler::prefetchr(Address src) {
4060   assert(VM_Version::supports_3dnow_prefetch(), "must support");
4061   InstructionMark im(this);
4062   prefetch_prefix(src);
4063   emit_int8(0x0D);
4064   emit_operand(rax, src); // 0, src
4065 }
4066 
4067 void Assembler::prefetcht0(Address src) {
4068   NOT_LP64(assert(VM_Version::supports_sse(), "must support"));
4069   InstructionMark im(this);
4070   prefetch_prefix(src);
4071   emit_int8(0x18);
4072   emit_operand(rcx, src); // 1, src
4073 }
4074 
4075 void Assembler::prefetcht1(Address src) {
4076   NOT_LP64(assert(VM_Version::supports_sse(), "must support"));
4077   InstructionMark im(this);
4078   prefetch_prefix(src);
4079   emit_int8(0x18);
4080   emit_operand(rdx, src); // 2, src
4081 }
4082 
4083 void Assembler::prefetcht2(Address src) {
4084   NOT_LP64(assert(VM_Version::supports_sse(), "must support"));
4085   InstructionMark im(this);
4086   prefetch_prefix(src);
4087   emit_int8(0x18);
4088   emit_operand(rbx, src); // 3, src
4089 }
4090 
4091 void Assembler::prefetchw(Address src) {
4092   assert(VM_Version::supports_3dnow_prefetch(), "must support");
4093   InstructionMark im(this);
4094   prefetch_prefix(src);
4095   emit_int8(0x0D);
4096   emit_operand(rcx, src); // 1, src
4097 }
4098 
4099 void Assembler::prefix(Prefix p) {
4100   emit_int8(p);
4101 }
4102 
4103 void Assembler::pshufb(XMMRegister dst, XMMRegister src) {
4104   assert(VM_Version::supports_ssse3(), "");
4105   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4106   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4107   emit_int8(0x00);
4108   emit_int8((unsigned char)(0xC0 | encode));
4109 }
4110 
4111 void Assembler::vpshufb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
4112   assert(vector_len == AVX_128bit? VM_Version::supports_avx() :
4113          vector_len == AVX_256bit? VM_Version::supports_avx2() :
4114          0, "");
4115   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4116   int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4117   emit_int8(0x00);
4118   emit_int8((unsigned char)(0xC0 | encode));
4119 }
4120 
4121 void Assembler::pshufb(XMMRegister dst, Address src) {
4122   assert(VM_Version::supports_ssse3(), "");
4123   InstructionMark im(this);
4124   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4125   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
4126   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4127   emit_int8(0x00);
4128   emit_operand(dst, src);
4129 }
4130 
4131 void Assembler::pshufd(XMMRegister dst, XMMRegister src, int mode) {
4132   assert(isByte(mode), "invalid value");
4133   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4134   int vector_len = VM_Version::supports_avx512novl() ? AVX_512bit : AVX_128bit;
4135   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4136   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4137   emit_int8(0x70);
4138   emit_int8((unsigned char)(0xC0 | encode));
4139   emit_int8(mode &amp; 0xFF);
4140 }
4141 
4142 void Assembler::vpshufd(XMMRegister dst, XMMRegister src, int mode, int vector_len) {
4143   assert(vector_len == AVX_128bit? VM_Version::supports_avx() :
4144          vector_len == AVX_256bit? VM_Version::supports_avx2() :
4145          0, "");
4146   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4147   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4148   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4149   emit_int8(0x70);
4150   emit_int8((unsigned char)(0xC0 | encode));
4151   emit_int8(mode &amp; 0xFF);
4152 }
4153 
4154 void Assembler::pshufd(XMMRegister dst, Address src, int mode) {
4155   assert(isByte(mode), "invalid value");
4156   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4157   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
4158   InstructionMark im(this);
4159   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4160   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
4161   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4162   emit_int8(0x70);
4163   emit_operand(dst, src);
4164   emit_int8(mode &amp; 0xFF);
4165 }
4166 
4167 void Assembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
4168   assert(isByte(mode), "invalid value");
4169   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4170   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4171   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4172   emit_int8(0x70);
4173   emit_int8((unsigned char)(0xC0 | encode));
4174   emit_int8(mode &amp; 0xFF);
4175 }
4176 
4177 void Assembler::pshuflw(XMMRegister dst, Address src, int mode) {
4178   assert(isByte(mode), "invalid value");
4179   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4180   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
4181   InstructionMark im(this);
4182   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4183   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
4184   simd_prefix(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4185   emit_int8(0x70);
4186   emit_operand(dst, src);
4187   emit_int8(mode &amp; 0xFF);
4188 }
4189 void Assembler::evshufi64x2(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {
4190   assert(VM_Version::supports_evex(), "requires EVEX support");
4191   assert(vector_len == Assembler::AVX_256bit || vector_len == Assembler::AVX_512bit, "");
4192   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4193   attributes.set_is_evex_instruction();
4194   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
4195   emit_int8(0x43);
4196   emit_int8((unsigned char)(0xC0 | encode));
4197   emit_int8(imm8 &amp; 0xFF);
4198 }
4199 
4200 void Assembler::psrldq(XMMRegister dst, int shift) {
4201   // Shift left 128 bit value in dst XMMRegister by shift number of bytes.
4202   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4203   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4204   int encode = simd_prefix_and_encode(xmm3, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4205   emit_int8(0x73);
4206   emit_int8((unsigned char)(0xC0 | encode));
4207   emit_int8(shift);
4208 }
4209 
4210 void Assembler::vpsrldq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
4211   assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :
4212          vector_len == AVX_256bit ? VM_Version::supports_avx2() :
4213          vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : 0, "");
4214   InstructionAttr attributes(vector_len, /*vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4215   int encode = vex_prefix_and_encode(xmm3-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4216   emit_int8(0x73);
4217   emit_int8((unsigned char)(0xC0 | encode));
4218   emit_int8(shift &amp; 0xFF);
4219 }
4220 
4221 void Assembler::pslldq(XMMRegister dst, int shift) {
4222   // Shift left 128 bit value in dst XMMRegister by shift number of bytes.
4223   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4224   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4225   // XMM7 is for /7 encoding: 66 0F 73 /7 ib
4226   int encode = simd_prefix_and_encode(xmm7, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4227   emit_int8(0x73);
4228   emit_int8((unsigned char)(0xC0 | encode));
4229   emit_int8(shift);
4230 }
4231 
4232 void Assembler::vpslldq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
4233   assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :
4234          vector_len == AVX_256bit ? VM_Version::supports_avx2() :
4235          vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : 0, "");
4236   InstructionAttr attributes(vector_len, /*vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4237   int encode = vex_prefix_and_encode(xmm7-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4238   emit_int8(0x73);
4239   emit_int8((unsigned char)(0xC0 | encode));
4240   emit_int8(shift &amp; 0xFF);
4241 }
4242 
4243 void Assembler::ptest(XMMRegister dst, Address src) {
4244   assert(VM_Version::supports_sse4_1(), "");
4245   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
4246   InstructionMark im(this);
4247   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4248   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4249   emit_int8(0x17);
4250   emit_operand(dst, src);
4251 }
4252 
4253 void Assembler::ptest(XMMRegister dst, XMMRegister src) {
4254   assert(VM_Version::supports_sse4_1() || VM_Version::supports_avx(), "");
4255   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4256   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4257   emit_int8(0x17);
4258   emit_int8((unsigned char)(0xC0 | encode));
4259 }
4260 
4261 void Assembler::vptest(XMMRegister dst, Address src) {
4262   assert(VM_Version::supports_avx(), "");
4263   InstructionMark im(this);
4264   InstructionAttr attributes(AVX_256bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4265   assert(dst != xnoreg, "sanity");
4266   // swap src&lt;-&gt;dst for encoding
4267   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4268   emit_int8(0x17);
4269   emit_operand(dst, src);
4270 }
4271 
4272 void Assembler::vptest(XMMRegister dst, XMMRegister src) {
4273   assert(VM_Version::supports_avx(), "");
4274   InstructionAttr attributes(AVX_256bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4275   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
4276   emit_int8(0x17);
4277   emit_int8((unsigned char)(0xC0 | encode));
4278 }
4279 
4280 void Assembler::punpcklbw(XMMRegister dst, Address src) {
4281   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4282   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
4283   InstructionMark im(this);
4284   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_vlbw, /* no_mask_reg */ true, /* uses_vl */ true);
4285   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
4286   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4287   emit_int8(0x60);
4288   emit_operand(dst, src);
4289 }
4290 
4291 void Assembler::punpcklbw(XMMRegister dst, XMMRegister src) {
4292   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4293   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_vlbw, /* no_mask_reg */ true, /* uses_vl */ true);
4294   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4295   emit_int8(0x60);
4296   emit_int8((unsigned char)(0xC0 | encode));
4297 }
4298 
4299 void Assembler::punpckldq(XMMRegister dst, Address src) {
4300   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4301   assert((UseAVX &gt; 0), "SSE mode requires address alignment 16 bytes");
4302   InstructionMark im(this);
4303   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4304   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
4305   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4306   emit_int8(0x62);
4307   emit_operand(dst, src);
4308 }
4309 
4310 void Assembler::punpckldq(XMMRegister dst, XMMRegister src) {
4311   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4312   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4313   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4314   emit_int8(0x62);
4315   emit_int8((unsigned char)(0xC0 | encode));
4316 }
4317 
4318 void Assembler::punpcklqdq(XMMRegister dst, XMMRegister src) {
4319   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4320   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4321   attributes.set_rex_vex_w_reverted();
4322   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4323   emit_int8(0x6C);
4324   emit_int8((unsigned char)(0xC0 | encode));
4325 }
4326 
4327 void Assembler::push(int32_t imm32) {
4328   // in 64bits we push 64bits onto the stack but only
4329   // take a 32bit immediate
4330   emit_int8(0x68);
4331   emit_int32(imm32);
4332 }
4333 
4334 void Assembler::push(Register src) {
4335   int encode = prefix_and_encode(src-&gt;encoding());
4336 
4337   emit_int8(0x50 | encode);
4338 }
4339 
4340 void Assembler::pushf() {
4341   emit_int8((unsigned char)0x9C);
4342 }
4343 
4344 #ifndef _LP64 // no 32bit push/pop on amd64
4345 void Assembler::pushl(Address src) {
4346   // Note this will push 64bit on 64bit
4347   InstructionMark im(this);
4348   prefix(src);
4349   emit_int8((unsigned char)0xFF);
4350   emit_operand(rsi, src);
4351 }
4352 #endif
4353 
4354 void Assembler::rcll(Register dst, int imm8) {
4355   assert(isShiftCount(imm8), "illegal shift count");
4356   int encode = prefix_and_encode(dst-&gt;encoding());
4357   if (imm8 == 1) {
4358     emit_int8((unsigned char)0xD1);
4359     emit_int8((unsigned char)(0xD0 | encode));
4360   } else {
4361     emit_int8((unsigned char)0xC1);
4362     emit_int8((unsigned char)0xD0 | encode);
4363     emit_int8(imm8);
4364   }
4365 }
4366 
4367 void Assembler::rcpps(XMMRegister dst, XMMRegister src) {
4368   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4369   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4370   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
4371   emit_int8(0x53);
4372   emit_int8((unsigned char)(0xC0 | encode));
4373 }
4374 
4375 void Assembler::rcpss(XMMRegister dst, XMMRegister src) {
4376   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4377   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4378   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
4379   emit_int8(0x53);
4380   emit_int8((unsigned char)(0xC0 | encode));
4381 }
4382 
4383 void Assembler::rdtsc() {
4384   emit_int8((unsigned char)0x0F);
4385   emit_int8((unsigned char)0x31);
4386 }
4387 
4388 // copies data from [esi] to [edi] using rcx pointer sized words
4389 // generic
4390 void Assembler::rep_mov() {
4391   emit_int8((unsigned char)0xF3);
4392   // MOVSQ
4393   LP64_ONLY(prefix(REX_W));
4394   emit_int8((unsigned char)0xA5);
4395 }
4396 
4397 // sets rcx bytes with rax, value at [edi]
4398 void Assembler::rep_stosb() {
4399   emit_int8((unsigned char)0xF3); // REP
4400   LP64_ONLY(prefix(REX_W));
4401   emit_int8((unsigned char)0xAA); // STOSB
4402 }
4403 
4404 // sets rcx pointer sized words with rax, value at [edi]
4405 // generic
4406 void Assembler::rep_stos() {
4407   emit_int8((unsigned char)0xF3); // REP
4408   LP64_ONLY(prefix(REX_W));       // LP64:STOSQ, LP32:STOSD
4409   emit_int8((unsigned char)0xAB);
4410 }
4411 
4412 // scans rcx pointer sized words at [edi] for occurance of rax,
4413 // generic
4414 void Assembler::repne_scan() { // repne_scan
4415   emit_int8((unsigned char)0xF2);
4416   // SCASQ
4417   LP64_ONLY(prefix(REX_W));
4418   emit_int8((unsigned char)0xAF);
4419 }
4420 
4421 #ifdef _LP64
4422 // scans rcx 4 byte words at [edi] for occurance of rax,
4423 // generic
4424 void Assembler::repne_scanl() { // repne_scan
4425   emit_int8((unsigned char)0xF2);
4426   // SCASL
4427   emit_int8((unsigned char)0xAF);
4428 }
4429 #endif
4430 
4431 void Assembler::ret(int imm16) {
4432   if (imm16 == 0) {
4433     emit_int8((unsigned char)0xC3);
4434   } else {
4435     emit_int8((unsigned char)0xC2);
4436     emit_int16(imm16);
4437   }
4438 }
4439 
4440 void Assembler::sahf() {
4441 #ifdef _LP64
4442   // Not supported in 64bit mode
4443   ShouldNotReachHere();
4444 #endif
4445   emit_int8((unsigned char)0x9E);
4446 }
4447 
4448 void Assembler::sarl(Register dst, int imm8) {
4449   int encode = prefix_and_encode(dst-&gt;encoding());
4450   assert(isShiftCount(imm8), "illegal shift count");
4451   if (imm8 == 1) {
4452     emit_int8((unsigned char)0xD1);
4453     emit_int8((unsigned char)(0xF8 | encode));
4454   } else {
4455     emit_int8((unsigned char)0xC1);
4456     emit_int8((unsigned char)(0xF8 | encode));
4457     emit_int8(imm8);
4458   }
4459 }
4460 
4461 void Assembler::sarl(Register dst) {
4462   int encode = prefix_and_encode(dst-&gt;encoding());
4463   emit_int8((unsigned char)0xD3);
4464   emit_int8((unsigned char)(0xF8 | encode));
4465 }
4466 
4467 void Assembler::sbbl(Address dst, int32_t imm32) {
4468   InstructionMark im(this);
4469   prefix(dst);
4470   emit_arith_operand(0x81, rbx, dst, imm32);
4471 }
4472 
4473 void Assembler::sbbl(Register dst, int32_t imm32) {
4474   prefix(dst);
4475   emit_arith(0x81, 0xD8, dst, imm32);
4476 }
4477 
4478 
4479 void Assembler::sbbl(Register dst, Address src) {
4480   InstructionMark im(this);
4481   prefix(src, dst);
4482   emit_int8(0x1B);
4483   emit_operand(dst, src);
4484 }
4485 
4486 void Assembler::sbbl(Register dst, Register src) {
4487   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4488   emit_arith(0x1B, 0xC0, dst, src);
4489 }
4490 
4491 void Assembler::setb(Condition cc, Register dst) {
4492   assert(0 &lt;= cc &amp;&amp; cc &lt; 16, "illegal cc");
4493   int encode = prefix_and_encode(dst-&gt;encoding(), true);
4494   emit_int8(0x0F);
4495   emit_int8((unsigned char)0x90 | cc);
4496   emit_int8((unsigned char)(0xC0 | encode));
4497 }
4498 
4499 void Assembler::palignr(XMMRegister dst, XMMRegister src, int imm8) {
4500   assert(VM_Version::supports_ssse3(), "");
4501   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4502   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
4503   emit_int8((unsigned char)0x0F);
4504   emit_int8((unsigned char)(0xC0 | encode));
4505   emit_int8(imm8);
4506 }
4507 
4508 void Assembler::vpalignr(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {
4509   assert(vector_len == AVX_128bit? VM_Version::supports_avx() :
4510          vector_len == AVX_256bit? VM_Version::supports_avx2() :
4511          0, "");
4512   InstructionAttr attributes(vector_len, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
4513   int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
4514   emit_int8((unsigned char)0x0F);
4515   emit_int8((unsigned char)(0xC0 | encode));
4516   emit_int8(imm8);
4517 }
4518 
4519 void Assembler::evalignq(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
4520   assert(VM_Version::supports_evex(), "");
4521   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
4522   attributes.set_is_evex_instruction();
4523   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
4524   emit_int8(0x3);
4525   emit_int8((unsigned char)(0xC0 | encode));
4526   emit_int8(imm8);
4527 }
4528 
4529 void Assembler::pblendw(XMMRegister dst, XMMRegister src, int imm8) {
4530   assert(VM_Version::supports_sse4_1(), "");
4531   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4532   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
4533   emit_int8((unsigned char)0x0E);
4534   emit_int8((unsigned char)(0xC0 | encode));
4535   emit_int8(imm8);
4536 }
4537 
4538 void Assembler::sha1rnds4(XMMRegister dst, XMMRegister src, int imm8) {
4539   assert(VM_Version::supports_sha(), "");
4540   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_3A, /* rex_w */ false);
4541   emit_int8((unsigned char)0xCC);
4542   emit_int8((unsigned char)(0xC0 | encode));
4543   emit_int8((unsigned char)imm8);
4544 }
4545 
4546 void Assembler::sha1nexte(XMMRegister dst, XMMRegister src) {
4547   assert(VM_Version::supports_sha(), "");
4548   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4549   emit_int8((unsigned char)0xC8);
4550   emit_int8((unsigned char)(0xC0 | encode));
4551 }
4552 
4553 void Assembler::sha1msg1(XMMRegister dst, XMMRegister src) {
4554   assert(VM_Version::supports_sha(), "");
4555   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4556   emit_int8((unsigned char)0xC9);
4557   emit_int8((unsigned char)(0xC0 | encode));
4558 }
4559 
4560 void Assembler::sha1msg2(XMMRegister dst, XMMRegister src) {
4561   assert(VM_Version::supports_sha(), "");
4562   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4563   emit_int8((unsigned char)0xCA);
4564   emit_int8((unsigned char)(0xC0 | encode));
4565 }
4566 
4567 // xmm0 is implicit additional source to this instruction.
4568 void Assembler::sha256rnds2(XMMRegister dst, XMMRegister src) {
4569   assert(VM_Version::supports_sha(), "");
4570   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4571   emit_int8((unsigned char)0xCB);
4572   emit_int8((unsigned char)(0xC0 | encode));
4573 }
4574 
4575 void Assembler::sha256msg1(XMMRegister dst, XMMRegister src) {
4576   assert(VM_Version::supports_sha(), "");
4577   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4578   emit_int8((unsigned char)0xCC);
4579   emit_int8((unsigned char)(0xC0 | encode));
4580 }
4581 
4582 void Assembler::sha256msg2(XMMRegister dst, XMMRegister src) {
4583   assert(VM_Version::supports_sha(), "");
4584   int encode = rex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, /* rex_w */ false);
4585   emit_int8((unsigned char)0xCD);
4586   emit_int8((unsigned char)(0xC0 | encode));
4587 }
4588 
4589 
4590 void Assembler::shll(Register dst, int imm8) {
4591   assert(isShiftCount(imm8), "illegal shift count");
4592   int encode = prefix_and_encode(dst-&gt;encoding());
4593   if (imm8 == 1 ) {
4594     emit_int8((unsigned char)0xD1);
4595     emit_int8((unsigned char)(0xE0 | encode));
4596   } else {
4597     emit_int8((unsigned char)0xC1);
4598     emit_int8((unsigned char)(0xE0 | encode));
4599     emit_int8(imm8);
4600   }
4601 }
4602 
4603 void Assembler::shll(Register dst) {
4604   int encode = prefix_and_encode(dst-&gt;encoding());
4605   emit_int8((unsigned char)0xD3);
4606   emit_int8((unsigned char)(0xE0 | encode));
4607 }
4608 
4609 void Assembler::shrl(Register dst, int imm8) {
4610   assert(isShiftCount(imm8), "illegal shift count");
4611   int encode = prefix_and_encode(dst-&gt;encoding());
4612   emit_int8((unsigned char)0xC1);
4613   emit_int8((unsigned char)(0xE8 | encode));
4614   emit_int8(imm8);
4615 }
4616 
4617 void Assembler::shrl(Register dst) {
4618   int encode = prefix_and_encode(dst-&gt;encoding());
4619   emit_int8((unsigned char)0xD3);
4620   emit_int8((unsigned char)(0xE8 | encode));
4621 }
4622 
4623 // copies a single word from [esi] to [edi]
4624 void Assembler::smovl() {
4625   emit_int8((unsigned char)0xA5);
4626 }
4627 
4628 void Assembler::sqrtsd(XMMRegister dst, XMMRegister src) {
4629   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4630   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4631   attributes.set_rex_vex_w_reverted();
4632   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4633   emit_int8(0x51);
4634   emit_int8((unsigned char)(0xC0 | encode));
4635 }
4636 
4637 void Assembler::sqrtsd(XMMRegister dst, Address src) {
4638   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4639   InstructionMark im(this);
4640   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4641   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
4642   attributes.set_rex_vex_w_reverted();
4643   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4644   emit_int8(0x51);
4645   emit_operand(dst, src);
4646 }
4647 
4648 void Assembler::sqrtss(XMMRegister dst, XMMRegister src) {
4649   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4650   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4651   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
4652   emit_int8(0x51);
4653   emit_int8((unsigned char)(0xC0 | encode));
4654 }
4655 
4656 void Assembler::std() {
4657   emit_int8((unsigned char)0xFD);
4658 }
4659 
4660 void Assembler::sqrtss(XMMRegister dst, Address src) {
4661   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4662   InstructionMark im(this);
4663   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4664   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
4665   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
4666   emit_int8(0x51);
4667   emit_operand(dst, src);
4668 }
4669 
4670 void Assembler::stmxcsr( Address dst) {
4671   if (UseAVX &gt; 0 ) {
4672     assert(VM_Version::supports_avx(), "");
4673     InstructionMark im(this);
4674     InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
4675     vex_prefix(dst, 0, 0, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
4676     emit_int8((unsigned char)0xAE);
4677     emit_operand(as_Register(3), dst);
4678   } else {
4679     NOT_LP64(assert(VM_Version::supports_sse(), ""));
4680     InstructionMark im(this);
4681     prefix(dst);
4682     emit_int8(0x0F);
4683     emit_int8((unsigned char)0xAE);
4684     emit_operand(as_Register(3), dst);
4685   }
4686 }
4687 
4688 void Assembler::subl(Address dst, int32_t imm32) {
4689   InstructionMark im(this);
4690   prefix(dst);
4691   emit_arith_operand(0x81, rbp, dst, imm32);
4692 }
4693 
4694 void Assembler::subl(Address dst, Register src) {
4695   InstructionMark im(this);
4696   prefix(dst, src);
4697   emit_int8(0x29);
4698   emit_operand(src, dst);
4699 }
4700 
4701 void Assembler::subl(Register dst, int32_t imm32) {
4702   prefix(dst);
4703   emit_arith(0x81, 0xE8, dst, imm32);
4704 }
4705 
4706 // Force generation of a 4 byte immediate value even if it fits into 8bit
4707 void Assembler::subl_imm32(Register dst, int32_t imm32) {
4708   prefix(dst);
4709   emit_arith_imm32(0x81, 0xE8, dst, imm32);
4710 }
4711 
4712 void Assembler::subl(Register dst, Address src) {
4713   InstructionMark im(this);
4714   prefix(src, dst);
4715   emit_int8(0x2B);
4716   emit_operand(dst, src);
4717 }
4718 
4719 void Assembler::subl(Register dst, Register src) {
4720   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4721   emit_arith(0x2B, 0xC0, dst, src);
4722 }
4723 
4724 void Assembler::subsd(XMMRegister dst, XMMRegister src) {
4725   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4726   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4727   attributes.set_rex_vex_w_reverted();
4728   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4729   emit_int8(0x5C);
4730   emit_int8((unsigned char)(0xC0 | encode));
4731 }
4732 
4733 void Assembler::subsd(XMMRegister dst, Address src) {
4734   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4735   InstructionMark im(this);
4736   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4737   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
4738   attributes.set_rex_vex_w_reverted();
4739   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4740   emit_int8(0x5C);
4741   emit_operand(dst, src);
4742 }
4743 
4744 void Assembler::subss(XMMRegister dst, XMMRegister src) {
4745   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4746   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true , /* uses_vl */ false);
4747   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
4748   emit_int8(0x5C);
4749   emit_int8((unsigned char)(0xC0 | encode));
4750 }
4751 
4752 void Assembler::subss(XMMRegister dst, Address src) {
4753   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4754   InstructionMark im(this);
4755   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4756   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
4757   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
4758   emit_int8(0x5C);
4759   emit_operand(dst, src);
4760 }
4761 
4762 void Assembler::testb(Register dst, int imm8) {
4763   NOT_LP64(assert(dst-&gt;has_byte_register(), "must have byte register"));
4764   (void) prefix_and_encode(dst-&gt;encoding(), true);
4765   emit_arith_b(0xF6, 0xC0, dst, imm8);
4766 }
4767 
4768 void Assembler::testb(Address dst, int imm8) {
4769   InstructionMark im(this);
4770   prefix(dst);
4771   emit_int8((unsigned char)0xF6);
4772   emit_operand(rax, dst, 1);
4773   emit_int8(imm8);
4774 }
4775 
4776 void Assembler::testl(Register dst, int32_t imm32) {
4777   // not using emit_arith because test
4778   // doesn't support sign-extension of
4779   // 8bit operands
4780   int encode = dst-&gt;encoding();
4781   if (encode == 0) {
4782     emit_int8((unsigned char)0xA9);
4783   } else {
4784     encode = prefix_and_encode(encode);
4785     emit_int8((unsigned char)0xF7);
4786     emit_int8((unsigned char)(0xC0 | encode));
4787   }
4788   emit_int32(imm32);
4789 }
4790 
4791 void Assembler::testl(Register dst, Register src) {
4792   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4793   emit_arith(0x85, 0xC0, dst, src);
4794 }
4795 
4796 void Assembler::testl(Register dst, Address src) {
4797   InstructionMark im(this);
4798   prefix(src, dst);
4799   emit_int8((unsigned char)0x85);
4800   emit_operand(dst, src);
4801 }
4802 
4803 void Assembler::tzcntl(Register dst, Register src) {
4804   assert(VM_Version::supports_bmi1(), "tzcnt instruction not supported");
4805   emit_int8((unsigned char)0xF3);
4806   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4807   emit_int8(0x0F);
4808   emit_int8((unsigned char)0xBC);
4809   emit_int8((unsigned char)0xC0 | encode);
4810 }
4811 
4812 void Assembler::tzcntq(Register dst, Register src) {
4813   assert(VM_Version::supports_bmi1(), "tzcnt instruction not supported");
4814   emit_int8((unsigned char)0xF3);
4815   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4816   emit_int8(0x0F);
4817   emit_int8((unsigned char)0xBC);
4818   emit_int8((unsigned char)(0xC0 | encode));
4819 }
4820 
4821 void Assembler::ucomisd(XMMRegister dst, Address src) {
4822   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4823   InstructionMark im(this);
4824   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4825   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
4826   attributes.set_rex_vex_w_reverted();
4827   simd_prefix(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4828   emit_int8(0x2E);
4829   emit_operand(dst, src);
4830 }
4831 
4832 void Assembler::ucomisd(XMMRegister dst, XMMRegister src) {
4833   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
4834   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4835   attributes.set_rex_vex_w_reverted();
4836   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
4837   emit_int8(0x2E);
4838   emit_int8((unsigned char)(0xC0 | encode));
4839 }
4840 
4841 void Assembler::ucomiss(XMMRegister dst, Address src) {
4842   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4843   InstructionMark im(this);
4844   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4845   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
4846   simd_prefix(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
4847   emit_int8(0x2E);
4848   emit_operand(dst, src);
4849 }
4850 
4851 void Assembler::ucomiss(XMMRegister dst, XMMRegister src) {
4852   NOT_LP64(assert(VM_Version::supports_sse(), ""));
4853   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4854   int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
4855   emit_int8(0x2E);
4856   emit_int8((unsigned char)(0xC0 | encode));
4857 }
4858 
4859 void Assembler::xabort(int8_t imm8) {
4860   emit_int8((unsigned char)0xC6);
4861   emit_int8((unsigned char)0xF8);
4862   emit_int8((unsigned char)(imm8 &amp; 0xFF));
4863 }
4864 
4865 void Assembler::xaddb(Address dst, Register src) {
4866   InstructionMark im(this);
4867   prefix(dst, src, true);
4868   emit_int8(0x0F);
4869   emit_int8((unsigned char)0xC0);
4870   emit_operand(src, dst);
4871 }
4872 
4873 void Assembler::xaddw(Address dst, Register src) {
4874   InstructionMark im(this);
4875   emit_int8(0x66);
4876   prefix(dst, src);
4877   emit_int8(0x0F);
4878   emit_int8((unsigned char)0xC1);
4879   emit_operand(src, dst);
4880 }
4881 
4882 void Assembler::xaddl(Address dst, Register src) {
4883   InstructionMark im(this);
4884   prefix(dst, src);
4885   emit_int8(0x0F);
4886   emit_int8((unsigned char)0xC1);
4887   emit_operand(src, dst);
4888 }
4889 
4890 void Assembler::xbegin(Label&amp; abort, relocInfo::relocType rtype) {
4891   InstructionMark im(this);
4892   relocate(rtype);
4893   if (abort.is_bound()) {
4894     address entry = target(abort);
4895     assert(entry != NULL, "abort entry NULL");
4896     intptr_t offset = entry - pc();
4897     emit_int8((unsigned char)0xC7);
4898     emit_int8((unsigned char)0xF8);
4899     emit_int32(offset - 6); // 2 opcode + 4 address
4900   } else {
4901     abort.add_patch_at(code(), locator());
4902     emit_int8((unsigned char)0xC7);
4903     emit_int8((unsigned char)0xF8);
4904     emit_int32(0);
4905   }
4906 }
4907 
4908 void Assembler::xchgb(Register dst, Address src) { // xchg
4909   InstructionMark im(this);
4910   prefix(src, dst, true);
4911   emit_int8((unsigned char)0x86);
4912   emit_operand(dst, src);
4913 }
4914 
4915 void Assembler::xchgw(Register dst, Address src) { // xchg
4916   InstructionMark im(this);
4917   emit_int8(0x66);
4918   prefix(src, dst);
4919   emit_int8((unsigned char)0x87);
4920   emit_operand(dst, src);
4921 }
4922 
4923 void Assembler::xchgl(Register dst, Address src) { // xchg
4924   InstructionMark im(this);
4925   prefix(src, dst);
4926   emit_int8((unsigned char)0x87);
4927   emit_operand(dst, src);
4928 }
4929 
4930 void Assembler::xchgl(Register dst, Register src) {
4931   int encode = prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4932   emit_int8((unsigned char)0x87);
4933   emit_int8((unsigned char)(0xC0 | encode));
4934 }
4935 
4936 void Assembler::xend() {
4937   emit_int8((unsigned char)0x0F);
4938   emit_int8((unsigned char)0x01);
4939   emit_int8((unsigned char)0xD5);
4940 }
4941 
4942 void Assembler::xgetbv() {
4943   emit_int8(0x0F);
4944   emit_int8(0x01);
4945   emit_int8((unsigned char)0xD0);
4946 }
4947 
4948 void Assembler::xorl(Register dst, int32_t imm32) {
4949   prefix(dst);
4950   emit_arith(0x81, 0xF0, dst, imm32);
4951 }
4952 
4953 void Assembler::xorl(Register dst, Address src) {
4954   InstructionMark im(this);
4955   prefix(src, dst);
4956   emit_int8(0x33);
4957   emit_operand(dst, src);
4958 }
4959 
4960 void Assembler::xorl(Register dst, Register src) {
4961   (void) prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding());
4962   emit_arith(0x33, 0xC0, dst, src);
4963 }
4964 
4965 void Assembler::xorb(Register dst, Address src) {
4966   InstructionMark im(this);
4967   prefix(src, dst);
4968   emit_int8(0x32);
4969   emit_operand(dst, src);
4970 }
4971 
4972 // AVX 3-operands scalar float-point arithmetic instructions
4973 
4974 void Assembler::vaddsd(XMMRegister dst, XMMRegister nds, Address src) {
4975   assert(VM_Version::supports_avx(), "");
4976   InstructionMark im(this);
4977   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4978   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
4979   attributes.set_rex_vex_w_reverted();
4980   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4981   emit_int8(0x58);
4982   emit_operand(dst, src);
4983 }
4984 
4985 void Assembler::vaddsd(XMMRegister dst, XMMRegister nds, XMMRegister src) {
4986   assert(VM_Version::supports_avx(), "");
4987   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4988   attributes.set_rex_vex_w_reverted();
4989   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
4990   emit_int8(0x58);
4991   emit_int8((unsigned char)(0xC0 | encode));
4992 }
4993 
4994 void Assembler::vaddss(XMMRegister dst, XMMRegister nds, Address src) {
4995   assert(VM_Version::supports_avx(), "");
4996   InstructionMark im(this);
4997   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
4998   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
4999   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5000   emit_int8(0x58);
5001   emit_operand(dst, src);
5002 }
5003 
5004 void Assembler::vaddss(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5005   assert(VM_Version::supports_avx(), "");
5006   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5007   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5008   emit_int8(0x58);
5009   emit_int8((unsigned char)(0xC0 | encode));
5010 }
5011 
5012 void Assembler::vdivsd(XMMRegister dst, XMMRegister nds, Address src) {
5013   assert(VM_Version::supports_avx(), "");
5014   InstructionMark im(this);
5015   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5016   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
5017   attributes.set_rex_vex_w_reverted();
5018   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5019   emit_int8(0x5E);
5020   emit_operand(dst, src);
5021 }
5022 
5023 void Assembler::vdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5024   assert(VM_Version::supports_avx(), "");
5025   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5026   attributes.set_rex_vex_w_reverted();
5027   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5028   emit_int8(0x5E);
5029   emit_int8((unsigned char)(0xC0 | encode));
5030 }
5031 
5032 void Assembler::vdivss(XMMRegister dst, XMMRegister nds, Address src) {
5033   assert(VM_Version::supports_avx(), "");
5034   InstructionMark im(this);
5035   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5036   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
5037   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5038   emit_int8(0x5E);
5039   emit_operand(dst, src);
5040 }
5041 
5042 void Assembler::vdivss(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5043   assert(VM_Version::supports_avx(), "");
5044   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5045   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5046   emit_int8(0x5E);
5047   emit_int8((unsigned char)(0xC0 | encode));
5048 }
5049 
5050 void Assembler::vfmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {
5051   assert(VM_Version::supports_fma(), "");
5052   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5053   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5054   emit_int8((unsigned char)0xB9);
5055   emit_int8((unsigned char)(0xC0 | encode));
5056 }
5057 
5058 void Assembler::vfmadd231ss(XMMRegister dst, XMMRegister src1, XMMRegister src2) {
5059   assert(VM_Version::supports_fma(), "");
5060   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5061   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5062   emit_int8((unsigned char)0xB9);
5063   emit_int8((unsigned char)(0xC0 | encode));
5064 }
5065 
5066 void Assembler::vmulsd(XMMRegister dst, XMMRegister nds, Address src) {
5067   assert(VM_Version::supports_avx(), "");
5068   InstructionMark im(this);
5069   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5070   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
5071   attributes.set_rex_vex_w_reverted();
5072   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5073   emit_int8(0x59);
5074   emit_operand(dst, src);
5075 }
5076 
5077 void Assembler::vmulsd(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5078   assert(VM_Version::supports_avx(), "");
5079   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5080   attributes.set_rex_vex_w_reverted();
5081   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5082   emit_int8(0x59);
5083   emit_int8((unsigned char)(0xC0 | encode));
5084 }
5085 
5086 void Assembler::vmulss(XMMRegister dst, XMMRegister nds, Address src) {
5087   assert(VM_Version::supports_avx(), "");
5088   InstructionMark im(this);
5089   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5090   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
5091   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5092   emit_int8(0x59);
5093   emit_operand(dst, src);
5094 }
5095 
5096 void Assembler::vmulss(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5097   assert(VM_Version::supports_avx(), "");
5098   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5099   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5100   emit_int8(0x59);
5101   emit_int8((unsigned char)(0xC0 | encode));
5102 }
5103 
5104 void Assembler::vsubsd(XMMRegister dst, XMMRegister nds, Address src) {
5105   assert(VM_Version::supports_avx(), "");
5106   InstructionMark im(this);
5107   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5108   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
5109   attributes.set_rex_vex_w_reverted();
5110   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5111   emit_int8(0x5C);
5112   emit_operand(dst, src);
5113 }
5114 
5115 void Assembler::vsubsd(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5116   assert(VM_Version::supports_avx(), "");
5117   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5118   attributes.set_rex_vex_w_reverted();
5119   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
5120   emit_int8(0x5C);
5121   emit_int8((unsigned char)(0xC0 | encode));
5122 }
5123 
5124 void Assembler::vsubss(XMMRegister dst, XMMRegister nds, Address src) {
5125   assert(VM_Version::supports_avx(), "");
5126   InstructionMark im(this);
5127   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5128   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
5129   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5130   emit_int8(0x5C);
5131   emit_operand(dst, src);
5132 }
5133 
5134 void Assembler::vsubss(XMMRegister dst, XMMRegister nds, XMMRegister src) {
5135   assert(VM_Version::supports_avx(), "");
5136   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
5137   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
5138   emit_int8(0x5C);
5139   emit_int8((unsigned char)(0xC0 | encode));
5140 }
5141 
5142 //====================VECTOR ARITHMETIC=====================================
5143 
5144 // Float-point vector arithmetic
5145 
5146 void Assembler::addpd(XMMRegister dst, XMMRegister src) {
5147   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5148   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5149   attributes.set_rex_vex_w_reverted();
5150   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5151   emit_int8(0x58);
5152   emit_int8((unsigned char)(0xC0 | encode));
5153 }
5154 
5155 void Assembler::addpd(XMMRegister dst, Address src) {
5156   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5157   InstructionMark im(this);
5158   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5159   attributes.set_rex_vex_w_reverted();
5160   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5161   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5162   emit_int8(0x58);
5163   emit_operand(dst, src);
5164 }
5165 
5166 
5167 void Assembler::addps(XMMRegister dst, XMMRegister src) {
5168   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5169   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5170   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5171   emit_int8(0x58);
5172   emit_int8((unsigned char)(0xC0 | encode));
5173 }
5174 
5175 void Assembler::vaddpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5176   assert(VM_Version::supports_avx(), "");
5177   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5178   attributes.set_rex_vex_w_reverted();
5179   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5180   emit_int8(0x58);
5181   emit_int8((unsigned char)(0xC0 | encode));
5182 }
5183 
5184 void Assembler::vaddps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5185   assert(VM_Version::supports_avx(), "");
5186   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5187   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5188   emit_int8(0x58);
5189   emit_int8((unsigned char)(0xC0 | encode));
5190 }
5191 
5192 void Assembler::vaddpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5193   assert(VM_Version::supports_avx(), "");
5194   InstructionMark im(this);
5195   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5196   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5197   attributes.set_rex_vex_w_reverted();
5198   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5199   emit_int8(0x58);
5200   emit_operand(dst, src);
5201 }
5202 
5203 void Assembler::vaddps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5204   assert(VM_Version::supports_avx(), "");
5205   InstructionMark im(this);
5206   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5207   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5208   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5209   emit_int8(0x58);
5210   emit_operand(dst, src);
5211 }
5212 
5213 void Assembler::subpd(XMMRegister dst, XMMRegister src) {
5214   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5215   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5216   attributes.set_rex_vex_w_reverted();
5217   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5218   emit_int8(0x5C);
5219   emit_int8((unsigned char)(0xC0 | encode));
5220 }
5221 
5222 void Assembler::subps(XMMRegister dst, XMMRegister src) {
5223   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5224   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5225   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5226   emit_int8(0x5C);
5227   emit_int8((unsigned char)(0xC0 | encode));
5228 }
5229 
5230 void Assembler::vsubpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5231   assert(VM_Version::supports_avx(), "");
5232   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5233   attributes.set_rex_vex_w_reverted();
5234   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5235   emit_int8(0x5C);
5236   emit_int8((unsigned char)(0xC0 | encode));
5237 }
5238 
5239 void Assembler::vsubps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5240   assert(VM_Version::supports_avx(), "");
5241   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5242   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5243   emit_int8(0x5C);
5244   emit_int8((unsigned char)(0xC0 | encode));
5245 }
5246 
5247 void Assembler::vsubpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5248   assert(VM_Version::supports_avx(), "");
5249   InstructionMark im(this);
5250   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5251   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5252   attributes.set_rex_vex_w_reverted();
5253   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5254   emit_int8(0x5C);
5255   emit_operand(dst, src);
5256 }
5257 
5258 void Assembler::vsubps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5259   assert(VM_Version::supports_avx(), "");
5260   InstructionMark im(this);
5261   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5262   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5263   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5264   emit_int8(0x5C);
5265   emit_operand(dst, src);
5266 }
5267 
5268 void Assembler::mulpd(XMMRegister dst, XMMRegister src) {
5269   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5270   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5271   attributes.set_rex_vex_w_reverted();
5272   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5273   emit_int8(0x59);
5274   emit_int8((unsigned char)(0xC0 | encode));
5275 }
5276 
5277 void Assembler::mulpd(XMMRegister dst, Address src) {
5278   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5279   InstructionMark im(this);
5280   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5281   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5282   attributes.set_rex_vex_w_reverted();
5283   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5284   emit_int8(0x59);
5285   emit_operand(dst, src);
5286 }
5287 
5288 void Assembler::mulps(XMMRegister dst, XMMRegister src) {
5289   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5290   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5291   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5292   emit_int8(0x59);
5293   emit_int8((unsigned char)(0xC0 | encode));
5294 }
5295 
5296 void Assembler::vmulpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5297   assert(VM_Version::supports_avx(), "");
5298   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5299   attributes.set_rex_vex_w_reverted();
5300   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5301   emit_int8(0x59);
5302   emit_int8((unsigned char)(0xC0 | encode));
5303 }
5304 
5305 void Assembler::vmulps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5306   assert(VM_Version::supports_avx(), "");
5307   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5308   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5309   emit_int8(0x59);
5310   emit_int8((unsigned char)(0xC0 | encode));
5311 }
5312 
5313 void Assembler::vmulpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5314   assert(VM_Version::supports_avx(), "");
5315   InstructionMark im(this);
5316   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5317   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5318   attributes.set_rex_vex_w_reverted();
5319   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5320   emit_int8(0x59);
5321   emit_operand(dst, src);
5322 }
5323 
5324 void Assembler::vmulps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5325   assert(VM_Version::supports_avx(), "");
5326   InstructionMark im(this);
5327   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5328   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5329   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5330   emit_int8(0x59);
5331   emit_operand(dst, src);
5332 }
5333 
5334 void Assembler::vfmadd231pd(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {
5335   assert(VM_Version::supports_fma(), "");
5336   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5337   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5338   emit_int8((unsigned char)0xB8);
5339   emit_int8((unsigned char)(0xC0 | encode));
5340 }
5341 
5342 void Assembler::vfmadd231ps(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {
5343   assert(VM_Version::supports_fma(), "");
5344   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5345   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5346   emit_int8((unsigned char)0xB8);
5347   emit_int8((unsigned char)(0xC0 | encode));
5348 }
5349 
5350 void Assembler::vfmadd231pd(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {
5351   assert(VM_Version::supports_fma(), "");
5352   InstructionMark im(this);
5353   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5354   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5355   vex_prefix(src2, src1-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5356   emit_int8((unsigned char)0xB8);
5357   emit_operand(dst, src2);
5358 }
5359 
5360 void Assembler::vfmadd231ps(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {
5361   assert(VM_Version::supports_fma(), "");
5362   InstructionMark im(this);
5363   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5364   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5365   vex_prefix(src2, src1-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5366   emit_int8((unsigned char)0xB8);
5367   emit_operand(dst, src2);
5368 }
5369 
5370 void Assembler::divpd(XMMRegister dst, XMMRegister src) {
5371   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5372   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5373   attributes.set_rex_vex_w_reverted();
5374   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5375   emit_int8(0x5E);
5376   emit_int8((unsigned char)(0xC0 | encode));
5377 }
5378 
5379 void Assembler::divps(XMMRegister dst, XMMRegister src) {
5380   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5381   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5382   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5383   emit_int8(0x5E);
5384   emit_int8((unsigned char)(0xC0 | encode));
5385 }
5386 
5387 void Assembler::vdivpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5388   assert(VM_Version::supports_avx(), "");
5389   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5390   attributes.set_rex_vex_w_reverted();
5391   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5392   emit_int8(0x5E);
5393   emit_int8((unsigned char)(0xC0 | encode));
5394 }
5395 
5396 void Assembler::vdivps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5397   assert(VM_Version::supports_avx(), "");
5398   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5399   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5400   emit_int8(0x5E);
5401   emit_int8((unsigned char)(0xC0 | encode));
5402 }
5403 
5404 void Assembler::vdivpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5405   assert(VM_Version::supports_avx(), "");
5406   InstructionMark im(this);
5407   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5408   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5409   attributes.set_rex_vex_w_reverted();
5410   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5411   emit_int8(0x5E);
5412   emit_operand(dst, src);
5413 }
5414 
5415 void Assembler::vdivps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5416   assert(VM_Version::supports_avx(), "");
5417   InstructionMark im(this);
5418   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5419   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5420   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5421   emit_int8(0x5E);
5422   emit_operand(dst, src);
5423 }
5424 
5425 void Assembler::vsqrtpd(XMMRegister dst, XMMRegister src, int vector_len) {
5426   assert(VM_Version::supports_avx(), "");
5427   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5428   attributes.set_rex_vex_w_reverted();
5429   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5430   emit_int8(0x51);
5431   emit_int8((unsigned char)(0xC0 | encode));
5432 }
5433 
5434 void Assembler::vsqrtpd(XMMRegister dst, Address src, int vector_len) {
5435   assert(VM_Version::supports_avx(), "");
5436   InstructionMark im(this);
5437   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5438   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5439   attributes.set_rex_vex_w_reverted();
5440   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5441   emit_int8(0x51);
5442   emit_operand(dst, src);
5443 }
5444 
5445 void Assembler::vsqrtps(XMMRegister dst, XMMRegister src, int vector_len) {
5446   assert(VM_Version::supports_avx(), "");
5447   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5448   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5449   emit_int8(0x51);
5450   emit_int8((unsigned char)(0xC0 | encode));
5451 }
5452 
5453 void Assembler::vsqrtps(XMMRegister dst, Address src, int vector_len) {
5454   assert(VM_Version::supports_avx(), "");
5455   InstructionMark im(this);
5456   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5457   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5458   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5459   emit_int8(0x51);
5460   emit_operand(dst, src);
5461 }
5462 
5463 void Assembler::andpd(XMMRegister dst, XMMRegister src) {
5464   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5465   InstructionAttr attributes(AVX_128bit, /* rex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5466   attributes.set_rex_vex_w_reverted();
5467   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5468   emit_int8(0x54);
5469   emit_int8((unsigned char)(0xC0 | encode));
5470 }
5471 
5472 void Assembler::andps(XMMRegister dst, XMMRegister src) {
5473   NOT_LP64(assert(VM_Version::supports_sse(), ""));
5474   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5475   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5476   emit_int8(0x54);
5477   emit_int8((unsigned char)(0xC0 | encode));
5478 }
5479 
5480 void Assembler::andps(XMMRegister dst, Address src) {
5481   NOT_LP64(assert(VM_Version::supports_sse(), ""));
5482   InstructionMark im(this);
5483   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5484   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5485   simd_prefix(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5486   emit_int8(0x54);
5487   emit_operand(dst, src);
5488 }
5489 
5490 void Assembler::andpd(XMMRegister dst, Address src) {
5491   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5492   InstructionMark im(this);
5493   InstructionAttr attributes(AVX_128bit, /* rex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5494   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5495   attributes.set_rex_vex_w_reverted();
5496   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5497   emit_int8(0x54);
5498   emit_operand(dst, src);
5499 }
5500 
5501 void Assembler::vandpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5502   assert(VM_Version::supports_avx(), "");
5503   InstructionAttr attributes(vector_len, /* vex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5504   attributes.set_rex_vex_w_reverted();
5505   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5506   emit_int8(0x54);
5507   emit_int8((unsigned char)(0xC0 | encode));
5508 }
5509 
5510 void Assembler::vandps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5511   assert(VM_Version::supports_avx(), "");
5512   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5513   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5514   emit_int8(0x54);
5515   emit_int8((unsigned char)(0xC0 | encode));
5516 }
5517 
5518 void Assembler::vandpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5519   assert(VM_Version::supports_avx(), "");
5520   InstructionMark im(this);
5521   InstructionAttr attributes(vector_len, /* vex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5522   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5523   attributes.set_rex_vex_w_reverted();
5524   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5525   emit_int8(0x54);
5526   emit_operand(dst, src);
5527 }
5528 
5529 void Assembler::vandps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5530   assert(VM_Version::supports_avx(), "");
5531   InstructionMark im(this);
5532   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5533   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5534   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5535   emit_int8(0x54);
5536   emit_operand(dst, src);
5537 }
5538 
5539 void Assembler::unpckhpd(XMMRegister dst, XMMRegister src) {
5540   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5541   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5542   attributes.set_rex_vex_w_reverted();
5543   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5544   emit_int8(0x15);
5545   emit_int8((unsigned char)(0xC0 | encode));
5546 }
5547 
5548 void Assembler::unpcklpd(XMMRegister dst, XMMRegister src) {
5549   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5550   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5551   attributes.set_rex_vex_w_reverted();
5552   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5553   emit_int8(0x14);
5554   emit_int8((unsigned char)(0xC0 | encode));
5555 }
5556 
5557 void Assembler::xorpd(XMMRegister dst, XMMRegister src) {
5558   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5559   InstructionAttr attributes(AVX_128bit, /* rex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5560   attributes.set_rex_vex_w_reverted();
5561   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5562   emit_int8(0x57);
5563   emit_int8((unsigned char)(0xC0 | encode));
5564 }
5565 
5566 void Assembler::xorps(XMMRegister dst, XMMRegister src) {
5567   NOT_LP64(assert(VM_Version::supports_sse(), ""));
5568   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5569   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5570   emit_int8(0x57);
5571   emit_int8((unsigned char)(0xC0 | encode));
5572 }
5573 
5574 void Assembler::xorpd(XMMRegister dst, Address src) {
5575   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5576   InstructionMark im(this);
5577   InstructionAttr attributes(AVX_128bit, /* rex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5578   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5579   attributes.set_rex_vex_w_reverted();
5580   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5581   emit_int8(0x57);
5582   emit_operand(dst, src);
5583 }
5584 
5585 void Assembler::xorps(XMMRegister dst, Address src) {
5586   NOT_LP64(assert(VM_Version::supports_sse(), ""));
5587   InstructionMark im(this);
5588   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5589   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5590   simd_prefix(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5591   emit_int8(0x57);
5592   emit_operand(dst, src);
5593 }
5594 
5595 void Assembler::vxorpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5596   assert(VM_Version::supports_avx(), "");
5597   InstructionAttr attributes(vector_len, /* vex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5598   attributes.set_rex_vex_w_reverted();
5599   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5600   emit_int8(0x57);
5601   emit_int8((unsigned char)(0xC0 | encode));
5602 }
5603 
5604 void Assembler::vxorps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5605   assert(VM_Version::supports_avx(), "");
5606   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5607   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5608   emit_int8(0x57);
5609   emit_int8((unsigned char)(0xC0 | encode));
5610 }
5611 
5612 void Assembler::vxorpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5613   assert(VM_Version::supports_avx(), "");
5614   InstructionMark im(this);
5615   InstructionAttr attributes(vector_len, /* vex_w */ !_legacy_mode_dq, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5616   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5617   attributes.set_rex_vex_w_reverted();
5618   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5619   emit_int8(0x57);
5620   emit_operand(dst, src);
5621 }
5622 
5623 void Assembler::vxorps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5624   assert(VM_Version::supports_avx(), "");
5625   InstructionMark im(this);
5626   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5627   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5628   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
5629   emit_int8(0x57);
5630   emit_operand(dst, src);
5631 }
5632 
5633 // Integer vector arithmetic
5634 void Assembler::vphaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5635   assert(VM_Version::supports_avx() &amp;&amp; (vector_len == 0) ||
5636          VM_Version::supports_avx2(), "256 bit integer vectors requires AVX2");
5637   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
5638   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5639   emit_int8(0x01);
5640   emit_int8((unsigned char)(0xC0 | encode));
5641 }
5642 
5643 void Assembler::vphaddd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5644   assert(VM_Version::supports_avx() &amp;&amp; (vector_len == 0) ||
5645          VM_Version::supports_avx2(), "256 bit integer vectors requires AVX2");
5646   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
5647   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5648   emit_int8(0x02);
5649   emit_int8((unsigned char)(0xC0 | encode));
5650 }
5651 
5652 void Assembler::paddb(XMMRegister dst, XMMRegister src) {
5653   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5654   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5655   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5656   emit_int8((unsigned char)0xFC);
5657   emit_int8((unsigned char)(0xC0 | encode));
5658 }
5659 
5660 void Assembler::paddw(XMMRegister dst, XMMRegister src) {
5661   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5662   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5663   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5664   emit_int8((unsigned char)0xFD);
5665   emit_int8((unsigned char)(0xC0 | encode));
5666 }
5667 
5668 void Assembler::paddd(XMMRegister dst, XMMRegister src) {
5669   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5670   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5671   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5672   emit_int8((unsigned char)0xFE);
5673   emit_int8((unsigned char)(0xC0 | encode));
5674 }
5675 
5676 void Assembler::paddd(XMMRegister dst, Address src) {
5677   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5678   InstructionMark im(this);
5679   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5680   simd_prefix(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5681   emit_int8((unsigned char)0xFE);
5682   emit_operand(dst, src);
5683 }
5684 
5685 void Assembler::paddq(XMMRegister dst, XMMRegister src) {
5686   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5687   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5688   attributes.set_rex_vex_w_reverted();
5689   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5690   emit_int8((unsigned char)0xD4);
5691   emit_int8((unsigned char)(0xC0 | encode));
5692 }
5693 
5694 void Assembler::phaddw(XMMRegister dst, XMMRegister src) {
5695   assert(VM_Version::supports_sse3(), "");
5696   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
5697   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5698   emit_int8(0x01);
5699   emit_int8((unsigned char)(0xC0 | encode));
5700 }
5701 
5702 void Assembler::phaddd(XMMRegister dst, XMMRegister src) {
5703   assert(VM_Version::supports_sse3(), "");
5704   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
5705   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5706   emit_int8(0x02);
5707   emit_int8((unsigned char)(0xC0 | encode));
5708 }
5709 
5710 void Assembler::vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5711   assert(UseAVX &gt; 0, "requires some form of AVX");
5712   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5713   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5714   emit_int8((unsigned char)0xFC);
5715   emit_int8((unsigned char)(0xC0 | encode));
5716 }
5717 
5718 void Assembler::vpaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5719   assert(UseAVX &gt; 0, "requires some form of AVX");
5720   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5721   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5722   emit_int8((unsigned char)0xFD);
5723   emit_int8((unsigned char)(0xC0 | encode));
5724 }
5725 
5726 void Assembler::vpaddd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5727   assert(UseAVX &gt; 0, "requires some form of AVX");
5728   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5729   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5730   emit_int8((unsigned char)0xFE);
5731   emit_int8((unsigned char)(0xC0 | encode));
5732 }
5733 
5734 void Assembler::vpaddq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5735   assert(UseAVX &gt; 0, "requires some form of AVX");
5736   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5737   attributes.set_rex_vex_w_reverted();
5738   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5739   emit_int8((unsigned char)0xD4);
5740   emit_int8((unsigned char)(0xC0 | encode));
5741 }
5742 
5743 void Assembler::vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5744   assert(UseAVX &gt; 0, "requires some form of AVX");
5745   InstructionMark im(this);
5746   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5747   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
5748   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5749   emit_int8((unsigned char)0xFC);
5750   emit_operand(dst, src);
5751 }
5752 
5753 void Assembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5754   assert(UseAVX &gt; 0, "requires some form of AVX");
5755   InstructionMark im(this);
5756   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5757   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
5758   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5759   emit_int8((unsigned char)0xFD);
5760   emit_operand(dst, src);
5761 }
5762 
5763 void Assembler::vpaddd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5764   assert(UseAVX &gt; 0, "requires some form of AVX");
5765   InstructionMark im(this);
5766   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5767   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5768   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5769   emit_int8((unsigned char)0xFE);
5770   emit_operand(dst, src);
5771 }
5772 
5773 void Assembler::vpaddq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5774   assert(UseAVX &gt; 0, "requires some form of AVX");
5775   InstructionMark im(this);
5776   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5777   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5778   attributes.set_rex_vex_w_reverted();
5779   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5780   emit_int8((unsigned char)0xD4);
5781   emit_operand(dst, src);
5782 }
5783 
5784 void Assembler::psubb(XMMRegister dst, XMMRegister src) {
5785   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5786   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5787   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5788   emit_int8((unsigned char)0xF8);
5789   emit_int8((unsigned char)(0xC0 | encode));
5790 }
5791 
5792 void Assembler::psubw(XMMRegister dst, XMMRegister src) {
5793   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5794   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5795   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5796   emit_int8((unsigned char)0xF9);
5797   emit_int8((unsigned char)(0xC0 | encode));
5798 }
5799 
5800 void Assembler::psubd(XMMRegister dst, XMMRegister src) {
5801   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5802   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5803   emit_int8((unsigned char)0xFA);
5804   emit_int8((unsigned char)(0xC0 | encode));
5805 }
5806 
5807 void Assembler::psubq(XMMRegister dst, XMMRegister src) {
5808   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5809   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5810   attributes.set_rex_vex_w_reverted();
5811   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5812   emit_int8((unsigned char)0xFB);
5813   emit_int8((unsigned char)(0xC0 | encode));
5814 }
5815 
5816 void Assembler::vpsubb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5817   assert(UseAVX &gt; 0, "requires some form of AVX");
5818   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5819   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5820   emit_int8((unsigned char)0xF8);
5821   emit_int8((unsigned char)(0xC0 | encode));
5822 }
5823 
5824 void Assembler::vpsubw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5825   assert(UseAVX &gt; 0, "requires some form of AVX");
5826   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5827   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5828   emit_int8((unsigned char)0xF9);
5829   emit_int8((unsigned char)(0xC0 | encode));
5830 }
5831 
5832 void Assembler::vpsubd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5833   assert(UseAVX &gt; 0, "requires some form of AVX");
5834   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5835   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5836   emit_int8((unsigned char)0xFA);
5837   emit_int8((unsigned char)(0xC0 | encode));
5838 }
5839 
5840 void Assembler::vpsubq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5841   assert(UseAVX &gt; 0, "requires some form of AVX");
5842   InstructionAttr attributes(vector_len, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5843   attributes.set_rex_vex_w_reverted();
5844   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5845   emit_int8((unsigned char)0xFB);
5846   emit_int8((unsigned char)(0xC0 | encode));
5847 }
5848 
5849 void Assembler::vpsubb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5850   assert(UseAVX &gt; 0, "requires some form of AVX");
5851   InstructionMark im(this);
5852   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5853   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
5854   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5855   emit_int8((unsigned char)0xF8);
5856   emit_operand(dst, src);
5857 }
5858 
5859 void Assembler::vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5860   assert(UseAVX &gt; 0, "requires some form of AVX");
5861   InstructionMark im(this);
5862   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5863   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
5864   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5865   emit_int8((unsigned char)0xF9);
5866   emit_operand(dst, src);
5867 }
5868 
5869 void Assembler::vpsubd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5870   assert(UseAVX &gt; 0, "requires some form of AVX");
5871   InstructionMark im(this);
5872   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5873   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5874   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5875   emit_int8((unsigned char)0xFA);
5876   emit_operand(dst, src);
5877 }
5878 
5879 void Assembler::vpsubq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5880   assert(UseAVX &gt; 0, "requires some form of AVX");
5881   InstructionMark im(this);
5882   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5883   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5884   attributes.set_rex_vex_w_reverted();
5885   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5886   emit_int8((unsigned char)0xFB);
5887   emit_operand(dst, src);
5888 }
5889 
5890 void Assembler::pmullw(XMMRegister dst, XMMRegister src) {
5891   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5892   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5893   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5894   emit_int8((unsigned char)0xD5);
5895   emit_int8((unsigned char)(0xC0 | encode));
5896 }
5897 
5898 void Assembler::pmulld(XMMRegister dst, XMMRegister src) {
5899   assert(VM_Version::supports_sse4_1(), "");
5900   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5901   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5902   emit_int8(0x40);
5903   emit_int8((unsigned char)(0xC0 | encode));
5904 }
5905 
5906 void Assembler::vpmullw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5907   assert(UseAVX &gt; 0, "requires some form of AVX");
5908   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5909   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5910   emit_int8((unsigned char)0xD5);
5911   emit_int8((unsigned char)(0xC0 | encode));
5912 }
5913 
5914 void Assembler::vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5915   assert(UseAVX &gt; 0, "requires some form of AVX");
5916   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5917   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5918   emit_int8(0x40);
5919   emit_int8((unsigned char)(0xC0 | encode));
5920 }
5921 
5922 void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
5923   assert(UseAVX &gt; 2, "requires some form of EVEX");
5924   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5925   attributes.set_is_evex_instruction();
5926   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5927   emit_int8(0x40);
5928   emit_int8((unsigned char)(0xC0 | encode));
5929 }
5930 
5931 void Assembler::vpmullw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5932   assert(UseAVX &gt; 0, "requires some form of AVX");
5933   InstructionMark im(this);
5934   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5935   attributes.set_address_attributes(/* tuple_type */ EVEX_FVM, /* input_size_in_bits */ EVEX_NObit);
5936   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5937   emit_int8((unsigned char)0xD5);
5938   emit_operand(dst, src);
5939 }
5940 
5941 void Assembler::vpmulld(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5942   assert(UseAVX &gt; 0, "requires some form of AVX");
5943   InstructionMark im(this);
5944   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5945   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
5946   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5947   emit_int8(0x40);
5948   emit_operand(dst, src);
5949 }
5950 
5951 void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
5952   assert(UseAVX &gt; 2, "requires some form of EVEX");
5953   InstructionMark im(this);
5954   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ _legacy_mode_dq, /* no_mask_reg */ true, /* uses_vl */ true);
5955   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
5956   attributes.set_is_evex_instruction();
5957   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
5958   emit_int8(0x40);
5959   emit_operand(dst, src);
5960 }
5961 
5962 // Shift packed integers left by specified number of bits.
5963 void Assembler::psllw(XMMRegister dst, int shift) {
5964   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5965   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5966   // XMM6 is for /6 encoding: 66 0F 71 /6 ib
5967   int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5968   emit_int8(0x71);
5969   emit_int8((unsigned char)(0xC0 | encode));
5970   emit_int8(shift &amp; 0xFF);
5971 }
5972 
5973 void Assembler::pslld(XMMRegister dst, int shift) {
5974   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5975   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5976   // XMM6 is for /6 encoding: 66 0F 72 /6 ib
5977   int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5978   emit_int8(0x72);
5979   emit_int8((unsigned char)(0xC0 | encode));
5980   emit_int8(shift &amp; 0xFF);
5981 }
5982 
5983 void Assembler::psllq(XMMRegister dst, int shift) {
5984   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5985   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
5986   // XMM6 is for /6 encoding: 66 0F 73 /6 ib
5987   int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5988   emit_int8(0x73);
5989   emit_int8((unsigned char)(0xC0 | encode));
5990   emit_int8(shift &amp; 0xFF);
5991 }
5992 
5993 void Assembler::psllw(XMMRegister dst, XMMRegister shift) {
5994   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
5995   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
5996   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
5997   emit_int8((unsigned char)0xF1);
5998   emit_int8((unsigned char)(0xC0 | encode));
5999 }
6000 
6001 void Assembler::pslld(XMMRegister dst, XMMRegister shift) {
6002   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6003   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6004   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6005   emit_int8((unsigned char)0xF2);
6006   emit_int8((unsigned char)(0xC0 | encode));
6007 }
6008 
6009 void Assembler::psllq(XMMRegister dst, XMMRegister shift) {
6010   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6011   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6012   attributes.set_rex_vex_w_reverted();
6013   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6014   emit_int8((unsigned char)0xF3);
6015   emit_int8((unsigned char)(0xC0 | encode));
6016 }
6017 
6018 void Assembler::vpsllw(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6019   assert(UseAVX &gt; 0, "requires some form of AVX");
6020   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6021   // XMM6 is for /6 encoding: 66 0F 71 /6 ib
6022   int encode = vex_prefix_and_encode(xmm6-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6023   emit_int8(0x71);
6024   emit_int8((unsigned char)(0xC0 | encode));
6025   emit_int8(shift &amp; 0xFF);
6026 }
6027 
6028 void Assembler::vpslld(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6029   assert(UseAVX &gt; 0, "requires some form of AVX");
6030   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6031   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6032   // XMM6 is for /6 encoding: 66 0F 72 /6 ib
6033   int encode = vex_prefix_and_encode(xmm6-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6034   emit_int8(0x72);
6035   emit_int8((unsigned char)(0xC0 | encode));
6036   emit_int8(shift &amp; 0xFF);
6037 }
6038 
6039 void Assembler::vpsllq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6040   assert(UseAVX &gt; 0, "requires some form of AVX");
6041   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6042   attributes.set_rex_vex_w_reverted();
6043   // XMM6 is for /6 encoding: 66 0F 73 /6 ib
6044   int encode = vex_prefix_and_encode(xmm6-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6045   emit_int8(0x73);
6046   emit_int8((unsigned char)(0xC0 | encode));
6047   emit_int8(shift &amp; 0xFF);
6048 }
6049 
6050 void Assembler::vpsllw(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6051   assert(UseAVX &gt; 0, "requires some form of AVX");
6052   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6053   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6054   emit_int8((unsigned char)0xF1);
6055   emit_int8((unsigned char)(0xC0 | encode));
6056 }
6057 
6058 void Assembler::vpslld(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6059   assert(UseAVX &gt; 0, "requires some form of AVX");
6060   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6061   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6062   emit_int8((unsigned char)0xF2);
6063   emit_int8((unsigned char)(0xC0 | encode));
6064 }
6065 
6066 void Assembler::vpsllq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6067   assert(UseAVX &gt; 0, "requires some form of AVX");
6068   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6069   attributes.set_rex_vex_w_reverted();
6070   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6071   emit_int8((unsigned char)0xF3);
6072   emit_int8((unsigned char)(0xC0 | encode));
6073 }
6074 
6075 // Shift packed integers logically right by specified number of bits.
6076 void Assembler::psrlw(XMMRegister dst, int shift) {
6077   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6078   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6079   // XMM2 is for /2 encoding: 66 0F 71 /2 ib
6080   int encode = simd_prefix_and_encode(xmm2, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6081   emit_int8(0x71);
6082   emit_int8((unsigned char)(0xC0 | encode));
6083   emit_int8(shift &amp; 0xFF);
6084 }
6085 
6086 void Assembler::psrld(XMMRegister dst, int shift) {
6087   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6088   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6089   // XMM2 is for /2 encoding: 66 0F 72 /2 ib
6090   int encode = simd_prefix_and_encode(xmm2, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6091   emit_int8(0x72);
6092   emit_int8((unsigned char)(0xC0 | encode));
6093   emit_int8(shift &amp; 0xFF);
6094 }
6095 
6096 void Assembler::psrlq(XMMRegister dst, int shift) {
6097   // Do not confuse it with psrldq SSE2 instruction which
6098   // shifts 128 bit value in xmm register by number of bytes.
6099   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6100   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6101   attributes.set_rex_vex_w_reverted();
6102   // XMM2 is for /2 encoding: 66 0F 73 /2 ib
6103   int encode = simd_prefix_and_encode(xmm2, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6104   emit_int8(0x73);
6105   emit_int8((unsigned char)(0xC0 | encode));
6106   emit_int8(shift &amp; 0xFF);
6107 }
6108 
6109 void Assembler::psrlw(XMMRegister dst, XMMRegister shift) {
6110   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6111   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6112   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6113   emit_int8((unsigned char)0xD1);
6114   emit_int8((unsigned char)(0xC0 | encode));
6115 }
6116 
6117 void Assembler::psrld(XMMRegister dst, XMMRegister shift) {
6118   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6119   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6120   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6121   emit_int8((unsigned char)0xD2);
6122   emit_int8((unsigned char)(0xC0 | encode));
6123 }
6124 
6125 void Assembler::psrlq(XMMRegister dst, XMMRegister shift) {
6126   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6127   InstructionAttr attributes(AVX_128bit, /* rex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6128   attributes.set_rex_vex_w_reverted();
6129   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6130   emit_int8((unsigned char)0xD3);
6131   emit_int8((unsigned char)(0xC0 | encode));
6132 }
6133 
6134 void Assembler::vpsrlw(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6135   assert(UseAVX &gt; 0, "requires some form of AVX");
6136   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6137   // XMM2 is for /2 encoding: 66 0F 71 /2 ib
6138   int encode = vex_prefix_and_encode(xmm2-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6139   emit_int8(0x71);
6140   emit_int8((unsigned char)(0xC0 | encode));
6141   emit_int8(shift &amp; 0xFF);
6142 }
6143 
6144 void Assembler::vpsrld(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6145   assert(UseAVX &gt; 0, "requires some form of AVX");
6146   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6147   // XMM2 is for /2 encoding: 66 0F 72 /2 ib
6148   int encode = vex_prefix_and_encode(xmm2-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6149   emit_int8(0x72);
6150   emit_int8((unsigned char)(0xC0 | encode));
6151   emit_int8(shift &amp; 0xFF);
6152 }
6153 
6154 void Assembler::vpsrlq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6155   assert(UseAVX &gt; 0, "requires some form of AVX");
6156   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6157   attributes.set_rex_vex_w_reverted();
6158   // XMM2 is for /2 encoding: 66 0F 73 /2 ib
6159   int encode = vex_prefix_and_encode(xmm2-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6160   emit_int8(0x73);
6161   emit_int8((unsigned char)(0xC0 | encode));
6162   emit_int8(shift &amp; 0xFF);
6163 }
6164 
6165 void Assembler::vpsrlw(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6166   assert(UseAVX &gt; 0, "requires some form of AVX");
6167   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6168   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6169   emit_int8((unsigned char)0xD1);
6170   emit_int8((unsigned char)(0xC0 | encode));
6171 }
6172 
6173 void Assembler::vpsrld(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6174   assert(UseAVX &gt; 0, "requires some form of AVX");
6175   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6176   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6177   emit_int8((unsigned char)0xD2);
6178   emit_int8((unsigned char)(0xC0 | encode));
6179 }
6180 
6181 void Assembler::vpsrlq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6182   assert(UseAVX &gt; 0, "requires some form of AVX");
6183   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6184   attributes.set_rex_vex_w_reverted();
6185   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6186   emit_int8((unsigned char)0xD3);
6187   emit_int8((unsigned char)(0xC0 | encode));
6188 }
6189 
6190 void Assembler::evpsrlvw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6191   assert(VM_Version::supports_avx512bw(), "");
6192   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6193   attributes.set_is_evex_instruction();
6194   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6195   emit_int8(0x10);
6196   emit_int8((unsigned char)(0xC0 | encode));
6197 }
6198 
6199 void Assembler::evpsllvw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6200   assert(VM_Version::supports_avx512bw(), "");
6201   InstructionAttr attributes(vector_len, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6202   attributes.set_is_evex_instruction();
6203   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6204   emit_int8(0x12);
6205   emit_int8((unsigned char)(0xC0 | encode));
6206 }
6207 
6208 // Shift packed integers arithmetically right by specified number of bits.
6209 void Assembler::psraw(XMMRegister dst, int shift) {
6210   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6211   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6212   // XMM4 is for /4 encoding: 66 0F 71 /4 ib
6213   int encode = simd_prefix_and_encode(xmm4, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6214   emit_int8(0x71);
6215   emit_int8((unsigned char)(0xC0 | encode));
6216   emit_int8(shift &amp; 0xFF);
6217 }
6218 
6219 void Assembler::psrad(XMMRegister dst, int shift) {
6220   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6221   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6222   // XMM4 is for /4 encoding: 66 0F 72 /4 ib
6223   int encode = simd_prefix_and_encode(xmm4, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6224   emit_int8(0x72);
6225   emit_int8((unsigned char)(0xC0 | encode));
6226   emit_int8(shift &amp; 0xFF);
6227 }
6228 
6229 void Assembler::psraw(XMMRegister dst, XMMRegister shift) {
6230   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6231   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6232   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6233   emit_int8((unsigned char)0xE1);
6234   emit_int8((unsigned char)(0xC0 | encode));
6235 }
6236 
6237 void Assembler::psrad(XMMRegister dst, XMMRegister shift) {
6238   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6239   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6240   int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6241   emit_int8((unsigned char)0xE2);
6242   emit_int8((unsigned char)(0xC0 | encode));
6243 }
6244 
6245 void Assembler::vpsraw(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6246   assert(UseAVX &gt; 0, "requires some form of AVX");
6247   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6248   // XMM4 is for /4 encoding: 66 0F 71 /4 ib
6249   int encode = vex_prefix_and_encode(xmm4-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6250   emit_int8(0x71);
6251   emit_int8((unsigned char)(0xC0 | encode));
6252   emit_int8(shift &amp; 0xFF);
6253 }
6254 
6255 void Assembler::vpsrad(XMMRegister dst, XMMRegister src, int shift, int vector_len) {
6256   assert(UseAVX &gt; 0, "requires some form of AVX");
6257   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6258   // XMM4 is for /4 encoding: 66 0F 71 /4 ib
6259   int encode = vex_prefix_and_encode(xmm4-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6260   emit_int8(0x72);
6261   emit_int8((unsigned char)(0xC0 | encode));
6262   emit_int8(shift &amp; 0xFF);
6263 }
6264 
6265 void Assembler::vpsraw(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6266   assert(UseAVX &gt; 0, "requires some form of AVX");
6267   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6268   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6269   emit_int8((unsigned char)0xE1);
6270   emit_int8((unsigned char)(0xC0 | encode));
6271 }
6272 
6273 void Assembler::vpsrad(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {
6274   assert(UseAVX &gt; 0, "requires some form of AVX");
6275   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6276   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src-&gt;encoding(), shift-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6277   emit_int8((unsigned char)0xE2);
6278   emit_int8((unsigned char)(0xC0 | encode));
6279 }
6280 
6281 
6282 // logical operations packed integers
6283 void Assembler::pand(XMMRegister dst, XMMRegister src) {
6284   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6285   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6286   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6287   emit_int8((unsigned char)0xDB);
6288   emit_int8((unsigned char)(0xC0 | encode));
6289 }
6290 
6291 void Assembler::vpand(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6292   assert(UseAVX &gt; 0, "requires some form of AVX");
6293   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6294   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6295   emit_int8((unsigned char)0xDB);
6296   emit_int8((unsigned char)(0xC0 | encode));
6297 }
6298 
6299 void Assembler::vpand(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
6300   assert(UseAVX &gt; 0, "requires some form of AVX");
6301   InstructionMark im(this);
6302   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6303   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
6304   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6305   emit_int8((unsigned char)0xDB);
6306   emit_operand(dst, src);
6307 }
6308 
6309 void Assembler::vpandq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6310   assert(VM_Version::supports_evex(), "");
6311   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6312   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6313   emit_int8((unsigned char)0xDB);
6314   emit_int8((unsigned char)(0xC0 | encode));
6315 }
6316 
6317 
6318 void Assembler::pandn(XMMRegister dst, XMMRegister src) {
6319   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6320   InstructionAttr attributes(AVX_128bit, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6321   attributes.set_rex_vex_w_reverted();
6322   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6323   emit_int8((unsigned char)0xDF);
6324   emit_int8((unsigned char)(0xC0 | encode));
6325 }
6326 
6327 void Assembler::vpandn(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6328   assert(UseAVX &gt; 0, "requires some form of AVX");
6329   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6330   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6331   emit_int8((unsigned char)0xDF);
6332   emit_int8((unsigned char)(0xC0 | encode));
6333 }
6334 
6335 
6336 void Assembler::por(XMMRegister dst, XMMRegister src) {
6337   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6338   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6339   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6340   emit_int8((unsigned char)0xEB);
6341   emit_int8((unsigned char)(0xC0 | encode));
6342 }
6343 
6344 void Assembler::vpor(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6345   assert(UseAVX &gt; 0, "requires some form of AVX");
6346   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6347   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6348   emit_int8((unsigned char)0xEB);
6349   emit_int8((unsigned char)(0xC0 | encode));
6350 }
6351 
6352 void Assembler::vpor(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
6353   assert(UseAVX &gt; 0, "requires some form of AVX");
6354   InstructionMark im(this);
6355   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6356   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
6357   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6358   emit_int8((unsigned char)0xEB);
6359   emit_operand(dst, src);
6360 }
6361 
6362 void Assembler::vporq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6363   assert(VM_Version::supports_evex(), "");
6364   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6365   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6366   emit_int8((unsigned char)0xEB);
6367   emit_int8((unsigned char)(0xC0 | encode));
6368 }
6369 
6370 
6371 void Assembler::pxor(XMMRegister dst, XMMRegister src) {
6372   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
6373   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6374   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6375   emit_int8((unsigned char)0xEF);
6376   emit_int8((unsigned char)(0xC0 | encode));
6377 }
6378 
6379 void Assembler::vpxor(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6380   assert(UseAVX &gt; 0, "requires some form of AVX");
6381   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6382   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6383   emit_int8((unsigned char)0xEF);
6384   emit_int8((unsigned char)(0xC0 | encode));
6385 }
6386 
6387 void Assembler::vpxor(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
6388   assert(UseAVX &gt; 0, "requires some form of AVX");
6389   InstructionMark im(this);
6390   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6391   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_32bit);
6392   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6393   emit_int8((unsigned char)0xEF);
6394   emit_operand(dst, src);
6395 }
6396 
6397 void Assembler::evpxorq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
6398   assert(VM_Version::supports_evex(), "requires EVEX support");
6399   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6400   attributes.set_is_evex_instruction();
6401   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6402   emit_int8((unsigned char)0xEF);
6403   emit_int8((unsigned char)(0xC0 | encode));
6404 }
6405 
6406 void Assembler::evpxorq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
6407   assert(VM_Version::supports_evex(), "requires EVEX support");
6408   assert(dst != xnoreg, "sanity");
6409   InstructionMark im(this);
6410   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6411   attributes.set_is_evex_instruction();
6412   attributes.set_address_attributes(/* tuple_type */ EVEX_FV, /* input_size_in_bits */ EVEX_64bit);
6413   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
6414   emit_int8((unsigned char)0xEF);
6415   emit_operand(dst, src);
6416 }
6417 
6418 
6419 // vinserti forms
6420 
6421 void Assembler::vinserti128(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6422   assert(VM_Version::supports_avx2(), "");
6423   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6424   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6425   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6426   emit_int8(0x38);
6427   emit_int8((unsigned char)(0xC0 | encode));
6428   // 0x00 - insert into lower 128 bits
6429   // 0x01 - insert into upper 128 bits
6430   emit_int8(imm8 &amp; 0x01);
6431 }
6432 
6433 void Assembler::vinserti128(XMMRegister dst, XMMRegister nds, Address src, uint8_t imm8) {
6434   assert(VM_Version::supports_avx2(), "");
6435   assert(dst != xnoreg, "sanity");
6436   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6437   InstructionMark im(this);
6438   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6439   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6440   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6441   emit_int8(0x38);
6442   emit_operand(dst, src);
6443   // 0x00 - insert into lower 128 bits
6444   // 0x01 - insert into upper 128 bits
6445   emit_int8(imm8 &amp; 0x01);
6446 }
6447 
6448 void Assembler::vinserti32x4(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6449   assert(VM_Version::supports_evex(), "");
6450   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6451   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6452   attributes.set_is_evex_instruction();
6453   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6454   emit_int8(0x38);
6455   emit_int8((unsigned char)(0xC0 | encode));
6456   // 0x00 - insert into q0 128 bits (0..127)
6457   // 0x01 - insert into q1 128 bits (128..255)
6458   // 0x02 - insert into q2 128 bits (256..383)
6459   // 0x03 - insert into q3 128 bits (384..511)
6460   emit_int8(imm8 &amp; 0x03);
6461 }
6462 
6463 void Assembler::vinserti32x4(XMMRegister dst, XMMRegister nds, Address src, uint8_t imm8) {
6464   assert(VM_Version::supports_avx(), "");
6465   assert(dst != xnoreg, "sanity");
6466   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6467   InstructionMark im(this);
6468   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6469   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6470   attributes.set_is_evex_instruction();
6471   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6472   emit_int8(0x18);
6473   emit_operand(dst, src);
6474   // 0x00 - insert into q0 128 bits (0..127)
6475   // 0x01 - insert into q1 128 bits (128..255)
6476   // 0x02 - insert into q2 128 bits (256..383)
6477   // 0x03 - insert into q3 128 bits (384..511)
6478   emit_int8(imm8 &amp; 0x03);
6479 }
6480 
6481 void Assembler::vinserti64x4(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6482   assert(VM_Version::supports_evex(), "");
6483   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6484   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6485   attributes.set_is_evex_instruction();
6486   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6487   emit_int8(0x3A);
6488   emit_int8((unsigned char)(0xC0 | encode));
6489   // 0x00 - insert into lower 256 bits
6490   // 0x01 - insert into upper 256 bits
6491   emit_int8(imm8 &amp; 0x01);
6492 }
6493 
6494 
6495 // vinsertf forms
6496 
6497 void Assembler::vinsertf128(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6498   assert(VM_Version::supports_avx(), "");
6499   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6500   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6501   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6502   emit_int8(0x18);
6503   emit_int8((unsigned char)(0xC0 | encode));
6504   // 0x00 - insert into lower 128 bits
6505   // 0x01 - insert into upper 128 bits
6506   emit_int8(imm8 &amp; 0x01);
6507 }
6508 
6509 void Assembler::vinsertf128(XMMRegister dst, XMMRegister nds, Address src, uint8_t imm8) {
6510   assert(VM_Version::supports_avx(), "");
6511   assert(dst != xnoreg, "sanity");
6512   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6513   InstructionMark im(this);
6514   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6515   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6516   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6517   emit_int8(0x18);
6518   emit_operand(dst, src);
6519   // 0x00 - insert into lower 128 bits
6520   // 0x01 - insert into upper 128 bits
6521   emit_int8(imm8 &amp; 0x01);
6522 }
6523 
6524 void Assembler::vinsertf32x4(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6525   assert(VM_Version::supports_avx2(), "");
6526   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6527   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6528   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6529   emit_int8(0x18);
6530   emit_int8((unsigned char)(0xC0 | encode));
6531   // 0x00 - insert into q0 128 bits (0..127)
6532   // 0x01 - insert into q1 128 bits (128..255)
6533   // 0x02 - insert into q0 128 bits (256..383)
6534   // 0x03 - insert into q1 128 bits (384..512)
6535   emit_int8(imm8 &amp; 0x03);
6536 }
6537 
6538 void Assembler::vinsertf32x4(XMMRegister dst, XMMRegister nds, Address src, uint8_t imm8) {
6539   assert(VM_Version::supports_avx(), "");
6540   assert(dst != xnoreg, "sanity");
6541   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6542   InstructionMark im(this);
6543   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6544   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6545   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6546   emit_int8(0x18);
6547   emit_operand(dst, src);
6548   // 0x00 - insert into q0 128 bits (0..127)
6549   // 0x01 - insert into q1 128 bits (128..255)
6550   // 0x02 - insert into q0 128 bits (256..383)
6551   // 0x03 - insert into q1 128 bits (384..512)
6552   emit_int8(imm8 &amp; 0x03);
6553 }
6554 
6555 void Assembler::vinsertf64x4(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8) {
6556   assert(VM_Version::supports_evex(), "");
6557   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6558   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6559   attributes.set_is_evex_instruction();
6560   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6561   emit_int8(0x1A);
6562   emit_int8((unsigned char)(0xC0 | encode));
6563   // 0x00 - insert into lower 256 bits
6564   // 0x01 - insert into upper 256 bits
6565   emit_int8(imm8 &amp; 0x01);
6566 }
6567 
6568 void Assembler::vinsertf64x4(XMMRegister dst, XMMRegister nds, Address src, uint8_t imm8) {
6569   assert(VM_Version::supports_evex(), "");
6570   assert(dst != xnoreg, "sanity");
6571   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6572   InstructionMark im(this);
6573   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6574   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_64bit);
6575   attributes.set_is_evex_instruction();
6576   vex_prefix(src, nds-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6577   emit_int8(0x1A);
6578   emit_operand(dst, src);
6579   // 0x00 - insert into lower 256 bits
6580   // 0x01 - insert into upper 256 bits
6581   emit_int8(imm8 &amp; 0x01);
6582 }
6583 
6584 
6585 // vextracti forms
6586 
6587 void Assembler::vextracti128(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6588   assert(VM_Version::supports_avx2(), "");
6589   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6590   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6591   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6592   emit_int8(0x39);
6593   emit_int8((unsigned char)(0xC0 | encode));
6594   // 0x00 - extract from lower 128 bits
6595   // 0x01 - extract from upper 128 bits
6596   emit_int8(imm8 &amp; 0x01);
6597 }
6598 
6599 void Assembler::vextracti128(Address dst, XMMRegister src, uint8_t imm8) {
6600   assert(VM_Version::supports_avx2(), "");
6601   assert(src != xnoreg, "sanity");
6602   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6603   InstructionMark im(this);
6604   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6605   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6606   attributes.reset_is_clear_context();
6607   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6608   emit_int8(0x39);
6609   emit_operand(src, dst);
6610   // 0x00 - extract from lower 128 bits
6611   // 0x01 - extract from upper 128 bits
6612   emit_int8(imm8 &amp; 0x01);
6613 }
6614 
6615 void Assembler::vextracti32x4(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6616   assert(VM_Version::supports_evex(), "");
6617   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6618   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6619   attributes.set_is_evex_instruction();
6620   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6621   emit_int8(0x39);
6622   emit_int8((unsigned char)(0xC0 | encode));
6623   // 0x00 - extract from bits 127:0
6624   // 0x01 - extract from bits 255:128
6625   // 0x02 - extract from bits 383:256
6626   // 0x03 - extract from bits 511:384
6627   emit_int8(imm8 &amp; 0x03);
6628 }
6629 
6630 void Assembler::vextracti32x4(Address dst, XMMRegister src, uint8_t imm8) {
6631   assert(VM_Version::supports_evex(), "");
6632   assert(src != xnoreg, "sanity");
6633   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6634   InstructionMark im(this);
6635   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6636   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6637   attributes.reset_is_clear_context();
6638   attributes.set_is_evex_instruction();
6639   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6640   emit_int8(0x39);
6641   emit_operand(src, dst);
6642   // 0x00 - extract from bits 127:0
6643   // 0x01 - extract from bits 255:128
6644   // 0x02 - extract from bits 383:256
6645   // 0x03 - extract from bits 511:384
6646   emit_int8(imm8 &amp; 0x03);
6647 }
6648 
6649 void Assembler::vextracti64x2(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6650   assert(VM_Version::supports_avx512dq(), "");
6651   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6652   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6653   attributes.set_is_evex_instruction();
6654   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6655   emit_int8(0x39);
6656   emit_int8((unsigned char)(0xC0 | encode));
6657   // 0x00 - extract from bits 127:0
6658   // 0x01 - extract from bits 255:128
6659   // 0x02 - extract from bits 383:256
6660   // 0x03 - extract from bits 511:384
6661   emit_int8(imm8 &amp; 0x03);
6662 }
6663 
6664 void Assembler::vextracti64x4(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6665   assert(VM_Version::supports_evex(), "");
6666   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6667   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6668   attributes.set_is_evex_instruction();
6669   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6670   emit_int8(0x3B);
6671   emit_int8((unsigned char)(0xC0 | encode));
6672   // 0x00 - extract from lower 256 bits
6673   // 0x01 - extract from upper 256 bits
6674   emit_int8(imm8 &amp; 0x01);
6675 }
6676 
6677 void Assembler::vextracti64x4(Address dst, XMMRegister src, uint8_t imm8) {
6678   assert(VM_Version::supports_evex(), "");
6679   assert(src != xnoreg, "sanity");
6680   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6681   InstructionMark im(this);
6682   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6683   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_64bit);
6684   attributes.reset_is_clear_context();
6685   attributes.set_is_evex_instruction();
6686   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6687   emit_int8(0x38);
6688   emit_operand(src, dst);
6689   // 0x00 - extract from lower 256 bits
6690   // 0x01 - extract from upper 256 bits
6691   emit_int8(imm8 &amp; 0x01);
6692 }
6693 // vextractf forms
6694 
6695 void Assembler::vextractf128(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6696   assert(VM_Version::supports_avx(), "");
6697   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6698   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6699   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6700   emit_int8(0x19);
6701   emit_int8((unsigned char)(0xC0 | encode));
6702   // 0x00 - extract from lower 128 bits
6703   // 0x01 - extract from upper 128 bits
6704   emit_int8(imm8 &amp; 0x01);
6705 }
6706 
6707 void Assembler::vextractf128(Address dst, XMMRegister src, uint8_t imm8) {
6708   assert(VM_Version::supports_avx(), "");
6709   assert(src != xnoreg, "sanity");
6710   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6711   InstructionMark im(this);
6712   InstructionAttr attributes(AVX_256bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6713   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6714   attributes.reset_is_clear_context();
6715   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6716   emit_int8(0x19);
6717   emit_operand(src, dst);
6718   // 0x00 - extract from lower 128 bits
6719   // 0x01 - extract from upper 128 bits
6720   emit_int8(imm8 &amp; 0x01);
6721 }
6722 
6723 void Assembler::vextractf32x4(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6724   assert(VM_Version::supports_evex(), "");
6725   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6726   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6727   attributes.set_is_evex_instruction();
6728   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6729   emit_int8(0x19);
6730   emit_int8((unsigned char)(0xC0 | encode));
6731   // 0x00 - extract from bits 127:0
6732   // 0x01 - extract from bits 255:128
6733   // 0x02 - extract from bits 383:256
6734   // 0x03 - extract from bits 511:384
6735   emit_int8(imm8 &amp; 0x03);
6736 }
6737 
6738 void Assembler::vextractf32x4(Address dst, XMMRegister src, uint8_t imm8) {
6739   assert(VM_Version::supports_evex(), "");
6740   assert(src != xnoreg, "sanity");
6741   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6742   InstructionMark im(this);
6743   InstructionAttr attributes(AVX_512bit, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6744   attributes.set_address_attributes(/* tuple_type */ EVEX_T4, /* input_size_in_bits */ EVEX_32bit);
6745   attributes.reset_is_clear_context();
6746   attributes.set_is_evex_instruction();
6747   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6748   emit_int8(0x19);
6749   emit_operand(src, dst);
6750   // 0x00 - extract from bits 127:0
6751   // 0x01 - extract from bits 255:128
6752   // 0x02 - extract from bits 383:256
6753   // 0x03 - extract from bits 511:384
6754   emit_int8(imm8 &amp; 0x03);
6755 }
6756 
6757 void Assembler::vextractf64x2(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6758   assert(VM_Version::supports_avx512dq(), "");
6759   assert(imm8 &lt;= 0x03, "imm8: %u", imm8);
6760   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6761   attributes.set_is_evex_instruction();
6762   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6763   emit_int8(0x19);
6764   emit_int8((unsigned char)(0xC0 | encode));
6765   // 0x00 - extract from bits 127:0
6766   // 0x01 - extract from bits 255:128
6767   // 0x02 - extract from bits 383:256
6768   // 0x03 - extract from bits 511:384
6769   emit_int8(imm8 &amp; 0x03);
6770 }
6771 
6772 void Assembler::vextractf64x4(XMMRegister dst, XMMRegister src, uint8_t imm8) {
6773   assert(VM_Version::supports_evex(), "");
6774   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6775   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6776   attributes.set_is_evex_instruction();
6777   int encode = vex_prefix_and_encode(src-&gt;encoding(), 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6778   emit_int8(0x1B);
6779   emit_int8((unsigned char)(0xC0 | encode));
6780   // 0x00 - extract from lower 256 bits
6781   // 0x01 - extract from upper 256 bits
6782   emit_int8(imm8 &amp; 0x01);
6783 }
6784 
6785 void Assembler::vextractf64x4(Address dst, XMMRegister src, uint8_t imm8) {
6786   assert(VM_Version::supports_evex(), "");
6787   assert(src != xnoreg, "sanity");
6788   assert(imm8 &lt;= 0x01, "imm8: %u", imm8);
6789   InstructionMark im(this);
6790   InstructionAttr attributes(AVX_512bit, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6791   attributes.set_address_attributes(/* tuple_type */ EVEX_T4,/* input_size_in_bits */  EVEX_64bit);
6792   attributes.reset_is_clear_context();
6793   attributes.set_is_evex_instruction();
6794   vex_prefix(dst, 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
6795   emit_int8(0x1B);
6796   emit_operand(src, dst);
6797   // 0x00 - extract from lower 256 bits
6798   // 0x01 - extract from upper 256 bits
6799   emit_int8(imm8 &amp; 0x01);
6800 }
6801 
6802 // duplicate 1-byte integer data from src into programmed locations in dest : requires AVX512BW and AVX512VL
6803 void Assembler::vpbroadcastb(XMMRegister dst, XMMRegister src, int vector_len) {
6804   assert(VM_Version::supports_avx2(), "");
6805   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6806   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6807   emit_int8(0x78);
6808   emit_int8((unsigned char)(0xC0 | encode));
6809 }
6810 
6811 void Assembler::vpbroadcastb(XMMRegister dst, Address src, int vector_len) {
6812   assert(VM_Version::supports_avx2(), "");
6813   assert(dst != xnoreg, "sanity");
6814   InstructionMark im(this);
6815   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6816   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_8bit);
6817   // swap src&lt;-&gt;dst for encoding
6818   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6819   emit_int8(0x78);
6820   emit_operand(dst, src);
6821 }
6822 
6823 // duplicate 2-byte integer data from src into programmed locations in dest : requires AVX512BW and AVX512VL
6824 void Assembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
6825   assert(VM_Version::supports_avx2(), "");
6826   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6827   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6828   emit_int8(0x79);
6829   emit_int8((unsigned char)(0xC0 | encode));
6830 }
6831 
6832 void Assembler::vpbroadcastw(XMMRegister dst, Address src, int vector_len) {
6833   assert(VM_Version::supports_avx2(), "");
6834   assert(dst != xnoreg, "sanity");
6835   InstructionMark im(this);
6836   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6837   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_16bit);
6838   // swap src&lt;-&gt;dst for encoding
6839   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6840   emit_int8(0x79);
6841   emit_operand(dst, src);
6842 }
6843 
6844 // xmm/mem sourced byte/word/dword/qword replicate
6845 
6846 // duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL
6847 void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {
6848   assert(UseAVX &gt;= 2, "");
6849   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6850   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6851   emit_int8(0x58);
6852   emit_int8((unsigned char)(0xC0 | encode));
6853 }
6854 
6855 void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {
6856   assert(VM_Version::supports_avx2(), "");
6857   assert(dst != xnoreg, "sanity");
6858   InstructionMark im(this);
6859   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6860   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
6861   // swap src&lt;-&gt;dst for encoding
6862   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6863   emit_int8(0x58);
6864   emit_operand(dst, src);
6865 }
6866 
6867 // duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL
6868 void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {
6869   assert(VM_Version::supports_avx2(), "");
6870   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6871   attributes.set_rex_vex_w_reverted();
6872   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6873   emit_int8(0x59);
6874   emit_int8((unsigned char)(0xC0 | encode));
6875 }
6876 
6877 void Assembler::vpbroadcastq(XMMRegister dst, Address src, int vector_len) {
6878   assert(VM_Version::supports_avx2(), "");
6879   assert(dst != xnoreg, "sanity");
6880   InstructionMark im(this);
6881   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6882   attributes.set_rex_vex_w_reverted();
6883   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
6884   // swap src&lt;-&gt;dst for encoding
6885   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6886   emit_int8(0x59);
6887   emit_operand(dst, src);
6888 }
6889 void Assembler::evbroadcasti64x2(XMMRegister dst, XMMRegister src, int vector_len) {
6890   assert(vector_len != Assembler::AVX_128bit, "");
6891   assert(VM_Version::supports_avx512dq(), "");
6892   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6893   attributes.set_rex_vex_w_reverted();
6894   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6895   emit_int8(0x5A);
6896   emit_int8((unsigned char)(0xC0 | encode));
6897 }
6898 
6899 void Assembler::evbroadcasti64x2(XMMRegister dst, Address src, int vector_len) {
6900   assert(vector_len != Assembler::AVX_128bit, "");
6901   assert(VM_Version::supports_avx512dq(), "");
6902   assert(dst != xnoreg, "sanity");
6903   InstructionMark im(this);
6904   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6905   attributes.set_rex_vex_w_reverted();
6906   attributes.set_address_attributes(/* tuple_type */ EVEX_T2, /* input_size_in_bits */ EVEX_64bit);
6907   // swap src&lt;-&gt;dst for encoding
6908   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6909   emit_int8(0x5A);
6910   emit_operand(dst, src);
6911 }
6912 
6913 // scalar single/double precision replicate
6914 
6915 // duplicate single precision data from src into programmed locations in dest : requires AVX512VL
6916 void Assembler::vpbroadcastss(XMMRegister dst, XMMRegister src, int vector_len) {
6917   assert(VM_Version::supports_avx(), "");
6918   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6919   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6920   emit_int8(0x18);
6921   emit_int8((unsigned char)(0xC0 | encode));
6922 }
6923 
6924 void Assembler::vpbroadcastss(XMMRegister dst, Address src, int vector_len) {
6925   assert(VM_Version::supports_avx(), "");
6926   assert(dst != xnoreg, "sanity");
6927   InstructionMark im(this);
6928   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6929   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_32bit);
6930   // swap src&lt;-&gt;dst for encoding
6931   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6932   emit_int8(0x18);
6933   emit_operand(dst, src);
6934 }
6935 
6936 // duplicate double precision data from src into programmed locations in dest : requires AVX512VL
6937 void Assembler::vpbroadcastsd(XMMRegister dst, XMMRegister src, int vector_len) {
6938   assert(VM_Version::supports_avx(), "");
6939   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6940   attributes.set_rex_vex_w_reverted();
6941   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6942   emit_int8(0x19);
6943   emit_int8((unsigned char)(0xC0 | encode));
6944 }
6945 
6946 void Assembler::vpbroadcastsd(XMMRegister dst, Address src, int vector_len) {
6947   assert(VM_Version::supports_avx(), "");
6948   assert(dst != xnoreg, "sanity");
6949   InstructionMark im(this);
6950   InstructionAttr attributes(vector_len, /* vex_w */ VM_Version::supports_evex(), /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6951   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
6952   attributes.set_rex_vex_w_reverted();
6953   // swap src&lt;-&gt;dst for encoding
6954   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6955   emit_int8(0x19);
6956   emit_operand(dst, src);
6957 }
6958 
6959 
6960 // gpr source broadcast forms
6961 
6962 // duplicate 1-byte integer data from src into programmed locations in dest : requires AVX512BW and AVX512VL
6963 void Assembler::evpbroadcastb(XMMRegister dst, Register src, int vector_len) {
6964   assert(VM_Version::supports_avx512bw(), "");
6965   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6966   attributes.set_is_evex_instruction();
6967   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6968   emit_int8(0x7A);
6969   emit_int8((unsigned char)(0xC0 | encode));
6970 }
6971 
6972 // duplicate 2-byte integer data from src into programmed locations in dest : requires AVX512BW and AVX512VL
6973 void Assembler::evpbroadcastw(XMMRegister dst, Register src, int vector_len) {
6974   assert(VM_Version::supports_avx512bw(), "");
6975   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ _legacy_mode_bw, /* no_mask_reg */ true, /* uses_vl */ true);
6976   attributes.set_is_evex_instruction();
6977   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6978   emit_int8(0x7B);
6979   emit_int8((unsigned char)(0xC0 | encode));
6980 }
6981 
6982 // duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL
6983 void Assembler::evpbroadcastd(XMMRegister dst, Register src, int vector_len) {
6984   assert(VM_Version::supports_evex(), "");
6985   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6986   attributes.set_is_evex_instruction();
6987   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6988   emit_int8(0x7C);
6989   emit_int8((unsigned char)(0xC0 | encode));
6990 }
6991 
6992 // duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL
6993 void Assembler::evpbroadcastq(XMMRegister dst, Register src, int vector_len) {
6994   assert(VM_Version::supports_evex(), "");
6995   InstructionAttr attributes(vector_len, /* vex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
6996   attributes.set_is_evex_instruction();
6997   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
6998   emit_int8(0x7C);
6999   emit_int8((unsigned char)(0xC0 | encode));
7000 }
7001 
7002 void Assembler::evpgatherdd(XMMRegister dst, KRegister mask, Address src, int vector_len) {
7003   assert(VM_Version::supports_evex(), "");
7004   assert(dst != xnoreg, "sanity");
7005   InstructionMark im(this);
7006   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ false, /* uses_vl */ true);
7007   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
7008   attributes.reset_is_clear_context();
7009   attributes.set_embedded_opmask_register_specifier(mask);
7010   attributes.set_is_evex_instruction();
7011   // swap src&lt;-&gt;dst for encoding
7012   vex_prefix(src, 0, dst-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
7013   emit_int8((unsigned char)0x90);
7014   emit_operand(dst, src);
7015 }
7016 
7017 // Carry-Less Multiplication Quadword
7018 void Assembler::pclmulqdq(XMMRegister dst, XMMRegister src, int mask) {
7019   assert(VM_Version::supports_clmul(), "");
7020   InstructionAttr attributes(AVX_128bit, /* rex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7021   int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7022   emit_int8(0x44);
7023   emit_int8((unsigned char)(0xC0 | encode));
7024   emit_int8((unsigned char)mask);
7025 }
7026 
7027 // Carry-Less Multiplication Quadword
7028 void Assembler::vpclmulqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int mask) {
7029   assert(VM_Version::supports_avx() &amp;&amp; VM_Version::supports_clmul(), "");
7030   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7031   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7032   emit_int8(0x44);
7033   emit_int8((unsigned char)(0xC0 | encode));
7034   emit_int8((unsigned char)mask);
7035 }
7036 
7037 void Assembler::evpclmulqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int mask, int vector_len) {
7038   assert(VM_Version::supports_vpclmulqdq(), "Requires vector carryless multiplication support");
7039   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ true);
7040   attributes.set_is_evex_instruction();
7041   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7042   emit_int8(0x44);
7043   emit_int8((unsigned char)(0xC0 | encode));
7044   emit_int8((unsigned char)mask);
7045 }
7046 
7047 void Assembler::vzeroupper() {
7048   if (VM_Version::supports_vzeroupper()) {
7049     InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
7050     (void)vex_prefix_and_encode(0, 0, 0, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
7051     emit_int8(0x77);
7052   }
7053 }
7054 
7055 #ifndef _LP64
7056 // 32bit only pieces of the assembler
7057 
7058 void Assembler::cmp_literal32(Register src1, int32_t imm32, RelocationHolder const&amp; rspec) {
7059   // NO PREFIX AS NEVER 64BIT
7060   InstructionMark im(this);
7061   emit_int8((unsigned char)0x81);
7062   emit_int8((unsigned char)(0xF8 | src1-&gt;encoding()));
7063   emit_data(imm32, rspec, 0);
7064 }
7065 
7066 void Assembler::cmp_literal32(Address src1, int32_t imm32, RelocationHolder const&amp; rspec) {
7067   // NO PREFIX AS NEVER 64BIT (not even 32bit versions of 64bit regs
7068   InstructionMark im(this);
7069   emit_int8((unsigned char)0x81);
7070   emit_operand(rdi, src1);
7071   emit_data(imm32, rspec, 0);
7072 }
7073 
7074 // The 64-bit (32bit platform) cmpxchg compares the value at adr with the contents of rdx:rax,
7075 // and stores rcx:rbx into adr if so; otherwise, the value at adr is loaded
7076 // into rdx:rax.  The ZF is set if the compared values were equal, and cleared otherwise.
7077 void Assembler::cmpxchg8(Address adr) {
7078   InstructionMark im(this);
7079   emit_int8(0x0F);
7080   emit_int8((unsigned char)0xC7);
7081   emit_operand(rcx, adr);
7082 }
7083 
7084 void Assembler::decl(Register dst) {
7085   // Don't use it directly. Use MacroAssembler::decrementl() instead.
7086  emit_int8(0x48 | dst-&gt;encoding());
7087 }
7088 
7089 #endif // _LP64
7090 
7091 // 64bit typically doesn't use the x87 but needs to for the trig funcs
7092 
7093 void Assembler::fabs() {
7094   emit_int8((unsigned char)0xD9);
7095   emit_int8((unsigned char)0xE1);
7096 }
7097 
7098 void Assembler::fadd(int i) {
7099   emit_farith(0xD8, 0xC0, i);
7100 }
7101 
7102 void Assembler::fadd_d(Address src) {
7103   InstructionMark im(this);
7104   emit_int8((unsigned char)0xDC);
7105   emit_operand32(rax, src);
7106 }
7107 
7108 void Assembler::fadd_s(Address src) {
7109   InstructionMark im(this);
7110   emit_int8((unsigned char)0xD8);
7111   emit_operand32(rax, src);
7112 }
7113 
7114 void Assembler::fadda(int i) {
7115   emit_farith(0xDC, 0xC0, i);
7116 }
7117 
7118 void Assembler::faddp(int i) {
7119   emit_farith(0xDE, 0xC0, i);
7120 }
7121 
7122 void Assembler::fchs() {
7123   emit_int8((unsigned char)0xD9);
7124   emit_int8((unsigned char)0xE0);
7125 }
7126 
7127 void Assembler::fcom(int i) {
7128   emit_farith(0xD8, 0xD0, i);
7129 }
7130 
7131 void Assembler::fcomp(int i) {
7132   emit_farith(0xD8, 0xD8, i);
7133 }
7134 
7135 void Assembler::fcomp_d(Address src) {
7136   InstructionMark im(this);
7137   emit_int8((unsigned char)0xDC);
7138   emit_operand32(rbx, src);
7139 }
7140 
7141 void Assembler::fcomp_s(Address src) {
7142   InstructionMark im(this);
7143   emit_int8((unsigned char)0xD8);
7144   emit_operand32(rbx, src);
7145 }
7146 
7147 void Assembler::fcompp() {
7148   emit_int8((unsigned char)0xDE);
7149   emit_int8((unsigned char)0xD9);
7150 }
7151 
7152 void Assembler::fcos() {
7153   emit_int8((unsigned char)0xD9);
7154   emit_int8((unsigned char)0xFF);
7155 }
7156 
7157 void Assembler::fdecstp() {
7158   emit_int8((unsigned char)0xD9);
7159   emit_int8((unsigned char)0xF6);
7160 }
7161 
7162 void Assembler::fdiv(int i) {
7163   emit_farith(0xD8, 0xF0, i);
7164 }
7165 
7166 void Assembler::fdiv_d(Address src) {
7167   InstructionMark im(this);
7168   emit_int8((unsigned char)0xDC);
7169   emit_operand32(rsi, src);
7170 }
7171 
7172 void Assembler::fdiv_s(Address src) {
7173   InstructionMark im(this);
7174   emit_int8((unsigned char)0xD8);
7175   emit_operand32(rsi, src);
7176 }
7177 
7178 void Assembler::fdiva(int i) {
7179   emit_farith(0xDC, 0xF8, i);
7180 }
7181 
7182 // Note: The Intel manual (Pentium Processor User's Manual, Vol.3, 1994)
7183 //       is erroneous for some of the floating-point instructions below.
7184 
7185 void Assembler::fdivp(int i) {
7186   emit_farith(0xDE, 0xF8, i);                    // ST(0) &lt;- ST(0) / ST(1) and pop (Intel manual wrong)
7187 }
7188 
7189 void Assembler::fdivr(int i) {
7190   emit_farith(0xD8, 0xF8, i);
7191 }
7192 
7193 void Assembler::fdivr_d(Address src) {
7194   InstructionMark im(this);
7195   emit_int8((unsigned char)0xDC);
7196   emit_operand32(rdi, src);
7197 }
7198 
7199 void Assembler::fdivr_s(Address src) {
7200   InstructionMark im(this);
7201   emit_int8((unsigned char)0xD8);
7202   emit_operand32(rdi, src);
7203 }
7204 
7205 void Assembler::fdivra(int i) {
7206   emit_farith(0xDC, 0xF0, i);
7207 }
7208 
7209 void Assembler::fdivrp(int i) {
7210   emit_farith(0xDE, 0xF0, i);                    // ST(0) &lt;- ST(1) / ST(0) and pop (Intel manual wrong)
7211 }
7212 
7213 void Assembler::ffree(int i) {
7214   emit_farith(0xDD, 0xC0, i);
7215 }
7216 
7217 void Assembler::fild_d(Address adr) {
7218   InstructionMark im(this);
7219   emit_int8((unsigned char)0xDF);
7220   emit_operand32(rbp, adr);
7221 }
7222 
7223 void Assembler::fild_s(Address adr) {
7224   InstructionMark im(this);
7225   emit_int8((unsigned char)0xDB);
7226   emit_operand32(rax, adr);
7227 }
7228 
7229 void Assembler::fincstp() {
7230   emit_int8((unsigned char)0xD9);
7231   emit_int8((unsigned char)0xF7);
7232 }
7233 
7234 void Assembler::finit() {
7235   emit_int8((unsigned char)0x9B);
7236   emit_int8((unsigned char)0xDB);
7237   emit_int8((unsigned char)0xE3);
7238 }
7239 
7240 void Assembler::fist_s(Address adr) {
7241   InstructionMark im(this);
7242   emit_int8((unsigned char)0xDB);
7243   emit_operand32(rdx, adr);
7244 }
7245 
7246 void Assembler::fistp_d(Address adr) {
7247   InstructionMark im(this);
7248   emit_int8((unsigned char)0xDF);
7249   emit_operand32(rdi, adr);
7250 }
7251 
7252 void Assembler::fistp_s(Address adr) {
7253   InstructionMark im(this);
7254   emit_int8((unsigned char)0xDB);
7255   emit_operand32(rbx, adr);
7256 }
7257 
7258 void Assembler::fld1() {
7259   emit_int8((unsigned char)0xD9);
7260   emit_int8((unsigned char)0xE8);
7261 }
7262 
7263 void Assembler::fld_d(Address adr) {
7264   InstructionMark im(this);
7265   emit_int8((unsigned char)0xDD);
7266   emit_operand32(rax, adr);
7267 }
7268 
7269 void Assembler::fld_s(Address adr) {
7270   InstructionMark im(this);
7271   emit_int8((unsigned char)0xD9);
7272   emit_operand32(rax, adr);
7273 }
7274 
7275 
7276 void Assembler::fld_s(int index) {
7277   emit_farith(0xD9, 0xC0, index);
7278 }
7279 
7280 void Assembler::fld_x(Address adr) {
7281   InstructionMark im(this);
7282   emit_int8((unsigned char)0xDB);
7283   emit_operand32(rbp, adr);
7284 }
7285 
7286 void Assembler::fldcw(Address src) {
7287   InstructionMark im(this);
7288   emit_int8((unsigned char)0xD9);
7289   emit_operand32(rbp, src);
7290 }
7291 
7292 void Assembler::fldenv(Address src) {
7293   InstructionMark im(this);
7294   emit_int8((unsigned char)0xD9);
7295   emit_operand32(rsp, src);
7296 }
7297 
7298 void Assembler::fldlg2() {
7299   emit_int8((unsigned char)0xD9);
7300   emit_int8((unsigned char)0xEC);
7301 }
7302 
7303 void Assembler::fldln2() {
7304   emit_int8((unsigned char)0xD9);
7305   emit_int8((unsigned char)0xED);
7306 }
7307 
7308 void Assembler::fldz() {
7309   emit_int8((unsigned char)0xD9);
7310   emit_int8((unsigned char)0xEE);
7311 }
7312 
7313 void Assembler::flog() {
7314   fldln2();
7315   fxch();
7316   fyl2x();
7317 }
7318 
7319 void Assembler::flog10() {
7320   fldlg2();
7321   fxch();
7322   fyl2x();
7323 }
7324 
7325 void Assembler::fmul(int i) {
7326   emit_farith(0xD8, 0xC8, i);
7327 }
7328 
7329 void Assembler::fmul_d(Address src) {
7330   InstructionMark im(this);
7331   emit_int8((unsigned char)0xDC);
7332   emit_operand32(rcx, src);
7333 }
7334 
7335 void Assembler::fmul_s(Address src) {
7336   InstructionMark im(this);
7337   emit_int8((unsigned char)0xD8);
7338   emit_operand32(rcx, src);
7339 }
7340 
7341 void Assembler::fmula(int i) {
7342   emit_farith(0xDC, 0xC8, i);
7343 }
7344 
7345 void Assembler::fmulp(int i) {
7346   emit_farith(0xDE, 0xC8, i);
7347 }
7348 
7349 void Assembler::fnsave(Address dst) {
7350   InstructionMark im(this);
7351   emit_int8((unsigned char)0xDD);
7352   emit_operand32(rsi, dst);
7353 }
7354 
7355 void Assembler::fnstcw(Address src) {
7356   InstructionMark im(this);
7357   emit_int8((unsigned char)0x9B);
7358   emit_int8((unsigned char)0xD9);
7359   emit_operand32(rdi, src);
7360 }
7361 
7362 void Assembler::fnstsw_ax() {
7363   emit_int8((unsigned char)0xDF);
7364   emit_int8((unsigned char)0xE0);
7365 }
7366 
7367 void Assembler::fprem() {
7368   emit_int8((unsigned char)0xD9);
7369   emit_int8((unsigned char)0xF8);
7370 }
7371 
7372 void Assembler::fprem1() {
7373   emit_int8((unsigned char)0xD9);
7374   emit_int8((unsigned char)0xF5);
7375 }
7376 
7377 void Assembler::frstor(Address src) {
7378   InstructionMark im(this);
7379   emit_int8((unsigned char)0xDD);
7380   emit_operand32(rsp, src);
7381 }
7382 
7383 void Assembler::fsin() {
7384   emit_int8((unsigned char)0xD9);
7385   emit_int8((unsigned char)0xFE);
7386 }
7387 
7388 void Assembler::fsqrt() {
7389   emit_int8((unsigned char)0xD9);
7390   emit_int8((unsigned char)0xFA);
7391 }
7392 
7393 void Assembler::fst_d(Address adr) {
7394   InstructionMark im(this);
7395   emit_int8((unsigned char)0xDD);
7396   emit_operand32(rdx, adr);
7397 }
7398 
7399 void Assembler::fst_s(Address adr) {
7400   InstructionMark im(this);
7401   emit_int8((unsigned char)0xD9);
7402   emit_operand32(rdx, adr);
7403 }
7404 
7405 void Assembler::fstp_d(Address adr) {
7406   InstructionMark im(this);
7407   emit_int8((unsigned char)0xDD);
7408   emit_operand32(rbx, adr);
7409 }
7410 
7411 void Assembler::fstp_d(int index) {
7412   emit_farith(0xDD, 0xD8, index);
7413 }
7414 
7415 void Assembler::fstp_s(Address adr) {
7416   InstructionMark im(this);
7417   emit_int8((unsigned char)0xD9);
7418   emit_operand32(rbx, adr);
7419 }
7420 
7421 void Assembler::fstp_x(Address adr) {
7422   InstructionMark im(this);
7423   emit_int8((unsigned char)0xDB);
7424   emit_operand32(rdi, adr);
7425 }
7426 
7427 void Assembler::fsub(int i) {
7428   emit_farith(0xD8, 0xE0, i);
7429 }
7430 
7431 void Assembler::fsub_d(Address src) {
7432   InstructionMark im(this);
7433   emit_int8((unsigned char)0xDC);
7434   emit_operand32(rsp, src);
7435 }
7436 
7437 void Assembler::fsub_s(Address src) {
7438   InstructionMark im(this);
7439   emit_int8((unsigned char)0xD8);
7440   emit_operand32(rsp, src);
7441 }
7442 
7443 void Assembler::fsuba(int i) {
7444   emit_farith(0xDC, 0xE8, i);
7445 }
7446 
7447 void Assembler::fsubp(int i) {
7448   emit_farith(0xDE, 0xE8, i);                    // ST(0) &lt;- ST(0) - ST(1) and pop (Intel manual wrong)
7449 }
7450 
7451 void Assembler::fsubr(int i) {
7452   emit_farith(0xD8, 0xE8, i);
7453 }
7454 
7455 void Assembler::fsubr_d(Address src) {
7456   InstructionMark im(this);
7457   emit_int8((unsigned char)0xDC);
7458   emit_operand32(rbp, src);
7459 }
7460 
7461 void Assembler::fsubr_s(Address src) {
7462   InstructionMark im(this);
7463   emit_int8((unsigned char)0xD8);
7464   emit_operand32(rbp, src);
7465 }
7466 
7467 void Assembler::fsubra(int i) {
7468   emit_farith(0xDC, 0xE0, i);
7469 }
7470 
7471 void Assembler::fsubrp(int i) {
7472   emit_farith(0xDE, 0xE0, i);                    // ST(0) &lt;- ST(1) - ST(0) and pop (Intel manual wrong)
7473 }
7474 
7475 void Assembler::ftan() {
7476   emit_int8((unsigned char)0xD9);
7477   emit_int8((unsigned char)0xF2);
7478   emit_int8((unsigned char)0xDD);
7479   emit_int8((unsigned char)0xD8);
7480 }
7481 
7482 void Assembler::ftst() {
7483   emit_int8((unsigned char)0xD9);
7484   emit_int8((unsigned char)0xE4);
7485 }
7486 
7487 void Assembler::fucomi(int i) {
7488   // make sure the instruction is supported (introduced for P6, together with cmov)
7489   guarantee(VM_Version::supports_cmov(), "illegal instruction");
7490   emit_farith(0xDB, 0xE8, i);
7491 }
7492 
7493 void Assembler::fucomip(int i) {
7494   // make sure the instruction is supported (introduced for P6, together with cmov)
7495   guarantee(VM_Version::supports_cmov(), "illegal instruction");
7496   emit_farith(0xDF, 0xE8, i);
7497 }
7498 
7499 void Assembler::fwait() {
7500   emit_int8((unsigned char)0x9B);
7501 }
7502 
7503 void Assembler::fxch(int i) {
7504   emit_farith(0xD9, 0xC8, i);
7505 }
7506 
7507 void Assembler::fyl2x() {
7508   emit_int8((unsigned char)0xD9);
7509   emit_int8((unsigned char)0xF1);
7510 }
7511 
7512 void Assembler::frndint() {
7513   emit_int8((unsigned char)0xD9);
7514   emit_int8((unsigned char)0xFC);
7515 }
7516 
7517 void Assembler::f2xm1() {
7518   emit_int8((unsigned char)0xD9);
7519   emit_int8((unsigned char)0xF0);
7520 }
7521 
7522 void Assembler::fldl2e() {
7523   emit_int8((unsigned char)0xD9);
7524   emit_int8((unsigned char)0xEA);
7525 }
7526 
7527 // SSE SIMD prefix byte values corresponding to VexSimdPrefix encoding.
7528 static int simd_pre[4] = { 0, 0x66, 0xF3, 0xF2 };
7529 // SSE opcode second byte values (first is 0x0F) corresponding to VexOpcode encoding.
7530 static int simd_opc[4] = { 0,    0, 0x38, 0x3A };
7531 
7532 // Generate SSE legacy REX prefix and SIMD opcode based on VEX encoding.
7533 void Assembler::rex_prefix(Address adr, XMMRegister xreg, VexSimdPrefix pre, VexOpcode opc, bool rex_w) {
7534   if (pre &gt; 0) {
7535     emit_int8(simd_pre[pre]);
7536   }
7537   if (rex_w) {
7538     prefixq(adr, xreg);
7539   } else {
7540     prefix(adr, xreg);
7541   }
7542   if (opc &gt; 0) {
7543     emit_int8(0x0F);
7544     int opc2 = simd_opc[opc];
7545     if (opc2 &gt; 0) {
7546       emit_int8(opc2);
7547     }
7548   }
7549 }
7550 
7551 int Assembler::rex_prefix_and_encode(int dst_enc, int src_enc, VexSimdPrefix pre, VexOpcode opc, bool rex_w) {
7552   if (pre &gt; 0) {
7553     emit_int8(simd_pre[pre]);
7554   }
7555   int encode = (rex_w) ? prefixq_and_encode(dst_enc, src_enc) : prefix_and_encode(dst_enc, src_enc);
7556   if (opc &gt; 0) {
7557     emit_int8(0x0F);
7558     int opc2 = simd_opc[opc];
7559     if (opc2 &gt; 0) {
7560       emit_int8(opc2);
7561     }
7562   }
7563   return encode;
7564 }
7565 
7566 
7567 void Assembler::vex_prefix(bool vex_r, bool vex_b, bool vex_x, int nds_enc, VexSimdPrefix pre, VexOpcode opc) {
7568   int vector_len = _attributes-&gt;get_vector_len();
7569   bool vex_w = _attributes-&gt;is_rex_vex_w();
7570   if (vex_b || vex_x || vex_w || (opc == VEX_OPCODE_0F_38) || (opc == VEX_OPCODE_0F_3A)) {
7571     prefix(VEX_3bytes);
7572 
7573     int byte1 = (vex_r ? VEX_R : 0) | (vex_x ? VEX_X : 0) | (vex_b ? VEX_B : 0);
7574     byte1 = (~byte1) &amp; 0xE0;
7575     byte1 |= opc;
7576     emit_int8(byte1);
7577 
7578     int byte2 = ((~nds_enc) &amp; 0xf) &lt;&lt; 3;
7579     byte2 |= (vex_w ? VEX_W : 0) | ((vector_len &gt; 0) ? 4 : 0) | pre;
7580     emit_int8(byte2);
7581   } else {
7582     prefix(VEX_2bytes);
7583 
7584     int byte1 = vex_r ? VEX_R : 0;
7585     byte1 = (~byte1) &amp; 0x80;
7586     byte1 |= ((~nds_enc) &amp; 0xf) &lt;&lt; 3;
7587     byte1 |= ((vector_len &gt; 0 ) ? 4 : 0) | pre;
7588     emit_int8(byte1);
7589   }
7590 }
7591 
7592 // This is a 4 byte encoding
7593 void Assembler::evex_prefix(bool vex_r, bool vex_b, bool vex_x, bool evex_r, bool evex_v, int nds_enc, VexSimdPrefix pre, VexOpcode opc){
7594   // EVEX 0x62 prefix
7595   prefix(EVEX_4bytes);
7596   bool vex_w = _attributes-&gt;is_rex_vex_w();
7597   int evex_encoding = (vex_w ? VEX_W : 0);
7598   // EVEX.b is not currently used for broadcast of single element or data rounding modes
7599   _attributes-&gt;set_evex_encoding(evex_encoding);
7600 
7601   // P0: byte 2, initialized to RXBR`00mm
7602   // instead of not'd
7603   int byte2 = (vex_r ? VEX_R : 0) | (vex_x ? VEX_X : 0) | (vex_b ? VEX_B : 0) | (evex_r ? EVEX_Rb : 0);
7604   byte2 = (~byte2) &amp; 0xF0;
7605   // confine opc opcode extensions in mm bits to lower two bits
7606   // of form {0F, 0F_38, 0F_3A}
7607   byte2 |= opc;
7608   emit_int8(byte2);
7609 
7610   // P1: byte 3 as Wvvvv1pp
7611   int byte3 = ((~nds_enc) &amp; 0xf) &lt;&lt; 3;
7612   // p[10] is always 1
7613   byte3 |= EVEX_F;
7614   byte3 |= (vex_w &amp; 1) &lt;&lt; 7;
7615   // confine pre opcode extensions in pp bits to lower two bits
7616   // of form {66, F3, F2}
7617   byte3 |= pre;
7618   emit_int8(byte3);
7619 
7620   // P2: byte 4 as zL'Lbv'aaa
7621   // kregs are implemented in the low 3 bits as aaa
7622   int byte4 = (_attributes-&gt;is_no_reg_mask()) ?
7623               0 :
7624               _attributes-&gt;get_embedded_opmask_register_specifier();
7625   // EVEX.v` for extending EVEX.vvvv or VIDX
7626   byte4 |= (evex_v ? 0: EVEX_V);
7627   // third EXEC.b for broadcast actions
7628   byte4 |= (_attributes-&gt;is_extended_context() ? EVEX_Rb : 0);
7629   // fourth EVEX.L'L for vector length : 0 is 128, 1 is 256, 2 is 512, currently we do not support 1024
7630   byte4 |= ((_attributes-&gt;get_vector_len())&amp; 0x3) &lt;&lt; 5;
7631   // last is EVEX.z for zero/merge actions
7632   if (_attributes-&gt;is_no_reg_mask() == false) {
7633     byte4 |= (_attributes-&gt;is_clear_context() ? EVEX_Z : 0);
7634   }
7635   emit_int8(byte4);
7636 }
7637 
7638 void Assembler::vex_prefix(Address adr, int nds_enc, int xreg_enc, VexSimdPrefix pre, VexOpcode opc, InstructionAttr *attributes) {
7639   bool vex_r = ((xreg_enc &amp; 8) == 8) ? 1 : 0;
7640   bool vex_b = adr.base_needs_rex();
7641   bool vex_x;
7642   if (adr.isxmmindex()) {
7643     vex_x = adr.xmmindex_needs_rex();
7644   } else {
7645     vex_x = adr.index_needs_rex();
7646   }
7647   set_attributes(attributes);
7648   attributes-&gt;set_current_assembler(this);
7649 
7650   // For EVEX instruction (which is not marked as pure EVEX instruction) check and see if this instruction
7651   // is allowed in legacy mode and has resources which will fit in it.
7652   // Pure EVEX instructions will have is_evex_instruction set in their definition.
7653   if (!attributes-&gt;is_legacy_mode()) {
7654     if (UseAVX &gt; 2 &amp;&amp; !attributes-&gt;is_evex_instruction() &amp;&amp; !_is_managed) {
7655       if ((attributes-&gt;get_vector_len() != AVX_512bit) &amp;&amp; (nds_enc &lt; 16) &amp;&amp; (xreg_enc &lt; 16)) {
7656           attributes-&gt;set_is_legacy_mode();
7657       }
7658     }
7659   }
7660 
7661   if (UseAVX &gt; 2) {
7662     assert(((!attributes-&gt;uses_vl()) ||
7663             (attributes-&gt;get_vector_len() == AVX_512bit) ||
7664             (!_legacy_mode_vl) ||
7665             (attributes-&gt;is_legacy_mode())),"XMM register should be 0-15");
7666     assert(((nds_enc &lt; 16 &amp;&amp; xreg_enc &lt; 16) || (!attributes-&gt;is_legacy_mode())),"XMM register should be 0-15");
7667   }
7668 
7669   _is_managed = false;
7670   if (UseAVX &gt; 2 &amp;&amp; !attributes-&gt;is_legacy_mode())
7671   {
7672     bool evex_r = (xreg_enc &gt;= 16);
7673     bool evex_v;
7674     // EVEX.V' is set to true when VSIB is used as we may need to use higher order XMM registers (16-31)
7675     if (adr.isxmmindex())  {
7676       evex_v = ((adr._xmmindex-&gt;encoding() &gt; 15) ? true : false);
7677     } else {
7678       evex_v = (nds_enc &gt;= 16);
7679     }
7680     attributes-&gt;set_is_evex_instruction();
7681     evex_prefix(vex_r, vex_b, vex_x, evex_r, evex_v, nds_enc, pre, opc);
7682   } else {
7683     if (UseAVX &gt; 2 &amp;&amp; attributes-&gt;is_rex_vex_w_reverted()) {
7684       attributes-&gt;set_rex_vex_w(false);
7685     }
7686     vex_prefix(vex_r, vex_b, vex_x, nds_enc, pre, opc);
7687   }
7688 }
7689 
7690 int Assembler::vex_prefix_and_encode(int dst_enc, int nds_enc, int src_enc, VexSimdPrefix pre, VexOpcode opc, InstructionAttr *attributes) {
7691   bool vex_r = ((dst_enc &amp; 8) == 8) ? 1 : 0;
7692   bool vex_b = ((src_enc &amp; 8) == 8) ? 1 : 0;
7693   bool vex_x = false;
7694   set_attributes(attributes);
7695   attributes-&gt;set_current_assembler(this);
7696 
7697   // For EVEX instruction (which is not marked as pure EVEX instruction) check and see if this instruction
7698   // is allowed in legacy mode and has resources which will fit in it.
7699   // Pure EVEX instructions will have is_evex_instruction set in their definition.
7700   if (!attributes-&gt;is_legacy_mode()) {
7701     if (UseAVX &gt; 2 &amp;&amp; !attributes-&gt;is_evex_instruction() &amp;&amp; !_is_managed) {
7702       if ((!attributes-&gt;uses_vl() || (attributes-&gt;get_vector_len() != AVX_512bit)) &amp;&amp;
7703           (dst_enc &lt; 16) &amp;&amp; (nds_enc &lt; 16) &amp;&amp; (src_enc &lt; 16)) {
7704           attributes-&gt;set_is_legacy_mode();
7705       }
7706     }
7707   }
7708 
7709   if (UseAVX &gt; 2) {
7710     // All the scalar fp instructions (with uses_vl as false) can have legacy_mode as false
7711     // Instruction with uses_vl true are vector instructions
7712     // All the vector instructions with AVX_512bit length can have legacy_mode as false
7713     // All the vector instructions with &lt; AVX_512bit length can have legacy_mode as false if AVX512vl() is supported
7714     // Rest all should have legacy_mode set as true
7715     assert(((!attributes-&gt;uses_vl()) ||
7716             (attributes-&gt;get_vector_len() == AVX_512bit) ||
7717             (!_legacy_mode_vl) ||
7718             (attributes-&gt;is_legacy_mode())),"XMM register should be 0-15");
7719     // Instruction with legacy_mode true should have dst, nds and src &lt; 15
7720     assert(((dst_enc &lt; 16 &amp;&amp; nds_enc &lt; 16 &amp;&amp; src_enc &lt; 16) || (!attributes-&gt;is_legacy_mode())),"XMM register should be 0-15");
7721   }
7722 
7723   _is_managed = false;
7724   if (UseAVX &gt; 2 &amp;&amp; !attributes-&gt;is_legacy_mode())
7725   {
7726     bool evex_r = (dst_enc &gt;= 16);
7727     bool evex_v = (nds_enc &gt;= 16);
7728     // can use vex_x as bank extender on rm encoding
7729     vex_x = (src_enc &gt;= 16);
7730     attributes-&gt;set_is_evex_instruction();
7731     evex_prefix(vex_r, vex_b, vex_x, evex_r, evex_v, nds_enc, pre, opc);
7732   } else {
7733     if (UseAVX &gt; 2 &amp;&amp; attributes-&gt;is_rex_vex_w_reverted()) {
7734       attributes-&gt;set_rex_vex_w(false);
7735     }
7736     vex_prefix(vex_r, vex_b, vex_x, nds_enc, pre, opc);
7737   }
7738 
7739   // return modrm byte components for operands
7740   return (((dst_enc &amp; 7) &lt;&lt; 3) | (src_enc &amp; 7));
7741 }
7742 
7743 
7744 void Assembler::simd_prefix(XMMRegister xreg, XMMRegister nds, Address adr, VexSimdPrefix pre,
7745                             VexOpcode opc, InstructionAttr *attributes) {
7746   if (UseAVX &gt; 0) {
7747     int xreg_enc = xreg-&gt;encoding();
7748     int nds_enc = nds-&gt;is_valid() ? nds-&gt;encoding() : 0;
7749     vex_prefix(adr, nds_enc, xreg_enc, pre, opc, attributes);
7750   } else {
7751     assert((nds == xreg) || (nds == xnoreg), "wrong sse encoding");
7752     rex_prefix(adr, xreg, pre, opc, attributes-&gt;is_rex_vex_w());
7753   }
7754 }
7755 
7756 int Assembler::simd_prefix_and_encode(XMMRegister dst, XMMRegister nds, XMMRegister src, VexSimdPrefix pre,
7757                                       VexOpcode opc, InstructionAttr *attributes) {
7758   int dst_enc = dst-&gt;encoding();
7759   int src_enc = src-&gt;encoding();
7760   if (UseAVX &gt; 0) {
7761     int nds_enc = nds-&gt;is_valid() ? nds-&gt;encoding() : 0;
7762     return vex_prefix_and_encode(dst_enc, nds_enc, src_enc, pre, opc, attributes);
7763   } else {
7764     assert((nds == dst) || (nds == src) || (nds == xnoreg), "wrong sse encoding");
7765     return rex_prefix_and_encode(dst_enc, src_enc, pre, opc, attributes-&gt;is_rex_vex_w());
7766   }
7767 }
7768 
7769 void Assembler::cmppd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len) {
7770   assert(VM_Version::supports_avx(), "");
7771   assert(!VM_Version::supports_evex(), "");
7772   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7773   int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
7774   emit_int8((unsigned char)0xC2);
7775   emit_int8((unsigned char)(0xC0 | encode));
7776   emit_int8((unsigned char)(0xF &amp; cop));
7777 }
7778 
7779 void Assembler::blendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {
7780   assert(VM_Version::supports_avx(), "");
7781   assert(!VM_Version::supports_evex(), "");
7782   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7783   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src1-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7784   emit_int8((unsigned char)0x4B);
7785   emit_int8((unsigned char)(0xC0 | encode));
7786   int src2_enc = src2-&gt;encoding();
7787   emit_int8((unsigned char)(0xF0 &amp; src2_enc&lt;&lt;4));
7788 }
7789 
7790 void Assembler::cmpps(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len) {
7791   assert(VM_Version::supports_avx(), "");
7792   assert(!VM_Version::supports_evex(), "");
7793   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7794   int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &amp;attributes);
7795   emit_int8((unsigned char)0xC2);
7796   emit_int8((unsigned char)(0xC0 | encode));
7797   emit_int8((unsigned char)(0xF &amp; cop));
7798 }
7799 
7800 void Assembler::blendvps(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {
7801   assert(VM_Version::supports_avx(), "");
7802   assert(!VM_Version::supports_evex(), "");
7803   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7804   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src1-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7805   emit_int8((unsigned char)0x4A);
7806   emit_int8((unsigned char)(0xC0 | encode));
7807   int src2_enc = src2-&gt;encoding();
7808   emit_int8((unsigned char)(0xF0 &amp; src2_enc&lt;&lt;4));
7809 }
7810 
7811 void Assembler::vpblendd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {
7812   assert(VM_Version::supports_avx2(), "");
7813   InstructionAttr attributes(vector_len, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7814   int encode = vex_prefix_and_encode(dst-&gt;encoding(), nds-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &amp;attributes);
7815   emit_int8((unsigned char)0x02);
7816   emit_int8((unsigned char)(0xC0 | encode));
7817   emit_int8((unsigned char)imm8);
7818 }
7819 
7820 void Assembler::shlxl(Register dst, Register src1, Register src2) {
7821   assert(VM_Version::supports_bmi2(), "");
7822   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7823   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src2-&gt;encoding(), src1-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
7824   emit_int8((unsigned char)0xF7);
7825   emit_int8((unsigned char)(0xC0 | encode));
7826 }
7827 
7828 void Assembler::shlxq(Register dst, Register src1, Register src2) {
7829   assert(VM_Version::supports_bmi2(), "");
7830   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ true);
7831   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src2-&gt;encoding(), src1-&gt;encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &amp;attributes);
7832   emit_int8((unsigned char)0xF7);
7833   emit_int8((unsigned char)(0xC0 | encode));
7834 }
7835 
7836 #ifndef _LP64
7837 
7838 void Assembler::incl(Register dst) {
7839   // Don't use it directly. Use MacroAssembler::incrementl() instead.
7840   emit_int8(0x40 | dst-&gt;encoding());
7841 }
7842 
7843 void Assembler::lea(Register dst, Address src) {
7844   leal(dst, src);
7845 }
7846 
7847 void Assembler::mov_literal32(Address dst, int32_t imm32, RelocationHolder const&amp; rspec) {
7848   InstructionMark im(this);
7849   emit_int8((unsigned char)0xC7);
7850   emit_operand(rax, dst);
7851   emit_data((int)imm32, rspec, 0);
7852 }
7853 
7854 void Assembler::mov_literal32(Register dst, int32_t imm32, RelocationHolder const&amp; rspec) {
7855   InstructionMark im(this);
7856   int encode = prefix_and_encode(dst-&gt;encoding());
7857   emit_int8((unsigned char)(0xB8 | encode));
7858   emit_data((int)imm32, rspec, 0);
7859 }
7860 
7861 void Assembler::popa() { // 32bit
7862   emit_int8(0x61);
7863 }
7864 
7865 void Assembler::push_literal32(int32_t imm32, RelocationHolder const&amp; rspec) {
7866   InstructionMark im(this);
7867   emit_int8(0x68);
7868   emit_data(imm32, rspec, 0);
7869 }
7870 
7871 void Assembler::pusha() { // 32bit
7872   emit_int8(0x60);
7873 }
7874 
7875 void Assembler::set_byte_if_not_zero(Register dst) {
7876   emit_int8(0x0F);
7877   emit_int8((unsigned char)0x95);
7878   emit_int8((unsigned char)(0xE0 | dst-&gt;encoding()));
7879 }
7880 
7881 void Assembler::shldl(Register dst, Register src) {
7882   emit_int8(0x0F);
7883   emit_int8((unsigned char)0xA5);
7884   emit_int8((unsigned char)(0xC0 | src-&gt;encoding() &lt;&lt; 3 | dst-&gt;encoding()));
7885 }
7886 
7887 // 0F A4 / r ib
7888 void Assembler::shldl(Register dst, Register src, int8_t imm8) {
7889   emit_int8(0x0F);
7890   emit_int8((unsigned char)0xA4);
7891   emit_int8((unsigned char)(0xC0 | src-&gt;encoding() &lt;&lt; 3 | dst-&gt;encoding()));
7892   emit_int8(imm8);
7893 }
7894 
7895 void Assembler::shrdl(Register dst, Register src) {
7896   emit_int8(0x0F);
7897   emit_int8((unsigned char)0xAD);
7898   emit_int8((unsigned char)(0xC0 | src-&gt;encoding() &lt;&lt; 3 | dst-&gt;encoding()));
7899 }
7900 
7901 #else // LP64
7902 
7903 void Assembler::set_byte_if_not_zero(Register dst) {
7904   int enc = prefix_and_encode(dst-&gt;encoding(), true);
7905   emit_int8(0x0F);
7906   emit_int8((unsigned char)0x95);
7907   emit_int8((unsigned char)(0xE0 | enc));
7908 }
7909 
7910 // 64bit only pieces of the assembler
7911 // This should only be used by 64bit instructions that can use rip-relative
7912 // it cannot be used by instructions that want an immediate value.
7913 
7914 bool Assembler::reachable(AddressLiteral adr) {
7915   int64_t disp;
7916   // None will force a 64bit literal to the code stream. Likely a placeholder
7917   // for something that will be patched later and we need to certain it will
7918   // always be reachable.
7919   if (adr.reloc() == relocInfo::none) {
7920     return false;
7921   }
7922   if (adr.reloc() == relocInfo::internal_word_type) {
7923     // This should be rip relative and easily reachable.
7924     return true;
7925   }
7926   if (adr.reloc() == relocInfo::virtual_call_type ||
7927       adr.reloc() == relocInfo::opt_virtual_call_type ||
7928       adr.reloc() == relocInfo::static_call_type ||
7929       adr.reloc() == relocInfo::static_stub_type ) {
7930     // This should be rip relative within the code cache and easily
7931     // reachable until we get huge code caches. (At which point
7932     // ic code is going to have issues).
7933     return true;
7934   }
7935   if (adr.reloc() != relocInfo::external_word_type &amp;&amp;
7936       adr.reloc() != relocInfo::poll_return_type &amp;&amp;  // these are really external_word but need special
7937       adr.reloc() != relocInfo::poll_type &amp;&amp;         // relocs to identify them
7938       adr.reloc() != relocInfo::runtime_call_type ) {
7939     return false;
7940   }
7941 
7942   // Stress the correction code
7943   if (ForceUnreachable) {
7944     // Must be runtimecall reloc, see if it is in the codecache
7945     // Flipping stuff in the codecache to be unreachable causes issues
7946     // with things like inline caches where the additional instructions
7947     // are not handled.
7948     if (CodeCache::find_blob(adr._target) == NULL) {
7949       return false;
7950     }
7951   }
7952   // For external_word_type/runtime_call_type if it is reachable from where we
7953   // are now (possibly a temp buffer) and where we might end up
7954   // anywhere in the codeCache then we are always reachable.
7955   // This would have to change if we ever save/restore shared code
7956   // to be more pessimistic.
7957   disp = (int64_t)adr._target - ((int64_t)CodeCache::low_bound() + sizeof(int));
7958   if (!is_simm32(disp)) return false;
7959   disp = (int64_t)adr._target - ((int64_t)CodeCache::high_bound() + sizeof(int));
7960   if (!is_simm32(disp)) return false;
7961 
7962   disp = (int64_t)adr._target - ((int64_t)pc() + sizeof(int));
7963 
7964   // Because rip relative is a disp + address_of_next_instruction and we
7965   // don't know the value of address_of_next_instruction we apply a fudge factor
7966   // to make sure we will be ok no matter the size of the instruction we get placed into.
7967   // We don't have to fudge the checks above here because they are already worst case.
7968 
7969   // 12 == override/rex byte, opcode byte, rm byte, sib byte, a 4-byte disp , 4-byte literal
7970   // + 4 because better safe than sorry.
7971   const int fudge = 12 + 4;
7972   if (disp &lt; 0) {
7973     disp -= fudge;
7974   } else {
7975     disp += fudge;
7976   }
7977   return is_simm32(disp);
7978 }
7979 
7980 // Check if the polling page is not reachable from the code cache using rip-relative
7981 // addressing.
7982 bool Assembler::is_polling_page_far() {
7983   intptr_t addr = (intptr_t)os::get_polling_page();
7984   return ForceUnreachable ||
7985          !is_simm32(addr - (intptr_t)CodeCache::low_bound()) ||
7986          !is_simm32(addr - (intptr_t)CodeCache::high_bound());
7987 }
7988 
7989 void Assembler::emit_data64(jlong data,
7990                             relocInfo::relocType rtype,
7991                             int format) {
7992   if (rtype == relocInfo::none) {
7993     emit_int64(data);
7994   } else {
7995     emit_data64(data, Relocation::spec_simple(rtype), format);
7996   }
7997 }
7998 
7999 void Assembler::emit_data64(jlong data,
8000                             RelocationHolder const&amp; rspec,
8001                             int format) {
8002   assert(imm_operand == 0, "default format must be immediate in this file");
8003   assert(imm_operand == format, "must be immediate");
8004   assert(inst_mark() != NULL, "must be inside InstructionMark");
8005   // Do not use AbstractAssembler::relocate, which is not intended for
8006   // embedded words.  Instead, relocate to the enclosing instruction.
8007   code_section()-&gt;relocate(inst_mark(), rspec, format);
8008 #ifdef ASSERT
8009   check_relocation(rspec, format);
8010 #endif
8011   emit_int64(data);
8012 }
8013 
8014 int Assembler::prefix_and_encode(int reg_enc, bool byteinst) {
8015   if (reg_enc &gt;= 8) {
8016     prefix(REX_B);
8017     reg_enc -= 8;
8018   } else if (byteinst &amp;&amp; reg_enc &gt;= 4) {
8019     prefix(REX);
8020   }
8021   return reg_enc;
8022 }
8023 
8024 int Assembler::prefixq_and_encode(int reg_enc) {
8025   if (reg_enc &lt; 8) {
8026     prefix(REX_W);
8027   } else {
8028     prefix(REX_WB);
8029     reg_enc -= 8;
8030   }
8031   return reg_enc;
8032 }
8033 
8034 int Assembler::prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte) {
8035   if (dst_enc &lt; 8) {
8036     if (src_enc &gt;= 8) {
8037       prefix(REX_B);
8038       src_enc -= 8;
8039     } else if ((src_is_byte &amp;&amp; src_enc &gt;= 4) || (dst_is_byte &amp;&amp; dst_enc &gt;= 4)) {
8040       prefix(REX);
8041     }
8042   } else {
8043     if (src_enc &lt; 8) {
8044       prefix(REX_R);
8045     } else {
8046       prefix(REX_RB);
8047       src_enc -= 8;
8048     }
8049     dst_enc -= 8;
8050   }
8051   return dst_enc &lt;&lt; 3 | src_enc;
8052 }
8053 
8054 int Assembler::prefixq_and_encode(int dst_enc, int src_enc) {
8055   if (dst_enc &lt; 8) {
8056     if (src_enc &lt; 8) {
8057       prefix(REX_W);
8058     } else {
8059       prefix(REX_WB);
8060       src_enc -= 8;
8061     }
8062   } else {
8063     if (src_enc &lt; 8) {
8064       prefix(REX_WR);
8065     } else {
8066       prefix(REX_WRB);
8067       src_enc -= 8;
8068     }
8069     dst_enc -= 8;
8070   }
8071   return dst_enc &lt;&lt; 3 | src_enc;
8072 }
8073 
8074 void Assembler::prefix(Register reg) {
8075   if (reg-&gt;encoding() &gt;= 8) {
8076     prefix(REX_B);
8077   }
8078 }
8079 
8080 void Assembler::prefix(Register dst, Register src, Prefix p) {
8081   if (src-&gt;encoding() &gt;= 8) {
8082     p = (Prefix)(p | REX_B);
8083   }
8084   if (dst-&gt;encoding() &gt;= 8) {
8085     p = (Prefix)( p | REX_R);
8086   }
8087   if (p != Prefix_EMPTY) {
8088     // do not generate an empty prefix
8089     prefix(p);
8090   }
8091 }
8092 
8093 void Assembler::prefix(Register dst, Address adr, Prefix p) {
8094   if (adr.base_needs_rex()) {
8095     if (adr.index_needs_rex()) {
8096       assert(false, "prefix(Register dst, Address adr, Prefix p) does not support handling of an X");
8097     } else {
8098       prefix(REX_B);
8099     }
8100   } else {
8101     if (adr.index_needs_rex()) {
8102       assert(false, "prefix(Register dst, Address adr, Prefix p) does not support handling of an X");
8103     }
8104   }
8105   if (dst-&gt;encoding() &gt;= 8) {
8106     p = (Prefix)(p | REX_R);
8107   }
8108   if (p != Prefix_EMPTY) {
8109     // do not generate an empty prefix
8110     prefix(p);
8111   }
8112 }
8113 
8114 void Assembler::prefix(Address adr) {
8115   if (adr.base_needs_rex()) {
8116     if (adr.index_needs_rex()) {
8117       prefix(REX_XB);
8118     } else {
8119       prefix(REX_B);
8120     }
8121   } else {
8122     if (adr.index_needs_rex()) {
8123       prefix(REX_X);
8124     }
8125   }
8126 }
8127 
8128 void Assembler::prefixq(Address adr) {
8129   if (adr.base_needs_rex()) {
8130     if (adr.index_needs_rex()) {
8131       prefix(REX_WXB);
8132     } else {
8133       prefix(REX_WB);
8134     }
8135   } else {
8136     if (adr.index_needs_rex()) {
8137       prefix(REX_WX);
8138     } else {
8139       prefix(REX_W);
8140     }
8141   }
8142 }
8143 
8144 
8145 void Assembler::prefix(Address adr, Register reg, bool byteinst) {
8146   if (reg-&gt;encoding() &lt; 8) {
8147     if (adr.base_needs_rex()) {
8148       if (adr.index_needs_rex()) {
8149         prefix(REX_XB);
8150       } else {
8151         prefix(REX_B);
8152       }
8153     } else {
8154       if (adr.index_needs_rex()) {
8155         prefix(REX_X);
8156       } else if (byteinst &amp;&amp; reg-&gt;encoding() &gt;= 4 ) {
8157         prefix(REX);
8158       }
8159     }
8160   } else {
8161     if (adr.base_needs_rex()) {
8162       if (adr.index_needs_rex()) {
8163         prefix(REX_RXB);
8164       } else {
8165         prefix(REX_RB);
8166       }
8167     } else {
8168       if (adr.index_needs_rex()) {
8169         prefix(REX_RX);
8170       } else {
8171         prefix(REX_R);
8172       }
8173     }
8174   }
8175 }
8176 
8177 void Assembler::prefixq(Address adr, Register src) {
8178   if (src-&gt;encoding() &lt; 8) {
8179     if (adr.base_needs_rex()) {
8180       if (adr.index_needs_rex()) {
8181         prefix(REX_WXB);
8182       } else {
8183         prefix(REX_WB);
8184       }
8185     } else {
8186       if (adr.index_needs_rex()) {
8187         prefix(REX_WX);
8188       } else {
8189         prefix(REX_W);
8190       }
8191     }
8192   } else {
8193     if (adr.base_needs_rex()) {
8194       if (adr.index_needs_rex()) {
8195         prefix(REX_WRXB);
8196       } else {
8197         prefix(REX_WRB);
8198       }
8199     } else {
8200       if (adr.index_needs_rex()) {
8201         prefix(REX_WRX);
8202       } else {
8203         prefix(REX_WR);
8204       }
8205     }
8206   }
8207 }
8208 
8209 void Assembler::prefix(Address adr, XMMRegister reg) {
8210   if (reg-&gt;encoding() &lt; 8) {
8211     if (adr.base_needs_rex()) {
8212       if (adr.index_needs_rex()) {
8213         prefix(REX_XB);
8214       } else {
8215         prefix(REX_B);
8216       }
8217     } else {
8218       if (adr.index_needs_rex()) {
8219         prefix(REX_X);
8220       }
8221     }
8222   } else {
8223     if (adr.base_needs_rex()) {
8224       if (adr.index_needs_rex()) {
8225         prefix(REX_RXB);
8226       } else {
8227         prefix(REX_RB);
8228       }
8229     } else {
8230       if (adr.index_needs_rex()) {
8231         prefix(REX_RX);
8232       } else {
8233         prefix(REX_R);
8234       }
8235     }
8236   }
8237 }
8238 
8239 void Assembler::prefixq(Address adr, XMMRegister src) {
8240   if (src-&gt;encoding() &lt; 8) {
8241     if (adr.base_needs_rex()) {
8242       if (adr.index_needs_rex()) {
8243         prefix(REX_WXB);
8244       } else {
8245         prefix(REX_WB);
8246       }
8247     } else {
8248       if (adr.index_needs_rex()) {
8249         prefix(REX_WX);
8250       } else {
8251         prefix(REX_W);
8252       }
8253     }
8254   } else {
8255     if (adr.base_needs_rex()) {
8256       if (adr.index_needs_rex()) {
8257         prefix(REX_WRXB);
8258       } else {
8259         prefix(REX_WRB);
8260       }
8261     } else {
8262       if (adr.index_needs_rex()) {
8263         prefix(REX_WRX);
8264       } else {
8265         prefix(REX_WR);
8266       }
8267     }
8268   }
8269 }
8270 
8271 void Assembler::adcq(Register dst, int32_t imm32) {
8272   (void) prefixq_and_encode(dst-&gt;encoding());
8273   emit_arith(0x81, 0xD0, dst, imm32);
8274 }
8275 
8276 void Assembler::adcq(Register dst, Address src) {
8277   InstructionMark im(this);
8278   prefixq(src, dst);
8279   emit_int8(0x13);
8280   emit_operand(dst, src);
8281 }
8282 
8283 void Assembler::adcq(Register dst, Register src) {
8284   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8285   emit_arith(0x13, 0xC0, dst, src);
8286 }
8287 
8288 void Assembler::addq(Address dst, int32_t imm32) {
8289   InstructionMark im(this);
8290   prefixq(dst);
8291   emit_arith_operand(0x81, rax, dst,imm32);
8292 }
8293 
8294 void Assembler::addq(Address dst, Register src) {
8295   InstructionMark im(this);
8296   prefixq(dst, src);
8297   emit_int8(0x01);
8298   emit_operand(src, dst);
8299 }
8300 
8301 void Assembler::addq(Register dst, int32_t imm32) {
8302   (void) prefixq_and_encode(dst-&gt;encoding());
8303   emit_arith(0x81, 0xC0, dst, imm32);
8304 }
8305 
8306 void Assembler::addq(Register dst, Address src) {
8307   InstructionMark im(this);
8308   prefixq(src, dst);
8309   emit_int8(0x03);
8310   emit_operand(dst, src);
8311 }
8312 
8313 void Assembler::addq(Register dst, Register src) {
8314   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8315   emit_arith(0x03, 0xC0, dst, src);
8316 }
8317 
8318 void Assembler::adcxq(Register dst, Register src) {
8319   //assert(VM_Version::supports_adx(), "adx instructions not supported");
8320   emit_int8((unsigned char)0x66);
8321   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8322   emit_int8(0x0F);
8323   emit_int8(0x38);
8324   emit_int8((unsigned char)0xF6);
8325   emit_int8((unsigned char)(0xC0 | encode));
8326 }
8327 
8328 void Assembler::adoxq(Register dst, Register src) {
8329   //assert(VM_Version::supports_adx(), "adx instructions not supported");
8330   emit_int8((unsigned char)0xF3);
8331   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8332   emit_int8(0x0F);
8333   emit_int8(0x38);
8334   emit_int8((unsigned char)0xF6);
8335   emit_int8((unsigned char)(0xC0 | encode));
8336 }
8337 
8338 void Assembler::andq(Address dst, int32_t imm32) {
8339   InstructionMark im(this);
8340   prefixq(dst);
8341   emit_int8((unsigned char)0x81);
8342   emit_operand(rsp, dst, 4);
8343   emit_int32(imm32);
8344 }
8345 
8346 void Assembler::andq(Register dst, int32_t imm32) {
8347   (void) prefixq_and_encode(dst-&gt;encoding());
8348   emit_arith(0x81, 0xE0, dst, imm32);
8349 }
8350 
8351 void Assembler::andq(Register dst, Address src) {
8352   InstructionMark im(this);
8353   prefixq(src, dst);
8354   emit_int8(0x23);
8355   emit_operand(dst, src);
8356 }
8357 
8358 void Assembler::andq(Register dst, Register src) {
8359   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8360   emit_arith(0x23, 0xC0, dst, src);
8361 }
8362 
8363 void Assembler::andnq(Register dst, Register src1, Register src2) {
8364   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8365   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8366   int encode = vex_prefix_and_encode(dst-&gt;encoding(), src1-&gt;encoding(), src2-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8367   emit_int8((unsigned char)0xF2);
8368   emit_int8((unsigned char)(0xC0 | encode));
8369 }
8370 
8371 void Assembler::andnq(Register dst, Register src1, Address src2) {
8372   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8373   InstructionMark im(this);
8374   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8375   vex_prefix(src2, src1-&gt;encoding(), dst-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8376   emit_int8((unsigned char)0xF2);
8377   emit_operand(dst, src2);
8378 }
8379 
8380 void Assembler::bsfq(Register dst, Register src) {
8381   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8382   emit_int8(0x0F);
8383   emit_int8((unsigned char)0xBC);
8384   emit_int8((unsigned char)(0xC0 | encode));
8385 }
8386 
8387 void Assembler::bsrq(Register dst, Register src) {
8388   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8389   emit_int8(0x0F);
8390   emit_int8((unsigned char)0xBD);
8391   emit_int8((unsigned char)(0xC0 | encode));
8392 }
8393 
8394 void Assembler::bswapq(Register reg) {
8395   int encode = prefixq_and_encode(reg-&gt;encoding());
8396   emit_int8(0x0F);
8397   emit_int8((unsigned char)(0xC8 | encode));
8398 }
8399 
8400 void Assembler::blsiq(Register dst, Register src) {
8401   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8402   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8403   int encode = vex_prefix_and_encode(rbx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8404   emit_int8((unsigned char)0xF3);
8405   emit_int8((unsigned char)(0xC0 | encode));
8406 }
8407 
8408 void Assembler::blsiq(Register dst, Address src) {
8409   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8410   InstructionMark im(this);
8411   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8412   vex_prefix(src, dst-&gt;encoding(), rbx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8413   emit_int8((unsigned char)0xF3);
8414   emit_operand(rbx, src);
8415 }
8416 
8417 void Assembler::blsmskq(Register dst, Register src) {
8418   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8419   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8420   int encode = vex_prefix_and_encode(rdx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8421   emit_int8((unsigned char)0xF3);
8422   emit_int8((unsigned char)(0xC0 | encode));
8423 }
8424 
8425 void Assembler::blsmskq(Register dst, Address src) {
8426   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8427   InstructionMark im(this);
8428   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8429   vex_prefix(src, dst-&gt;encoding(), rdx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8430   emit_int8((unsigned char)0xF3);
8431   emit_operand(rdx, src);
8432 }
8433 
8434 void Assembler::blsrq(Register dst, Register src) {
8435   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8436   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8437   int encode = vex_prefix_and_encode(rcx-&gt;encoding(), dst-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8438   emit_int8((unsigned char)0xF3);
8439   emit_int8((unsigned char)(0xC0 | encode));
8440 }
8441 
8442 void Assembler::blsrq(Register dst, Address src) {
8443   assert(VM_Version::supports_bmi1(), "bit manipulation instructions not supported");
8444   InstructionMark im(this);
8445   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8446   vex_prefix(src, dst-&gt;encoding(), rcx-&gt;encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &amp;attributes);
8447   emit_int8((unsigned char)0xF3);
8448   emit_operand(rcx, src);
8449 }
8450 
8451 void Assembler::cdqq() {
8452   prefix(REX_W);
8453   emit_int8((unsigned char)0x99);
8454 }
8455 
8456 void Assembler::clflush(Address adr) {
8457   prefix(adr);
8458   emit_int8(0x0F);
8459   emit_int8((unsigned char)0xAE);
8460   emit_operand(rdi, adr);
8461 }
8462 
8463 void Assembler::cmovq(Condition cc, Register dst, Register src) {
8464   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8465   emit_int8(0x0F);
8466   emit_int8(0x40 | cc);
8467   emit_int8((unsigned char)(0xC0 | encode));
8468 }
8469 
8470 void Assembler::cmovq(Condition cc, Register dst, Address src) {
8471   InstructionMark im(this);
8472   prefixq(src, dst);
8473   emit_int8(0x0F);
8474   emit_int8(0x40 | cc);
8475   emit_operand(dst, src);
8476 }
8477 
8478 void Assembler::cmpq(Address dst, int32_t imm32) {
8479   InstructionMark im(this);
8480   prefixq(dst);
8481   emit_int8((unsigned char)0x81);
8482   emit_operand(rdi, dst, 4);
8483   emit_int32(imm32);
8484 }
8485 
8486 void Assembler::cmpq(Register dst, int32_t imm32) {
8487   (void) prefixq_and_encode(dst-&gt;encoding());
8488   emit_arith(0x81, 0xF8, dst, imm32);
8489 }
8490 
8491 void Assembler::cmpq(Address dst, Register src) {
8492   InstructionMark im(this);
8493   prefixq(dst, src);
8494   emit_int8(0x3B);
8495   emit_operand(src, dst);
8496 }
8497 
8498 void Assembler::cmpq(Register dst, Register src) {
8499   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8500   emit_arith(0x3B, 0xC0, dst, src);
8501 }
8502 
8503 void Assembler::cmpq(Register dst, Address  src) {
8504   InstructionMark im(this);
8505   prefixq(src, dst);
8506   emit_int8(0x3B);
8507   emit_operand(dst, src);
8508 }
8509 
8510 void Assembler::cmpxchgq(Register reg, Address adr) {
8511   InstructionMark im(this);
8512   prefixq(adr, reg);
8513   emit_int8(0x0F);
8514   emit_int8((unsigned char)0xB1);
8515   emit_operand(reg, adr);
8516 }
8517 
8518 void Assembler::cvtsi2sdq(XMMRegister dst, Register src) {
8519   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
8520   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8521   int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
8522   emit_int8(0x2A);
8523   emit_int8((unsigned char)(0xC0 | encode));
8524 }
8525 
8526 void Assembler::cvtsi2sdq(XMMRegister dst, Address src) {
8527   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
8528   InstructionMark im(this);
8529   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8530   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
8531   simd_prefix(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
8532   emit_int8(0x2A);
8533   emit_operand(dst, src);
8534 }
8535 
8536 void Assembler::cvtsi2ssq(XMMRegister dst, Address src) {
8537   NOT_LP64(assert(VM_Version::supports_sse(), ""));
8538   InstructionMark im(this);
8539   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8540   attributes.set_address_attributes(/* tuple_type */ EVEX_T1S, /* input_size_in_bits */ EVEX_64bit);
8541   simd_prefix(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
8542   emit_int8(0x2A);
8543   emit_operand(dst, src);
8544 }
8545 
8546 void Assembler::cvttsd2siq(Register dst, XMMRegister src) {
8547   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
8548   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8549   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &amp;attributes);
8550   emit_int8(0x2C);
8551   emit_int8((unsigned char)(0xC0 | encode));
8552 }
8553 
8554 void Assembler::cvttss2siq(Register dst, XMMRegister src) {
8555   NOT_LP64(assert(VM_Version::supports_sse(), ""));
8556   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8557   int encode = simd_prefix_and_encode(as_XMMRegister(dst-&gt;encoding()), xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &amp;attributes);
8558   emit_int8(0x2C);
8559   emit_int8((unsigned char)(0xC0 | encode));
8560 }
8561 
8562 void Assembler::decl(Register dst) {
8563   // Don't use it directly. Use MacroAssembler::decrementl() instead.
8564   // Use two-byte form (one-byte form is a REX prefix in 64-bit mode)
8565   int encode = prefix_and_encode(dst-&gt;encoding());
8566   emit_int8((unsigned char)0xFF);
8567   emit_int8((unsigned char)(0xC8 | encode));
8568 }
8569 
8570 void Assembler::decq(Register dst) {
8571   // Don't use it directly. Use MacroAssembler::decrementq() instead.
8572   // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
8573   int encode = prefixq_and_encode(dst-&gt;encoding());
8574   emit_int8((unsigned char)0xFF);
8575   emit_int8(0xC8 | encode);
8576 }
8577 
8578 void Assembler::decq(Address dst) {
8579   // Don't use it directly. Use MacroAssembler::decrementq() instead.
8580   InstructionMark im(this);
8581   prefixq(dst);
8582   emit_int8((unsigned char)0xFF);
8583   emit_operand(rcx, dst);
8584 }
8585 
8586 void Assembler::fxrstor(Address src) {
8587   prefixq(src);
8588   emit_int8(0x0F);
8589   emit_int8((unsigned char)0xAE);
8590   emit_operand(as_Register(1), src);
8591 }
8592 
8593 void Assembler::xrstor(Address src) {
8594   prefixq(src);
8595   emit_int8(0x0F);
8596   emit_int8((unsigned char)0xAE);
8597   emit_operand(as_Register(5), src);
8598 }
8599 
8600 void Assembler::fxsave(Address dst) {
8601   prefixq(dst);
8602   emit_int8(0x0F);
8603   emit_int8((unsigned char)0xAE);
8604   emit_operand(as_Register(0), dst);
8605 }
8606 
8607 void Assembler::xsave(Address dst) {
8608   prefixq(dst);
8609   emit_int8(0x0F);
8610   emit_int8((unsigned char)0xAE);
8611   emit_operand(as_Register(4), dst);
8612 }
8613 
8614 void Assembler::idivq(Register src) {
8615   int encode = prefixq_and_encode(src-&gt;encoding());
8616   emit_int8((unsigned char)0xF7);
8617   emit_int8((unsigned char)(0xF8 | encode));
8618 }
8619 
8620 void Assembler::imulq(Register dst, Register src) {
8621   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8622   emit_int8(0x0F);
8623   emit_int8((unsigned char)0xAF);
8624   emit_int8((unsigned char)(0xC0 | encode));
8625 }
8626 
8627 void Assembler::imulq(Register dst, Register src, int value) {
8628   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8629   if (is8bit(value)) {
8630     emit_int8(0x6B);
8631     emit_int8((unsigned char)(0xC0 | encode));
8632     emit_int8(value &amp; 0xFF);
8633   } else {
8634     emit_int8(0x69);
8635     emit_int8((unsigned char)(0xC0 | encode));
8636     emit_int32(value);
8637   }
8638 }
8639 
8640 void Assembler::imulq(Register dst, Address src) {
8641   InstructionMark im(this);
8642   prefixq(src, dst);
8643   emit_int8(0x0F);
8644   emit_int8((unsigned char) 0xAF);
8645   emit_operand(dst, src);
8646 }
8647 
8648 void Assembler::incl(Register dst) {
8649   // Don't use it directly. Use MacroAssembler::incrementl() instead.
8650   // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
8651   int encode = prefix_and_encode(dst-&gt;encoding());
8652   emit_int8((unsigned char)0xFF);
8653   emit_int8((unsigned char)(0xC0 | encode));
8654 }
8655 
8656 void Assembler::incq(Register dst) {
8657   // Don't use it directly. Use MacroAssembler::incrementq() instead.
8658   // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
8659   int encode = prefixq_and_encode(dst-&gt;encoding());
8660   emit_int8((unsigned char)0xFF);
8661   emit_int8((unsigned char)(0xC0 | encode));
8662 }
8663 
8664 void Assembler::incq(Address dst) {
8665   // Don't use it directly. Use MacroAssembler::incrementq() instead.
8666   InstructionMark im(this);
8667   prefixq(dst);
8668   emit_int8((unsigned char)0xFF);
8669   emit_operand(rax, dst);
8670 }
8671 
8672 void Assembler::lea(Register dst, Address src) {
8673   leaq(dst, src);
8674 }
8675 
8676 void Assembler::leaq(Register dst, Address src) {
8677   InstructionMark im(this);
8678   prefixq(src, dst);
8679   emit_int8((unsigned char)0x8D);
8680   emit_operand(dst, src);
8681 }
8682 
8683 void Assembler::mov64(Register dst, int64_t imm64) {
8684   InstructionMark im(this);
8685   int encode = prefixq_and_encode(dst-&gt;encoding());
8686   emit_int8((unsigned char)(0xB8 | encode));
8687   emit_int64(imm64);
8688 }
8689 
8690 void Assembler::mov_literal64(Register dst, intptr_t imm64, RelocationHolder const&amp; rspec) {
8691   InstructionMark im(this);
8692   int encode = prefixq_and_encode(dst-&gt;encoding());
8693   emit_int8(0xB8 | encode);
8694   emit_data64(imm64, rspec);
8695 }
8696 
8697 void Assembler::mov_narrow_oop(Register dst, int32_t imm32, RelocationHolder const&amp; rspec) {
8698   InstructionMark im(this);
8699   int encode = prefix_and_encode(dst-&gt;encoding());
8700   emit_int8((unsigned char)(0xB8 | encode));
8701   emit_data((int)imm32, rspec, narrow_oop_operand);
8702 }
8703 
8704 void Assembler::mov_narrow_oop(Address dst, int32_t imm32,  RelocationHolder const&amp; rspec) {
8705   InstructionMark im(this);
8706   prefix(dst);
8707   emit_int8((unsigned char)0xC7);
8708   emit_operand(rax, dst, 4);
8709   emit_data((int)imm32, rspec, narrow_oop_operand);
8710 }
8711 
8712 void Assembler::cmp_narrow_oop(Register src1, int32_t imm32, RelocationHolder const&amp; rspec) {
8713   InstructionMark im(this);
8714   int encode = prefix_and_encode(src1-&gt;encoding());
8715   emit_int8((unsigned char)0x81);
8716   emit_int8((unsigned char)(0xF8 | encode));
8717   emit_data((int)imm32, rspec, narrow_oop_operand);
8718 }
8719 
8720 void Assembler::cmp_narrow_oop(Address src1, int32_t imm32, RelocationHolder const&amp; rspec) {
8721   InstructionMark im(this);
8722   prefix(src1);
8723   emit_int8((unsigned char)0x81);
8724   emit_operand(rax, src1, 4);
8725   emit_data((int)imm32, rspec, narrow_oop_operand);
8726 }
8727 
8728 void Assembler::lzcntq(Register dst, Register src) {
8729   assert(VM_Version::supports_lzcnt(), "encoding is treated as BSR");
8730   emit_int8((unsigned char)0xF3);
8731   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8732   emit_int8(0x0F);
8733   emit_int8((unsigned char)0xBD);
8734   emit_int8((unsigned char)(0xC0 | encode));
8735 }
8736 
8737 void Assembler::movdq(XMMRegister dst, Register src) {
8738   // table D-1 says MMX/SSE2
8739   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
8740   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8741   int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
8742   emit_int8(0x6E);
8743   emit_int8((unsigned char)(0xC0 | encode));
8744 }
8745 
8746 void Assembler::movdq(Register dst, XMMRegister src) {
8747   // table D-1 says MMX/SSE2
8748   NOT_LP64(assert(VM_Version::supports_sse2(), ""));
8749   InstructionAttr attributes(AVX_128bit, /* rex_w */ true, /* legacy_mode */ false, /* no_mask_reg */ true, /* uses_vl */ false);
8750   // swap src/dst to get correct prefix
8751   int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst-&gt;encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &amp;attributes);
8752   emit_int8(0x7E);
8753   emit_int8((unsigned char)(0xC0 | encode));
8754 }
8755 
8756 void Assembler::movq(Register dst, Register src) {
8757   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8758   emit_int8((unsigned char)0x8B);
8759   emit_int8((unsigned char)(0xC0 | encode));
8760 }
8761 
8762 void Assembler::movq(Register dst, Address src) {
8763   InstructionMark im(this);
8764   prefixq(src, dst);
8765   emit_int8((unsigned char)0x8B);
8766   emit_operand(dst, src);
8767 }
8768 
8769 void Assembler::movq(Address dst, Register src) {
8770   InstructionMark im(this);
8771   prefixq(dst, src);
8772   emit_int8((unsigned char)0x89);
8773   emit_operand(src, dst);
8774 }
8775 
8776 void Assembler::movsbq(Register dst, Address src) {
8777   InstructionMark im(this);
8778   prefixq(src, dst);
8779   emit_int8(0x0F);
8780   emit_int8((unsigned char)0xBE);
8781   emit_operand(dst, src);
8782 }
8783 
8784 void Assembler::movsbq(Register dst, Register src) {
8785   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8786   emit_int8(0x0F);
8787   emit_int8((unsigned char)0xBE);
8788   emit_int8((unsigned char)(0xC0 | encode));
8789 }
8790 
8791 void Assembler::movslq(Register dst, int32_t imm32) {
8792   // dbx shows movslq(rcx, 3) as movq     $0x0000000049000000,(%rbx)
8793   // and movslq(r8, 3); as movl     $0x0000000048000000,(%rbx)
8794   // as a result we shouldn't use until tested at runtime...
8795   ShouldNotReachHere();
8796   InstructionMark im(this);
8797   int encode = prefixq_and_encode(dst-&gt;encoding());
8798   emit_int8((unsigned char)(0xC7 | encode));
8799   emit_int32(imm32);
8800 }
8801 
8802 void Assembler::movslq(Address dst, int32_t imm32) {
8803   assert(is_simm32(imm32), "lost bits");
8804   InstructionMark im(this);
8805   prefixq(dst);
8806   emit_int8((unsigned char)0xC7);
8807   emit_operand(rax, dst, 4);
8808   emit_int32(imm32);
8809 }
8810 
8811 void Assembler::movslq(Register dst, Address src) {
8812   InstructionMark im(this);
8813   prefixq(src, dst);
8814   emit_int8(0x63);
8815   emit_operand(dst, src);
8816 }
8817 
8818 void Assembler::movslq(Register dst, Register src) {
8819   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8820   emit_int8(0x63);
8821   emit_int8((unsigned char)(0xC0 | encode));
8822 }
8823 
8824 void Assembler::movswq(Register dst, Address src) {
8825   InstructionMark im(this);
8826   prefixq(src, dst);
8827   emit_int8(0x0F);
8828   emit_int8((unsigned char)0xBF);
8829   emit_operand(dst, src);
8830 }
8831 
8832 void Assembler::movswq(Register dst, Register src) {
8833   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8834   emit_int8((unsigned char)0x0F);
8835   emit_int8((unsigned char)0xBF);
8836   emit_int8((unsigned char)(0xC0 | encode));
8837 }
8838 
8839 void Assembler::movzbq(Register dst, Address src) {
8840   InstructionMark im(this);
8841   prefixq(src, dst);
8842   emit_int8((unsigned char)0x0F);
8843   emit_int8((unsigned char)0xB6);
8844   emit_operand(dst, src);
8845 }
8846 
8847 void Assembler::movzbq(Register dst, Register src) {
8848   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8849   emit_int8(0x0F);
8850   emit_int8((unsigned char)0xB6);
8851   emit_int8(0xC0 | encode);
8852 }
8853 
8854 void Assembler::movzwq(Register dst, Address src) {
8855   InstructionMark im(this);
8856   prefixq(src, dst);
8857   emit_int8((unsigned char)0x0F);
8858   emit_int8((unsigned char)0xB7);
8859   emit_operand(dst, src);
8860 }
8861 
8862 void Assembler::movzwq(Register dst, Register src) {
8863   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8864   emit_int8((unsigned char)0x0F);
8865   emit_int8((unsigned char)0xB7);
8866   emit_int8((unsigned char)(0xC0 | encode));
8867 }
8868 
8869 void Assembler::mulq(Address src) {
8870   InstructionMark im(this);
8871   prefixq(src);
8872   emit_int8((unsigned char)0xF7);
8873   emit_operand(rsp, src);
8874 }
8875 
8876 void Assembler::mulq(Register src) {
8877   int encode = prefixq_and_encode(src-&gt;encoding());
8878   emit_int8((unsigned char)0xF7);
8879   emit_int8((unsigned char)(0xE0 | encode));
8880 }
8881 
8882 void Assembler::mulxq(Register dst1, Register dst2, Register src) {
8883   assert(VM_Version::supports_bmi2(), "bit manipulation instructions not supported");
8884   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
8885   int encode = vex_prefix_and_encode(dst1-&gt;encoding(), dst2-&gt;encoding(), src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &amp;attributes);
8886   emit_int8((unsigned char)0xF6);
8887   emit_int8((unsigned char)(0xC0 | encode));
8888 }
8889 
8890 void Assembler::negq(Register dst) {
8891   int encode = prefixq_and_encode(dst-&gt;encoding());
8892   emit_int8((unsigned char)0xF7);
8893   emit_int8((unsigned char)(0xD8 | encode));
8894 }
8895 
8896 void Assembler::notq(Register dst) {
8897   int encode = prefixq_and_encode(dst-&gt;encoding());
8898   emit_int8((unsigned char)0xF7);
8899   emit_int8((unsigned char)(0xD0 | encode));
8900 }
8901 
8902 void Assembler::orq(Address dst, int32_t imm32) {
8903   InstructionMark im(this);
8904   prefixq(dst);
8905   emit_int8((unsigned char)0x81);
8906   emit_operand(rcx, dst, 4);
8907   emit_int32(imm32);
8908 }
8909 
8910 void Assembler::orq(Register dst, int32_t imm32) {
8911   (void) prefixq_and_encode(dst-&gt;encoding());
8912   emit_arith(0x81, 0xC8, dst, imm32);
8913 }
8914 
8915 void Assembler::orq(Register dst, Address src) {
8916   InstructionMark im(this);
8917   prefixq(src, dst);
8918   emit_int8(0x0B);
8919   emit_operand(dst, src);
8920 }
8921 
8922 void Assembler::orq(Register dst, Register src) {
8923   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8924   emit_arith(0x0B, 0xC0, dst, src);
8925 }
8926 
8927 void Assembler::popa() { // 64bit
8928   movq(r15, Address(rsp, 0));
8929   movq(r14, Address(rsp, wordSize));
8930   movq(r13, Address(rsp, 2 * wordSize));
8931   movq(r12, Address(rsp, 3 * wordSize));
8932   movq(r11, Address(rsp, 4 * wordSize));
8933   movq(r10, Address(rsp, 5 * wordSize));
8934   movq(r9,  Address(rsp, 6 * wordSize));
8935   movq(r8,  Address(rsp, 7 * wordSize));
8936   movq(rdi, Address(rsp, 8 * wordSize));
8937   movq(rsi, Address(rsp, 9 * wordSize));
8938   movq(rbp, Address(rsp, 10 * wordSize));
8939   // skip rsp
8940   movq(rbx, Address(rsp, 12 * wordSize));
8941   movq(rdx, Address(rsp, 13 * wordSize));
8942   movq(rcx, Address(rsp, 14 * wordSize));
8943   movq(rax, Address(rsp, 15 * wordSize));
8944 
8945   addq(rsp, 16 * wordSize);
8946 }
8947 
8948 void Assembler::popcntq(Register dst, Address src) {
8949   assert(VM_Version::supports_popcnt(), "must support");
8950   InstructionMark im(this);
8951   emit_int8((unsigned char)0xF3);
8952   prefixq(src, dst);
8953   emit_int8((unsigned char)0x0F);
8954   emit_int8((unsigned char)0xB8);
8955   emit_operand(dst, src);
8956 }
8957 
8958 void Assembler::popcntq(Register dst, Register src) {
8959   assert(VM_Version::supports_popcnt(), "must support");
8960   emit_int8((unsigned char)0xF3);
8961   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
8962   emit_int8((unsigned char)0x0F);
8963   emit_int8((unsigned char)0xB8);
8964   emit_int8((unsigned char)(0xC0 | encode));
8965 }
8966 
8967 void Assembler::popq(Address dst) {
8968   InstructionMark im(this);
8969   prefixq(dst);
8970   emit_int8((unsigned char)0x8F);
8971   emit_operand(rax, dst);
8972 }
8973 
8974 void Assembler::pusha() { // 64bit
8975   // we have to store original rsp.  ABI says that 128 bytes
8976   // below rsp are local scratch.
8977   movq(Address(rsp, -5 * wordSize), rsp);
8978 
8979   subq(rsp, 16 * wordSize);
8980 
8981   movq(Address(rsp, 15 * wordSize), rax);
8982   movq(Address(rsp, 14 * wordSize), rcx);
8983   movq(Address(rsp, 13 * wordSize), rdx);
8984   movq(Address(rsp, 12 * wordSize), rbx);
8985   // skip rsp
8986   movq(Address(rsp, 10 * wordSize), rbp);
8987   movq(Address(rsp, 9 * wordSize), rsi);
8988   movq(Address(rsp, 8 * wordSize), rdi);
8989   movq(Address(rsp, 7 * wordSize), r8);
8990   movq(Address(rsp, 6 * wordSize), r9);
8991   movq(Address(rsp, 5 * wordSize), r10);
8992   movq(Address(rsp, 4 * wordSize), r11);
8993   movq(Address(rsp, 3 * wordSize), r12);
8994   movq(Address(rsp, 2 * wordSize), r13);
8995   movq(Address(rsp, wordSize), r14);
8996   movq(Address(rsp, 0), r15);
8997 }
8998 
8999 void Assembler::pushq(Address src) {
9000   InstructionMark im(this);
9001   prefixq(src);
9002   emit_int8((unsigned char)0xFF);
9003   emit_operand(rsi, src);
9004 }
9005 
9006 void Assembler::rclq(Register dst, int imm8) {
9007   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9008   int encode = prefixq_and_encode(dst-&gt;encoding());
9009   if (imm8 == 1) {
9010     emit_int8((unsigned char)0xD1);
9011     emit_int8((unsigned char)(0xD0 | encode));
9012   } else {
9013     emit_int8((unsigned char)0xC1);
9014     emit_int8((unsigned char)(0xD0 | encode));
9015     emit_int8(imm8);
9016   }
9017 }
9018 
9019 void Assembler::rcrq(Register dst, int imm8) {
9020   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9021   int encode = prefixq_and_encode(dst-&gt;encoding());
9022   if (imm8 == 1) {
9023     emit_int8((unsigned char)0xD1);
9024     emit_int8((unsigned char)(0xD8 | encode));
9025   } else {
9026     emit_int8((unsigned char)0xC1);
9027     emit_int8((unsigned char)(0xD8 | encode));
9028     emit_int8(imm8);
9029   }
9030 }
9031 
9032 void Assembler::rorq(Register dst, int imm8) {
9033   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9034   int encode = prefixq_and_encode(dst-&gt;encoding());
9035   if (imm8 == 1) {
9036     emit_int8((unsigned char)0xD1);
9037     emit_int8((unsigned char)(0xC8 | encode));
9038   } else {
9039     emit_int8((unsigned char)0xC1);
9040     emit_int8((unsigned char)(0xc8 | encode));
9041     emit_int8(imm8);
9042   }
9043 }
9044 
9045 void Assembler::rorxq(Register dst, Register src, int imm8) {
9046   assert(VM_Version::supports_bmi2(), "bit manipulation instructions not supported");
9047   InstructionAttr attributes(AVX_128bit, /* vex_w */ true, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
9048   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &amp;attributes);
9049   emit_int8((unsigned char)0xF0);
9050   emit_int8((unsigned char)(0xC0 | encode));
9051   emit_int8(imm8);
9052 }
9053 
9054 void Assembler::rorxd(Register dst, Register src, int imm8) {
9055   assert(VM_Version::supports_bmi2(), "bit manipulation instructions not supported");
9056   InstructionAttr attributes(AVX_128bit, /* vex_w */ false, /* legacy_mode */ true, /* no_mask_reg */ true, /* uses_vl */ false);
9057   int encode = vex_prefix_and_encode(dst-&gt;encoding(), 0, src-&gt;encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &amp;attributes);
9058   emit_int8((unsigned char)0xF0);
9059   emit_int8((unsigned char)(0xC0 | encode));
9060   emit_int8(imm8);
9061 }
9062 
9063 void Assembler::sarq(Register dst, int imm8) {
9064   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9065   int encode = prefixq_and_encode(dst-&gt;encoding());
9066   if (imm8 == 1) {
9067     emit_int8((unsigned char)0xD1);
9068     emit_int8((unsigned char)(0xF8 | encode));
9069   } else {
9070     emit_int8((unsigned char)0xC1);
9071     emit_int8((unsigned char)(0xF8 | encode));
9072     emit_int8(imm8);
9073   }
9074 }
9075 
9076 void Assembler::sarq(Register dst) {
9077   int encode = prefixq_and_encode(dst-&gt;encoding());
9078   emit_int8((unsigned char)0xD3);
9079   emit_int8((unsigned char)(0xF8 | encode));
9080 }
9081 
9082 void Assembler::sbbq(Address dst, int32_t imm32) {
9083   InstructionMark im(this);
9084   prefixq(dst);
9085   emit_arith_operand(0x81, rbx, dst, imm32);
9086 }
9087 
9088 void Assembler::sbbq(Register dst, int32_t imm32) {
9089   (void) prefixq_and_encode(dst-&gt;encoding());
9090   emit_arith(0x81, 0xD8, dst, imm32);
9091 }
9092 
9093 void Assembler::sbbq(Register dst, Address src) {
9094   InstructionMark im(this);
9095   prefixq(src, dst);
9096   emit_int8(0x1B);
9097   emit_operand(dst, src);
9098 }
9099 
9100 void Assembler::sbbq(Register dst, Register src) {
9101   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
9102   emit_arith(0x1B, 0xC0, dst, src);
9103 }
9104 
9105 void Assembler::shlq(Register dst, int imm8) {
9106   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9107   int encode = prefixq_and_encode(dst-&gt;encoding());
9108   if (imm8 == 1) {
9109     emit_int8((unsigned char)0xD1);
9110     emit_int8((unsigned char)(0xE0 | encode));
9111   } else {
9112     emit_int8((unsigned char)0xC1);
9113     emit_int8((unsigned char)(0xE0 | encode));
9114     emit_int8(imm8);
9115   }
9116 }
9117 
9118 void Assembler::shlq(Register dst) {
9119   int encode = prefixq_and_encode(dst-&gt;encoding());
9120   emit_int8((unsigned char)0xD3);
9121   emit_int8((unsigned char)(0xE0 | encode));
9122 }
9123 
9124 void Assembler::shrq(Register dst, int imm8) {
9125   assert(isShiftCount(imm8 &gt;&gt; 1), "illegal shift count");
9126   int encode = prefixq_and_encode(dst-&gt;encoding());
9127   emit_int8((unsigned char)0xC1);
9128   emit_int8((unsigned char)(0xE8 | encode));
9129   emit_int8(imm8);
9130 }
9131 
9132 void Assembler::shrq(Register dst) {
9133   int encode = prefixq_and_encode(dst-&gt;encoding());
9134   emit_int8((unsigned char)0xD3);
9135   emit_int8(0xE8 | encode);
9136 }
9137 
9138 void Assembler::subq(Address dst, int32_t imm32) {
9139   InstructionMark im(this);
9140   prefixq(dst);
9141   emit_arith_operand(0x81, rbp, dst, imm32);
9142 }
9143 
9144 void Assembler::subq(Address dst, Register src) {
9145   InstructionMark im(this);
9146   prefixq(dst, src);
9147   emit_int8(0x29);
9148   emit_operand(src, dst);
9149 }
9150 
9151 void Assembler::subq(Register dst, int32_t imm32) {
9152   (void) prefixq_and_encode(dst-&gt;encoding());
9153   emit_arith(0x81, 0xE8, dst, imm32);
9154 }
9155 
9156 // Force generation of a 4 byte immediate value even if it fits into 8bit
9157 void Assembler::subq_imm32(Register dst, int32_t imm32) {
9158   (void) prefixq_and_encode(dst-&gt;encoding());
9159   emit_arith_imm32(0x81, 0xE8, dst, imm32);
9160 }
9161 
9162 void Assembler::subq(Register dst, Address src) {
9163   InstructionMark im(this);
9164   prefixq(src, dst);
9165   emit_int8(0x2B);
9166   emit_operand(dst, src);
9167 }
9168 
9169 void Assembler::subq(Register dst, Register src) {
9170   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
9171   emit_arith(0x2B, 0xC0, dst, src);
9172 }
9173 
9174 void Assembler::testq(Register dst, int32_t imm32) {
9175   // not using emit_arith because test
9176   // doesn't support sign-extension of
9177   // 8bit operands
9178   int encode = dst-&gt;encoding();
9179   if (encode == 0) {
9180     prefix(REX_W);
9181     emit_int8((unsigned char)0xA9);
9182   } else {
9183     encode = prefixq_and_encode(encode);
9184     emit_int8((unsigned char)0xF7);
9185     emit_int8((unsigned char)(0xC0 | encode));
9186   }
9187   emit_int32(imm32);
9188 }
9189 
9190 void Assembler::testq(Register dst, Register src) {
9191   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
9192   emit_arith(0x85, 0xC0, dst, src);
9193 }
9194 
9195 void Assembler::testq(Register dst, Address src) {
9196   InstructionMark im(this);
9197   prefixq(src, dst);
9198   emit_int8((unsigned char)0x85);
9199   emit_operand(dst, src);
9200 }
9201 
9202 void Assembler::xaddq(Address dst, Register src) {
9203   InstructionMark im(this);
9204   prefixq(dst, src);
9205   emit_int8(0x0F);
9206   emit_int8((unsigned char)0xC1);
9207   emit_operand(src, dst);
9208 }
9209 
9210 void Assembler::xchgq(Register dst, Address src) {
9211   InstructionMark im(this);
9212   prefixq(src, dst);
9213   emit_int8((unsigned char)0x87);
9214   emit_operand(dst, src);
9215 }
9216 
9217 void Assembler::xchgq(Register dst, Register src) {
9218   int encode = prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
9219   emit_int8((unsigned char)0x87);
9220   emit_int8((unsigned char)(0xc0 | encode));
9221 }
9222 
9223 void Assembler::xorq(Register dst, Register src) {
9224   (void) prefixq_and_encode(dst-&gt;encoding(), src-&gt;encoding());
9225   emit_arith(0x33, 0xC0, dst, src);
9226 }
9227 
9228 void Assembler::xorq(Register dst, Address src) {
9229   InstructionMark im(this);
9230   prefixq(src, dst);
9231   emit_int8(0x33);
9232   emit_operand(dst, src);
9233 }
9234 
9235 #endif // !LP64
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
